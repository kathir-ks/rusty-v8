{
  "file_path": "/home/kathirks_gc/v8_go/archive/codebase/src/compiler/backend/instruction-selector.cc",
  "error": "Response not JSON and not XML-like after cleanup",
  "json_error_if_any": "Skipped JSON parsing for whole response due to presence of XML tags; XML is primary.",
  "raw_response": "```xml\n<file>\n    <metadata>\n        {\n            \"path\": \"/home/kathirks_gc/v8_go/archive/codebase/src/compiler/backend/instruction-selector.cc\",\n            \"file_name\": \"instruction-selector.cc\",\n            \"language\": \"cpp\",\n            \"purpose\": \"Selects machine instructions from a Turboshaft graph for a specific architecture.\"\n        }\n    </metadata>\n    <imports>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"purpose\": \"Includes standard library headers and V8-specific headers for compiler backend, code generation, and related functionalities.\"\n            }\n        </metadata>\n        <code><![CDATA[\n#include \"src/compiler/backend/instruction-selector.h\"\n\n#include <limits>\n#include <optional>\n\n#include \"include/v8-internal.h\"\n#include \"src/base/iterator.h\"\n#include \"src/codegen/machine-type.h\"\n#include \"src/codegen/tick-counter.h\"\n#include \"src/common/globals.h\"\n#include \"src/compiler/backend/instruction-selector-adapter.h\"\n#include \"src/compiler/backend/instruction-selector-impl.h\"\n#include \"src/compiler/backend/instruction.h\"\n#include \"src/compiler/compiler-source-position-table.h\"\n#include \"src/compiler/globals.h\"\n#include \"src/compiler/js-heap-broker.h\"\n#include \"src/compiler/state-values-utils.h\"\n#include \"src/compiler/turboshaft/operations.h\"\n#include \"src/compiler/turboshaft/opmasks.h\"\n#include \"src/compiler/turboshaft/representations.h\"\n#include \"src/numbers/conversions-inl.h\"\n#include \"src/zone/zone-containers.h\"\n\n#if V8_ENABLE_WEBASSEMBLY\n#include \"src/wasm/simd-shuffle.h\"\n#endif  // V8_ENABLE_WEBASSEMBLY\n        ]]></code>\n    </imports>\n\n    <class>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"type\": \"class\",\n                \"name\": \"InstructionSelectorT\",\n                \"about\": \"Selects machine instructions from a Turboshaft graph.  This class implements the core instruction selection algorithm.\",\n                \"attributes\": [],\n                \"dependencies\": [\n                    \"TurboshaftAdapter\",\n                    \"Zone\",\n                    \"Linkage\",\n                    \"InstructionSequence\",\n                    \"Graph\",\n                    \"source_position_table_t\",\n                    \"Frame\",\n                    \"TickCounter\",\n                    \"JSHeapBroker\",\n                    \"InstructionScheduler\",\n                    \"FrameStateDescriptor\",\n                    \"Instruction\",\n                    \"OpIndex\",\n                    \"FlagsContinuation\",\n                    \"InstructionOperand\",\n                    \"ExternalReference\"\n                ]\n            }\n        </metadata>\n        <code><![CDATA[\nnamespace v8 {\nnamespace internal {\nnamespace compiler {\n\n#define VISIT_UNSUPPORTED_OP(op) \\\n  void InstructionSelectorT::Visit##op(OpIndex) { UNIMPLEMENTED(); }\n\nusing namespace turboshaft;  // NOLINT(build/namespaces)\n\nnamespace {\n// Here we really want the raw Bits of the mask, but the `.bits()` method is\n// not constexpr, and so users of this constant need to call it.\n// TODO(turboshaft): EffectDimensions could probably be defined via\n// base::Flags<> instead, which should solve this.\nconstexpr EffectDimensions kTurboshaftEffectLevelMask =\n    OpEffects().CanReadMemory().produces;\n}\n\nInstructionSelectorT::InstructionSelectorT(\n    Zone* zone, size_t node_count, Linkage* linkage,\n    InstructionSequence* sequence, Graph* schedule,\n    source_position_table_t* source_positions, Frame* frame,\n    InstructionSelector::EnableSwitchJumpTable enable_switch_jump_table,\n    TickCounter* tick_counter, JSHeapBroker* broker,\n    size_t* max_unoptimized_frame_height, size_t* max_pushed_argument_count,\n    InstructionSelector::SourcePositionMode source_position_mode,\n    Features features, InstructionSelector::EnableScheduling enable_scheduling,\n    InstructionSelector::EnableRootsRelativeAddressing\n        enable_roots_relative_addressing,\n    InstructionSelector::EnableTraceTurboJson trace_turbo)\n    : TurboshaftAdapter(schedule),\n      zone_(zone),\n      linkage_(linkage),\n      sequence_(sequence),\n      source_positions_(source_positions),\n      source_position_mode_(source_position_mode),\n      features_(features),\n      schedule_(schedule),\n      current_block_(nullptr),\n      instructions_(zone),\n      continuation_inputs_(sequence->zone()),\n      continuation_outputs_(sequence->zone()),\n      continuation_temps_(sequence->zone()),\n      defined_(static_cast<int>(node_count), zone),\n      used_(static_cast<int>(node_count), zone),\n      effect_level_(node_count, 0, zone),\n      virtual_registers_(node_count,\n                         InstructionOperand::kInvalidVirtualRegister, zone),\n      virtual_register_rename_(zone),\n      scheduler_(nullptr),\n      enable_scheduling_(enable_scheduling),\n      enable_roots_relative_addressing_(enable_roots_relative_addressing),\n      enable_switch_jump_table_(enable_switch_jump_table),\n      state_values_cache_(zone),\n      frame_(frame),\n      instruction_selection_failed_(false),\n      instr_origins_(sequence->zone()),\n      trace_turbo_(trace_turbo),\n      tick_counter_(tick_counter),\n      broker_(broker),\n      max_unoptimized_frame_height_(max_unoptimized_frame_height),\n      max_pushed_argument_count_(max_pushed_argument_count)\n#if V8_TARGET_ARCH_64_BIT\n      ,\n      node_count_(node_count),\n      phi_states_(zone)\n#endif\n{\n    turboshaft_use_map_.emplace(*schedule_, zone);\n    protected_loads_to_remove_.emplace(static_cast<int>(node_count), zone);\n    additional_protected_instructions_.emplace(static_cast<int>(node_count),\n                                               zone);\n\n  DCHECK_EQ(*max_unoptimized_frame_height, 0);  // Caller-initialized.\n\n  instructions_.reserve(node_count);\n  continuation_inputs_.reserve(5);\n  continuation_outputs_.reserve(2);\n\n  if (trace_turbo_ == InstructionSelector::kEnableTraceTurboJson) {\n    instr_origins_.assign(node_count, {-1, 0});\n  }\n}\n\nstd::optional<BailoutReason> InstructionSelectorT::SelectInstructions() {\n  // Mark the inputs of all phis in loop headers as used.\n  ZoneVector<Block*> blocks = this->rpo_order(schedule());\n  for (const Block* block : blocks) {\n    if (!this->IsLoopHeader(block)) continue;\n    DCHECK_LE(2u, this->PredecessorCount(block));\n    for (OpIndex node : this->nodes(block)) {\n      if (!this->IsPhi(node)) continue;\n\n      // Mark all inputs as used.\n      for (OpIndex input : this->inputs(node)) {\n        MarkAsUsed(input);\n      }\n    }\n  }\n\n  // Visit each basic block in post order.\n  for (auto i = blocks.rbegin(); i != blocks.rend(); ++i) {\n    VisitBlock(*i);\n    if (instruction_selection_failed())\n      return BailoutReason::kCodeGenerationFailed;\n  }\n\n  // Schedule the selected instructions.\n  if (UseInstructionScheduling()) {\n    scheduler_ = zone()->template New<InstructionScheduler>(zone(), sequence());\n  }\n\n  for (const Block* block : blocks) {\n    InstructionBlock* instruction_block =\n        sequence()->InstructionBlockAt(this->rpo_number(block));\n    for (size_t i = 0; i < instruction_block->phis().size(); i++) {\n      UpdateRenamesInPhi(instruction_block->PhiAt(i));\n    }\n    size_t end = instruction_block->code_end();\n    size_t start = instruction_block->code_start();\n    DCHECK_LE(end, start);\n    StartBlock(this->rpo_number(block));\n    if (end != start) {\n      while (start-- > end + 1) {\n        UpdateRenames(instructions_[start]);\n        AddInstruction(instructions_[start]);\n      }\n      UpdateRenames(instructions_[end]);\n      AddTerminator(instructions_[end]);\n    }\n    EndBlock(this->rpo_number(block));\n  }\n#if DEBUG\n  sequence()->ValidateSSA();\n#endif\n  return std::nullopt;\n}\n\nvoid InstructionSelectorT::StartBlock(RpoNumber rpo) {\n  if (UseInstructionScheduling()) {\n    DCHECK_NOT_NULL(scheduler_);\n    scheduler_->StartBlock(rpo);\n  } else {\n    sequence()->StartBlock(rpo);\n  }\n}\n\nvoid InstructionSelectorT::EndBlock(RpoNumber rpo) {\n  if (UseInstructionScheduling()) {\n    DCHECK_NOT_NULL(scheduler_);\n    scheduler_->EndBlock(rpo);\n  } else {\n    sequence()->EndBlock(rpo);\n  }\n}\n\nvoid InstructionSelectorT::AddTerminator(Instruction* instr) {\n  if (UseInstructionScheduling()) {\n    DCHECK_NOT_NULL(scheduler_);\n    scheduler_->AddTerminator(instr);\n  } else {\n    sequence()->AddInstruction(instr);\n  }\n}\n\nvoid InstructionSelectorT::AddInstruction(Instruction* instr) {\n  if (UseInstructionScheduling()) {\n    DCHECK_NOT_NULL(scheduler_);\n    scheduler_->AddInstruction(instr);\n  } else {\n    sequence()->AddInstruction(instr);\n  }\n}\n\nInstruction* InstructionSelectorT::Emit(InstructionCode opcode,\n                                        InstructionOperand output,\n                                        size_t temp_count,\n                                        InstructionOperand* temps) {\n  size_t output_count = output.IsInvalid() ? 0 : 1;\n  return Emit(opcode, output_count, &output, 0, nullptr, temp_count, temps);\n}\n\nInstruction* InstructionSelectorT::Emit(InstructionCode opcode,\n                                        InstructionOperand output,\n                                        InstructionOperand a, size_t temp_count,\n                                        InstructionOperand* temps) {\n  size_t output_count = output.IsInvalid() ? 0 : 1;\n  return Emit(opcode, output_count, &output, 1, &a, temp_count, temps);\n}\n\nInstruction* InstructionSelectorT::Emit(InstructionCode opcode,\n                                        InstructionOperand output,\n                                        InstructionOperand a,\n                                        InstructionOperand b, size_t temp_count,\n                                        InstructionOperand* temps) {\n  size_t output_count = output.IsInvalid() ? 0 : 1;\n  InstructionOperand inputs[] = {a, b};\n  size_t input_count = arraysize(inputs);\n  return Emit(opcode, output_count, &output, input_count, inputs, temp_count,\n              temps);\n}\n\nInstruction* InstructionSelectorT::Emit(InstructionCode opcode,\n                                        InstructionOperand output,\n                                        InstructionOperand a,\n                                        InstructionOperand b,\n                                        InstructionOperand c, size_t temp_count,\n                                        InstructionOperand* temps) {\n  size_t output_count = output.IsInvalid() ? 0 : 1;\n  InstructionOperand inputs[] = {a, b, c};\n  size_t input_count = arraysize(inputs);\n  return Emit(opcode, output_count, &output, input_count, inputs, temp_count,\n              temps);\n}\n\nInstruction* InstructionSelectorT::Emit(\n    InstructionCode opcode, InstructionOperand output, InstructionOperand a,\n    InstructionOperand b, InstructionOperand c, InstructionOperand d,\n    size_t temp_count, InstructionOperand* temps) {\n  size_t output_count = output.IsInvalid() ? 0 : 1;\n  InstructionOperand inputs[] = {a, b, c, d};\n  size_t input_count = arraysize(inputs);\n  return Emit(opcode, output_count, &output, input_count, inputs, temp_count,\n              temps);\n}\n\nInstruction* InstructionSelectorT::Emit(\n    InstructionCode opcode, InstructionOperand output, InstructionOperand a,\n    InstructionOperand b, InstructionOperand c, InstructionOperand d,\n    InstructionOperand e, size_t temp_count, InstructionOperand* temps) {\n  size_t output_count = output.IsInvalid() ? 0 : 1;\n  InstructionOperand inputs[] = {a, b, c, d, e};\n  size_t input_count = arraysize(inputs);\n  return Emit(opcode, output_count, &output, input_count, inputs, temp_count,\n              temps);\n}\n\nInstruction* InstructionSelectorT::Emit(\n    InstructionCode opcode, InstructionOperand output, InstructionOperand a,\n    InstructionOperand b, InstructionOperand c, InstructionOperand d,\n    InstructionOperand e, InstructionOperand f, size_t temp_count,\n    InstructionOperand* temps) {\n  size_t output_count = output.IsInvalid() ? 0 : 1;\n  InstructionOperand inputs[] = {a, b, c, d, e, f};\n  size_t input_count = arraysize(inputs);\n  return Emit(opcode, output_count, &output, input_count, inputs, temp_count,\n              temps);\n}\n\nInstruction* InstructionSelectorT::Emit(\n    InstructionCode opcode, InstructionOperand output, InstructionOperand a,\n    InstructionOperand b, InstructionOperand c, InstructionOperand d,\n    InstructionOperand e, InstructionOperand f, InstructionOperand g,\n    InstructionOperand h, size_t temp_count, InstructionOperand* temps) {\n  size_t output_count = output.IsInvalid() ? 0 : 1;\n  InstructionOperand inputs[] = {a, b, c, d, e, f, g, h};\n  size_t input_count = arraysize(inputs);\n  return Emit(opcode, output_count, &output, input_count, inputs, temp_count,\n              temps);\n}\n\nInstruction* InstructionSelectorT::Emit(\n    InstructionCode opcode, size_t output_count, InstructionOperand* outputs,\n    size_t input_count, InstructionOperand* inputs, size_t temp_count,\n    InstructionOperand* temps) {\n  if (output_count >= Instruction::kMaxOutputCount ||\n      input_count >= Instruction::kMaxInputCount ||\n      temp_count >= Instruction::kMaxTempCount) {\n    set_instruction_selection_failed();\n    return nullptr;\n  }\n\n  Instruction* instr =\n      Instruction::New(instruction_zone(), opcode, output_count, outputs,\n                       input_count, inputs, temp_count, temps);\n  return Emit(instr);\n}\n\nInstruction* InstructionSelectorT::Emit(Instruction* instr) {\n  instructions_.push_back(instr);\n  return instr;\n}\n\nbool InstructionSelectorT::CanCover(OpIndex user, OpIndex node) const {\n  // 1. Both {user} and {node} must be in the same basic block.\n  if (this->block(schedule(), node) != current_block_) {\n    return false;\n  }\n\n  const Operation& op = this->Get(node);\n  // 2. If node does not produce anything, it can be covered.\n  if (op.Effects().produces.bits() == 0) {\n    return this->is_exclusive_user_of(user, node);\n  }\n\n  // 3. Otherwise, the {node}'s effect level must match the {user}'s.\n  if (GetEffectLevel(node) != current_effect_level_) {\n    return false;\n  }\n\n  // 4. Only {node} must have value edges pointing to {user}.\n  return this->is_exclusive_user_of(user, node);\n}\n\nbool InstructionSelectorT::CanCoverProtectedLoad(OpIndex user,\n                                                 OpIndex node) const {\n  DCHECK(CanCover(user, node));\n  const Graph* graph = this->turboshaft_graph();\n  for (OpIndex next = graph->NextIndex(node); next.valid();\n       next = graph->NextIndex(next)) {\n    if (next == user) break;\n    const Operation& op = graph->Get(next);\n    OpEffects effects = op.Effects();\n    if (effects.produces.control_flow || effects.required_when_unused) {\n      return false;\n    }\n  }\n  return true;\n}\n\nbool InstructionSelectorT::IsOnlyUserOfNodeInSameBlock(OpIndex user,\n                                                       OpIndex node) const {\n  Block* bb_user = this->block(schedule(), user);\n  Block* bb_node = this->block(schedule(), node);\n  if (bb_user != bb_node) return false;\n\n  const Operation& node_op = this->turboshaft_graph()->Get(node);\n  if (node_op.saturated_use_count.Get() == 1) return true;\n  for (OpIndex use : turboshaft_uses(node)) {\n    if (use == user) continue;\n    if (this->block(schedule(), use) == bb_user) return false;\n  }\n    return true;\n}\n\nOptionalOpIndex InstructionSelectorT::FindProjection(OpIndex node,\n                                                     size_t projection_index) {\n  const Graph* graph = this->turboshaft_graph();\n  // Projections are always emitted right after the operation.\n  for (OpIndex next = graph->NextIndex(node); next.valid();\n       next = graph->NextIndex(next)) {\n    const ProjectionOp* projection = graph->Get(next).TryCast<ProjectionOp>();\n    if (projection == nullptr) break;\n    DCHECK(!projection->saturated_use_count.IsZero());\n    if (projection->saturated_use_count.IsOne()) {\n      // If the projection has a single use, it is the following tuple, so we\n      // don't return it, since there is no point in emitting it.\n      DCHECK(turboshaft_uses(next).size() == 1 &&\n             graph->Get(turboshaft_uses(next)[0]).Is<TupleOp>());\n      continue;\n    }\n    if (projection->index == projection_index) return next;\n  }\n\n  // If there is no Projection with index {projection_index} following the\n  // operation, then there shouldn't be any such Projection in the graph. We\n  // verify this in Debug mode.\n#ifdef DEBUG\n  for (OpIndex use : turboshaft_uses(node)) {\n    if (const ProjectionOp* projection =\n            this->Get(use).TryCast<ProjectionOp>()) {\n      DCHECK_EQ(projection->input(), node);\n      if (projection->index == projection_index) {\n        // If we found the projection, it should have a single use: a Tuple\n        // (which doesn't count as a regular use since it is just an artifact of\n        // the Turboshaft graph).\n        DCHECK(turboshaft_uses(use).size() == 1 &&\n               graph->Get(turboshaft_uses(use)[0]).Is<TupleOp>());\n      }\n    }\n  }\n#endif  // DEBUG\n  return OpIndex::Invalid();\n}\n\nvoid InstructionSelectorT::UpdateRenames(Instruction* instruction) {\n  for (size_t i = 0; i < instruction->InputCount(); i++) {\n    TryRename(instruction->InputAt(i));\n  }\n}\n\nvoid InstructionSelectorT::UpdateRenamesInPhi(PhiInstruction* phi) {\n  for (size_t i = 0; i < phi->operands().size(); i++) {\n    int vreg = phi->operands()[i];\n    int renamed = GetRename(vreg);\n    if (vreg != renamed) {\n      phi->RenameInput(i, renamed);\n    }\n  }\n}\n\nint InstructionSelectorT::GetRename(int virtual_register) {\n  int rename = virtual_register;\n  while (true) {\n    if (static_cast<size_t>(rename) >= virtual_register_rename_.size()) break;\n    int next = virtual_register_rename_[rename];\n    if (next == InstructionOperand::kInvalidVirtualRegister) {\n      break;\n    }\n    rename = next;\n  }\n  return rename;\n}\n\nvoid InstructionSelectorT::TryRename(InstructionOperand* op) {\n  if (!op->IsUnallocated()) return;\n  UnallocatedOperand* unalloc = UnallocatedOperand::cast(op);\n  int vreg = unalloc->virtual_register();\n  int rename = GetRename(vreg);\n  if (rename != vreg) {\n    *unalloc = UnallocatedOperand(*unalloc, rename);\n  }\n}\n\nvoid InstructionSelectorT::SetRename(OpIndex node, OpIndex rename) {\n  int vreg = GetVirtualRegister(node);\n  if (static_cast<size_t>(vreg) >= virtual_register_rename_.size()) {\n    int invalid = InstructionOperand::kInvalidVirtualRegister;\n    virtual_register_rename_.resize(vreg + 1, invalid);\n  }\n  virtual_register_rename_[vreg] = GetVirtualRegister(rename);\n}\n\nint InstructionSelectorT::GetVirtualRegister(OpIndex node) {\n  DCHECK(node.valid());\n  size_t const id = this->id(node);\n  DCHECK_LT(id, virtual_registers_.size());\n  int virtual_register = virtual_registers_[id];\n  if (virtual_register == InstructionOperand::kInvalidVirtualRegister) {\n    virtual_register = sequence()->NextVirtualRegister();\n    virtual_registers_[id] = virtual_register;\n  }\n  return virtual_register;\n}\n\nconst std::map<uint32_t, int>\nInstructionSelectorT::GetVirtualRegistersForTesting() const {\n  std::map<uint32_t, int> virtual_registers;\n  for (size_t n = 0; n < virtual_registers_.size(); ++n) {\n    if (virtual_registers_[n] != InstructionOperand::kInvalidVirtualRegister) {\n      const uint32_t id = static_cast<uint32_t>(n);\n      virtual_registers.insert(std::make_pair(id, virtual_registers_[n]));\n    }\n  }\n  return virtual_registers;\n}\n\nbool InstructionSelectorT::IsDefined(OpIndex node) const {\n  DCHECK(node.valid());\n  return defined_.Contains(this->id(node));\n}\n\nvoid InstructionSelectorT::MarkAsDefined(OpIndex node) {\n  DCHECK(node.valid());\n  defined_.Add(this->id(node));\n}\n\nbool InstructionSelectorT::IsUsed(OpIndex node) const {\n  DCHECK(node.valid());\n  if (!ShouldSkipOptimizationStep() && ShouldSkipOperation(this->Get(node))) {\n    return false;\n  }\n  if (this->IsRequiredWhenUnused(node)) return true;\n  return used_.Contains(this->id(node));\n}\n\nbool InstructionSelectorT::IsReallyUsed(OpIndex node) const {\n  DCHECK(node.valid());\n  if (!ShouldSkipOptimizationStep() && ShouldSkipOperation(this->Get(node))) {\n    return false;\n  }\n  return used_.Contains(this->id(node));\n}\n\nvoid InstructionSelectorT::MarkAsUsed(OpIndex node) {\n  DCHECK(node.valid());\n  used_.Add(this->id(node));\n}\n\nint InstructionSelectorT::GetEffectLevel(OpIndex node) const {\n  DCHECK(node.valid());\n  size_t const id = this->id(node);\n  DCHECK_LT(id, effect_level_.size());\n  return effect_level_[id];\n}\n\nint InstructionSelectorT::GetEffectLevel(OpIndex node,\n                                         FlagsContinuation* cont) const {\n  return cont->IsBranch() ? GetEffectLevel(this->block_terminator(\n                                this->PredecessorAt(cont->true_block(), 0)))\n                          : GetEffectLevel(node);\n}\n\nvoid InstructionSelectorT::SetEffectLevel(OpIndex node, int effect_level) {\n  DCHECK(node.valid());\n  size_t const id = this->id(node);\n  DCHECK_LT(id, effect_level_.size());\n  effect_level_[id] = effect_level;\n}\n\nbool InstructionSelectorT::CanAddressRelativeToRootsRegister(\n    const ExternalReference& reference) const {\n  // There are three things to consider here:\n  // 1. CanUseRootsRegister: Is kRootRegister initialized?\n  const bool root_register_is_available_and_initialized = CanUseRootsRegister();\n  if (!root_register_is_available_and_initialized) return false;\n\n  // 2. enable_roots_relative_addressing_: Can we address everything on the heap\n  //    through the root register, i.e. are root-relative addresses to arbitrary\n  //    addresses guaranteed not to change between code generation and\n  //    execution?\n  const bool all_root_relative_offsets_are_constant =\n      (enable_roots_relative_addressing_ ==\n       InstructionSelector::kEnableRootsRelativeAddressing);\n  if (all_root_relative_offsets_are_constant) return true;\n\n  // 3. IsAddressableThroughRootRegister: Is the target address guaranteed to\n  //    have a fixed root-relative offset? If so, we can ignore 2.\n  const bool this_root_relative_offset_is_constant =\n      MacroAssemblerBase::IsAddressableThroughRootRegister(isolate(),\n                                                           reference);\n  return this_root_relative_offset_is_constant;\n}\n\nbool InstructionSelectorT::CanUseRootsRegister() const {\n  return linkage()->GetIncomingDescriptor()->flags() &\n         CallDescriptor::kCanUseRoots;\n}\n\nvoid InstructionSelectorT::MarkAsRepresentation(MachineRepresentation rep,\n                                                const InstructionOperand& op) {\n  UnallocatedOperand unalloc = UnallocatedOperand::cast(op);\n  sequence()->MarkAsRepresentation(rep, unalloc.virtual_register());\n}\n\nvoid InstructionSelectorT::MarkAsRepresentation(MachineRepresentation rep,\n                                                OpIndex node) {\n  sequence()->MarkAsRepresentation(rep, GetVirtualRegister(node));\n}\n\nnamespace {\n\nInstructionOperand OperandForDeopt(Isolate* isolate, OperandGeneratorT* g,\n                                   OpIndex input, FrameStateInputKind kind,\n                                   MachineRepresentation rep) {\n  if (rep == MachineRepresentation::kNone) {\n    return g->TempImmediate(FrameStateDescriptor::kImpossibleValue);\n  }\n\n  const Operation& op = g->turboshaft_graph()->Get(input);\n  if (const ConstantOp* constant = op.TryCast<ConstantOp>()) {\n    using Kind = ConstantOp::Kind;\n    switch (constant->kind) {\n      case Kind::kWord32:\n      case Kind::kWord64:\n      case Kind::kSmi:\n      case Kind::kFloat32:\n      case Kind::kFloat64:\n        return g->UseImmediate(input);\n      case Kind::kNumber:\n        if (rep == MachineRepresentation::kWord32) {\n          const double d = constant->number().get_scalar();\n          Tagged<Smi> smi = Smi::FromInt(static_cast<int32_t>(d));\n          CHECK_EQ(smi.value(), d);\n          return g->UseImmediate(static_cast<int32_t>(smi.ptr()));\n        }\n        return g->UseImmediate(input);\n      case Kind::kHeapObject:\n      case Kind::kCompressedHeapObject:\n      case Kind::kTrustedHeapObject: {\n        if (!CanBeTaggedOrCompressedPointer(rep)) {\n          // If we have inconsistent static and dynamic types, e.g. if we\n          // smi-check a string, we can get here with a heap object that\n          // says it is a smi. In that case, we return an invalid instruction\n          // operand, which will be interpreted as an optimized-out value.\n\n          // TODO(jarin) Ideally, we should turn the current instruction\n          // into an abort (we should never execute it).\n          return InstructionOperand();\n        }\n\n        Handle<HeapObject> object = constant->handle();\n        RootIndex root_index;\n        if (isolate->roots_table().IsRootHandle(object, &root_index) &&\n            root_index == RootIndex::kOptimizedOut) {\n          // For an optimized-out object we return an invalid instruction\n          // operand, so that we take the fast path for optimized-out values.\n          return InstructionOperand();\n        }\n\n        return g->UseImmediate(input);\n      }\n      default:\n        UNIMPLEMENTED();\n    }\n  } else if (const TaggedBitcastOp* bitcast =\n                 op.TryCast<Opmask::kTaggedBitcastSmi>()) {\n    const Operation& bitcast_input = g->Get(bitcast->input());\n    if (const ConstantOp* cst =\n            bitcast_input.TryCast<Opmask::kWord32Constant>()) {\n      if constexpr (Is64()) {\n        return g->UseImmediate64(cst->word32());\n      } else {\n        return g->UseImmediate(cst->word32());\n      }\n    } else if (Is64() && bitcast_input.Is<Opmask::kWord64Constant>()) {\n      if (rep == MachineRepresentation::kWord32) {\n        return g->UseImmediate(bitcast_input.Cast<ConstantOp>().word32());\n      } else {\n        return g->UseImmediate64(bitcast_input.Cast<ConstantOp>().word64());\n      }\n    }\n  }\n\n  switch (kind) {\n    case FrameStateInputKind::kStackSlot:\n      return g->UseUniqueSlot(input);\n    case FrameStateInputKind::kAny:\n      // Currently deopts \"wrap\" other operations, so the deopt's inputs\n      // are potentially needed until the end of the deoptimising code.\n      return g->UseAnyAtEnd(input);\n  }\n}\n\n}  // namespace\n\nclass TurbofanStateObjectDeduplicator {\n public:\n  explicit TurbofanStateObjectDeduplicator(Zone* zone) : objects_(zone) {}\n  static const size_t kNotDuplicated = SIZE_MAX;\n\n  size_t GetObjectId(Node* node) {\n    DCHECK(node->opcode() == IrOpcode::kTypedObjectState ||\n           node->opcode() == IrOpcode::kObjectId ||\n           node->opcode() == IrOpcode::kArgumentsElementsState);\n    for (size_t i = 0; i < objects_.size(); ++i) {\n      if (objects_[i] == node) return i;\n      // ObjectId nodes are the Turbofan way to express objects with the same\n      // identity in the deopt info. So they should always be mapped to\n      // previously appearing TypedObjectState nodes.\n      if (HasObjectId(objects_[i]) && HasObjectId(node) &&\n          ObjectIdOf(objects_[i]->op()) == ObjectIdOf(node->op())) {\n        return i;\n      }\n    }\n    DCHECK(node->opcode() == IrOpcode::kTypedObjectState ||\n           node->opcode() == IrOpcode::kArgumentsElementsState);\n    return kNotDuplicated;\n  }\n\n  size_t InsertObject(Node* node) {\n    DCHECK(node->opcode() == IrOpcode::kTypedObjectState ||\n           node->opcode() == IrOpcode::kObjectId ||\n           node->opcode() == IrOpcode::kArgumentsElementsState);\n    size_t id = objects_.size();\n    objects_.push_back(node);\n    return id;\n  }\n\n  size_t size() const { return objects_.size(); }\n\n private:\n  static bool HasObjectId(Node* node) {\n    return node->opcode() == IrOpcode::kTypedObjectState ||\n           node->opcode() == IrOpcode::kObjectId;\n  }\n\n  ZoneVector<Node*> objects_;\n};\n\nenum class ObjectType { kRegularObject, kStringConcat };\n\nclass TurboshaftStateObjectDeduplicator {\n public:\n  explicit TurboshaftStateObjectDeduplicator(Zone* zone)\n      : objects_ids_mapping_(zone), string_ids_mapping_(zone) {}\n  static constexpr size_t kNotDuplicated = std::numeric_limits<size_t>::max();\n\n  size_t GetObjectId(uint32_t old_id, ObjectType type) {\n    auto& ids_map = GetMapForType(type);\n    auto it = ids_map.find(old_id);\n    if (it == ids_map.end()) return kNotDuplicated;\n    return it->second;\n  }\n\n  size_t InsertObject(uint32_t old_id, ObjectType type) {\n    auto& ids_map = GetMapForType(type);\n    uint32_t new_id = next_id_++;\n    ids_map.insert({old_id, new_id});\n    return new_id;\n  }\n\n  void InsertDummyForArgumentsElements() { next_id_++; }\n\n private:\n  ZoneAbslFlatHashMap<uint32_t, uint32_t>& GetMapForType(ObjectType type) {\n    switch (type) {\n      case ObjectType::kRegularObject:\n        return objects_ids_mapping_;\n      case ObjectType::kStringConcat:\n        return string_ids_mapping_;\n    }\n  }\n  uint32_t next_id_ = 0;\n\n  ZoneAbslFlatHashMap<uint32_t, uint32_t> objects_ids_mapping_;\n  ZoneAbslFlatHashMap<uint32_t, uint32_t> string_ids_mapping_;\n};\n\nstruct InstructionSelectorT::CachedStateValues : public ZoneObject {\n public:\n  CachedStateValues(Zone* zone, StateValueList* values, size_t values_start,\n                    InstructionOperandVector* inputs, size_t inputs_start)\n      : inputs_(inputs->begin() + inputs_start, inputs->end(), zone),\n        values_(values->MakeSlice(values_start)) {}\n\n  size_t Emit(InstructionOperandVector* inputs, StateValueList* values) {\n    inputs->insert(inputs->end(), inputs_.begin(), inputs_.end());\n    values->PushCachedSlice(values_);\n    return inputs_.size();\n  }\n\n private:\n  Instruction"
}