{
  "file_path": "/home/kathirks_gc/v8_go/archive/codebase/src/compiler/turboshaft/wasm-lowering-reducer.h",
  "error": "Response not JSON and not XML-like after cleanup",
  "json_error_if_any": "Skipped JSON parsing for whole response due to presence of XML tags; XML is primary.",
  "raw_response": "```xml\n<file>\n    <metadata>\n        {\n            \"path\": \"/home/kathirks_gc/v8_go/archive/codebase/src/compiler/turboshaft/wasm-lowering-reducer.h\",\n            \"file_name\": \"wasm-lowering-reducer.h\",\n            \"language\": \"cpp\",\n            \"purpose\": \"Defines the WasmLoweringReducer class for lowering WebAssembly operations in the Turboshaft compiler pipeline.\"\n        }\n    </metadata>\n    <imports>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"purpose\": \"Includes necessary headers for compiler, turboshaft, and wasm functionalities.\"\n            }\n        </metadata>\n        <code><![CDATA[\n#include \"src/compiler/turboshaft/builtin-call-descriptors.h\"\n#if !V8_ENABLE_WEBASSEMBLY\n#error This header should only be included if WebAssembly is enabled.\n#endif  // !V8_ENABLE_WEBASSEMBLY\n\n#include \"src/compiler/globals.h\"\n#include \"src/compiler/turboshaft/assembler.h\"\n#include \"src/compiler/turboshaft/index.h\"\n#include \"src/compiler/turboshaft/operations.h\"\n#include \"src/compiler/turboshaft/phase.h\"\n#include \"src/compiler/turboshaft/wasm-assembler-helpers.h\"\n#include \"src/wasm/wasm-engine.h\"\n#include \"src/wasm/wasm-module.h\"\n#include \"src/wasm/wasm-objects.h\"\n#include \"src/wasm/wasm-subtyping.h\"\n        ]]></code>\n    </imports>\n    <class>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"type\": \"class\",\n                \"name\": \"WasmLoweringReducer\",\n                \"about\": \"Reducer class for lowering WebAssembly specific operations to more generic Turboshaft operations.\",\n                \"attributes\": [],\n                \"dependencies\": [\n                    \"Next\",\n                    \"v8::internal::compiler::turboshaft::Assembler\",\n                    \"wasm::WasmGlobal\",\n                    \"wasm::ValueType\",\n                    \"wasm::StructType\",\n                    \"wasm::ArrayType\",\n                    \"wasm::WasmModule\"\n                ]\n            }\n        </metadata>\n        <code><![CDATA[\ntemplate <class Next>\nclass WasmLoweringReducer : public Next {\n public:\n  TURBOSHAFT_REDUCER_BOILERPLATE(WasmLowering)\n\n  V<Any> REDUCE(GlobalGet)(V<WasmTrustedInstanceData> instance,\n                           const wasm::WasmGlobal* global) {\n    return LowerGlobalSetOrGet(instance, OpIndex::Invalid(), global,\n                               GlobalMode::kLoad);\n  }\n\n  OpIndex REDUCE(GlobalSet)(V<WasmTrustedInstanceData> instance, V<Any> value,\n                            const wasm::WasmGlobal* global) {\n    return LowerGlobalSetOrGet(instance, value, global, GlobalMode::kStore);\n  }\n\n  OpIndex REDUCE(RootConstant)(RootIndex index) {\n    OpIndex roots = __ LoadRootRegister();\n    // We load the value as a pointer here and not as a TaggedPointer because\n    // it is stored uncompressed in the IsolateData, and a load of a\n    // TaggedPointer loads compressed pointers.\n#if V8_TARGET_BIG_ENDIAN\n    // On big endian a full pointer load is needed as otherwise the wrong half\n    // of the 64 bit address is loaded.\n    return __ BitcastWordPtrToTagged(__ Load(\n        roots, LoadOp::Kind::RawAligned().Immutable(),\n        MemoryRepresentation::UintPtr(), IsolateData::root_slot_offset(index)));\n#else\n    // On little endian a tagged load is enough and saves the bitcast.\n    return __ Load(roots, LoadOp::Kind::RawAligned().Immutable(),\n                   MemoryRepresentation::TaggedPointer(),\n                   IsolateData::root_slot_offset(index));\n#endif\n  }\n\n  V<Word32> REDUCE(IsRootConstant)(OpIndex object, RootIndex index) {\n#if V8_STATIC_ROOTS_BOOL\n    if (RootsTable::IsReadOnly(index)) {\n      V<Object> root = V<Object>::Cast(__ UintPtrConstant(\n          StaticReadOnlyRootsPointerTable[static_cast<size_t>(index)]));\n      return __ TaggedEqual(object, root);\n    }\n#endif\n    return __ TaggedEqual(object, __ RootConstant(index));\n  }\n\n  OpIndex REDUCE(Null)(wasm::ValueType type) {\n    RootIndex index =\n        type.use_wasm_null() ? RootIndex::kWasmNull : RootIndex::kNullValue;\n    return ReduceRootConstant(index);\n  }\n\n  V<Word32> REDUCE(IsNull)(OpIndex object, wasm::ValueType type) {\n    RootIndex index =\n        type.use_wasm_null() ? RootIndex::kWasmNull : RootIndex::kNullValue;\n    return ReduceIsRootConstant(object, index);\n  }\n\n  V<Object> REDUCE(AssertNotNull)(V<Object> object, wasm::ValueType type,\n                                  TrapId trap_id) {\n    if (trap_id == TrapId::kTrapNullDereference) {\n      // Skip the check altogether if null checks are turned off.\n      if (!v8_flags.experimental_wasm_skip_null_checks) {\n        // Use an explicit null check if\n        // (1) we cannot use trap handler or\n        // (2) the object might be a Smi or\n        // (3) the object might be a JS object.\n        if (null_check_strategy_ == NullCheckStrategy::kExplicit ||\n            wasm::IsSubtypeOf(wasm::kWasmI31Ref.AsNonNull(), type, module_) ||\n            !type.use_wasm_null()) {\n          __ TrapIf(__ IsNull(object, type), trap_id);\n        } else {\n          // Otherwise, load the word after the map word.\n          static_assert(WasmStruct::kHeaderSize > kTaggedSize);\n          static_assert(WasmArray::kHeaderSize > kTaggedSize);\n          static_assert(WasmInternalFunction::kHeaderSize > kTaggedSize);\n          __ Load(object, LoadOp::Kind::TrapOnNull().Immutable(),\n                  MemoryRepresentation::Int32(), kTaggedSize);\n        }\n      }\n    } else {\n      __ TrapIf(__ IsNull(object, type), trap_id);\n    }\n    return object;\n  }\n\n  V<Map> REDUCE(RttCanon)(V<FixedArray> rtts,\n                          wasm::ModuleTypeIndex type_index) {\n    int map_offset =\n        OFFSET_OF_DATA_START(FixedArray) + type_index.index * kTaggedSize;\n    return __ Load(rtts, LoadOp::Kind::TaggedBase().Immutable(),\n                   MemoryRepresentation::AnyTagged(), map_offset);\n  }\n\n  V<Word32> REDUCE(WasmTypeCheck)(V<Object> object, OptionalV<Map> rtt,\n                                  WasmTypeCheckConfig config) {\n    if (rtt.has_value()) {\n      return ReduceWasmTypeCheckRtt(object, rtt, config);\n    } else {\n      return ReduceWasmTypeCheckAbstract(object, config);\n    }\n  }\n\n  V<Object> REDUCE(WasmTypeCast)(V<Object> object, OptionalV<Map> rtt,\n                                 WasmTypeCheckConfig config) {\n    if (rtt.has_value()) {\n      return ReduceWasmTypeCastRtt(object, rtt, config);\n    } else {\n      return ReduceWasmTypeCastAbstract(object, config);\n    }\n  }\n\n  V<Object> REDUCE(AnyConvertExtern)(V<Object> object) {\n    Label<Object> end_label(&Asm());\n    Label<> null_label(&Asm());\n    Label<> smi_label(&Asm());\n    Label<> int_to_smi_label(&Asm());\n    Label<> heap_number_label(&Asm());\n\n    constexpr int32_t kInt31MaxValue = 0x3fffffff;\n    constexpr int32_t kInt31MinValue = -kInt31MaxValue - 1;\n\n    GOTO_IF(__ IsNull(object, wasm::kWasmExternRef), null_label);\n    GOTO_IF(__ IsSmi(object), smi_label);\n    GOTO_IF(__ HasInstanceType(object, HEAP_NUMBER_TYPE), heap_number_label);\n    // For anything else, just pass through the value.\n    GOTO(end_label, object);\n\n    BIND(null_label);\n    GOTO(end_label, __ Null(wasm::kWasmAnyRef));\n\n    // Canonicalize SMI.\n    BIND(smi_label);\n    if constexpr (SmiValuesAre31Bits()) {\n      GOTO(end_label, object);\n    } else {\n      Label<> convert_to_heap_number_label(&Asm());\n      V<Word32> int_value = __ UntagSmi(V<Smi>::Cast(object));\n\n      // Convert to heap number if the int32 does not fit into an i31ref.\n      GOTO_IF(__ Int32LessThan(__ Word32Constant(kInt31MaxValue), int_value),\n              convert_to_heap_number_label);\n      GOTO_IF(__ Int32LessThan(int_value, __ Word32Constant(kInt31MinValue)),\n              convert_to_heap_number_label);\n      GOTO(end_label, object);\n\n      BIND(convert_to_heap_number_label);\n      V<Object> heap_number = __ template WasmCallBuiltinThroughJumptable<\n          BuiltinCallDescriptor::WasmInt32ToHeapNumber>({int_value});\n      GOTO(end_label, heap_number);\n    }\n\n    // Convert HeapNumber to SMI if possible.\n    BIND(heap_number_label);\n    V<Float64> float_value =\n        __ LoadHeapNumberValue(V<HeapNumber>::Cast(object));\n    // Check range of float value.\n    GOTO_IF(__ Float64LessThan(float_value, __ Float64Constant(kInt31MinValue)),\n            end_label, object);\n    GOTO_IF(__ Float64LessThan(__ Float64Constant(kInt31MaxValue), float_value),\n            end_label, object);\n    // Check if value is -0.\n    V<Word32> is_minus_zero;\n    if constexpr (Is64()) {\n      V<Word64> minus_zero = __ Word64Constant(kMinusZeroBits);\n      V<Word64> float_bits = __ BitcastFloat64ToWord64(float_value);\n      is_minus_zero = __ Word64Equal(float_bits, minus_zero);\n    } else {\n      Label<Word32> done(&Asm());\n\n      V<Word32> value_lo = __ Float64ExtractLowWord32(float_value);\n      GOTO_IF_NOT(__ Word32Equal(value_lo, __ Word32Constant(kMinusZeroLoBits)),\n                  done, __ Word32Constant(0));\n      V<Word32> value_hi = __ Float64ExtractHighWord32(float_value);\n      GOTO(done, __ Word32Equal(value_hi, __ Word32Constant(kMinusZeroHiBits)));\n      BIND(done, phi_is_minus_zero);\n      is_minus_zero = phi_is_minus_zero;\n    }\n    GOTO_IF(is_minus_zero, end_label, object);\n    // Check if value is integral.\n    V<Word32> int_value =\n        __ TruncateFloat64ToInt32OverflowUndefined(float_value);\n    GOTO_IF(__ Float64Equal(float_value, __ ChangeInt32ToFloat64(int_value)),\n            int_to_smi_label);\n    GOTO(end_label, object);\n\n    BIND(int_to_smi_label);\n    GOTO(end_label, __ TagSmi(int_value));\n\n    BIND(end_label, result);\n    return result;\n  }\n\n  V<Object> REDUCE(ExternConvertAny)(V<Object> object) {\n    Label<Object> end(&Asm());\n    GOTO_IF_NOT(__ IsNull(object, wasm::kWasmAnyRef), end, object);\n    GOTO(end, __ Null(wasm::kWasmExternRef));\n    BIND(end, result);\n    return result;\n  }\n\n  V<Object> REDUCE(WasmTypeAnnotation)(V<Object> value, wasm::ValueType type) {\n    // Remove type annotation operations as they are not needed any more.\n    return value;\n  }\n\n  V<Any> REDUCE(StructGet)(V<WasmStructNullable> object,\n                           const wasm::StructType* type,\n                           wasm::ModuleTypeIndex type_index, int field_index,\n                           bool is_signed, CheckForNull null_check) {\n    auto [explicit_null_check, implicit_null_check] =\n        null_checks_for_struct_op(null_check, field_index);\n\n    if (explicit_null_check) {\n      __ TrapIf(__ IsNull(object, wasm::kWasmAnyRef),\n                TrapId::kTrapNullDereference);\n    }\n\n    LoadOp::Kind load_kind = implicit_null_check ? LoadOp::Kind::TrapOnNull()\n                                                 : LoadOp::Kind::TaggedBase();\n    if (!type->mutability(field_index)) {\n      load_kind = load_kind.Immutable();\n    }\n    MemoryRepresentation repr =\n        RepresentationFor(type->field(field_index), is_signed);\n\n    return __ Load(object, load_kind, repr, field_offset(type, field_index));\n  }\n\n  V<None> REDUCE(StructSet)(V<WasmStructNullable> object, V<Any> value,\n                            const wasm::StructType* type,\n                            wasm::ModuleTypeIndex type_index, int field_index,\n                            CheckForNull null_check) {\n    auto [explicit_null_check, implicit_null_check] =\n        null_checks_for_struct_op(null_check, field_index);\n\n    if (explicit_null_check) {\n      __ TrapIf(__ IsNull(object, wasm::kWasmAnyRef),\n                TrapId::kTrapNullDereference);\n    }\n\n    StoreOp::Kind store_kind = implicit_null_check\n                                   ? StoreOp::Kind::TrapOnNull()\n                                   : StoreOp::Kind::TaggedBase();\n    MemoryRepresentation repr =\n        RepresentationFor(type->field(field_index), true);\n\n    __ Store(object, value, store_kind, repr,\n             type->field(field_index).is_reference() ? kFullWriteBarrier\n                                                     : kNoWriteBarrier,\n             field_offset(type, field_index));\n\n    return OpIndex::Invalid();\n  }\n\n  V<Any> REDUCE(ArrayGet)(V<WasmArrayNullable> array, V<Word32> index,\n                          const wasm::ArrayType* array_type, bool is_signed) {\n    bool is_mutable = array_type->mutability();\n    LoadOp::Kind load_kind = is_mutable\n                                 ? LoadOp::Kind::TaggedBase()\n                                 : LoadOp::Kind::TaggedBase().Immutable();\n    return __ Load(array, __ ChangeInt32ToIntPtr(index), load_kind,\n                   RepresentationFor(array_type->element_type(), is_signed),\n                   WasmArray::kHeaderSize,\n                   array_type->element_type().value_kind_size_log2());\n  }\n\n  V<None> REDUCE(ArraySet)(V<WasmArrayNullable> array, V<Word32> index,\n                           V<Any> value, wasm::ValueType element_type) {\n    __ Store(array, __ ChangeInt32ToIntPtr(index), value,\n             LoadOp::Kind::TaggedBase(), RepresentationFor(element_type, true),\n             element_type.is_reference() ? kFullWriteBarrier : kNoWriteBarrier,\n             WasmArray::kHeaderSize, element_type.value_kind_size_log2());\n    return {};\n  }\n\n  V<Word32> REDUCE(ArrayLength)(V<WasmArrayNullable> array,\n                                CheckForNull null_check) {\n    bool explicit_null_check =\n        null_check == kWithNullCheck &&\n        null_check_strategy_ == NullCheckStrategy::kExplicit;\n    bool implicit_null_check =\n        null_check == kWithNullCheck &&\n        null_check_strategy_ == NullCheckStrategy::kTrapHandler;\n\n    if (explicit_null_check) {\n      __ TrapIf(__ IsNull(array, wasm::kWasmAnyRef),\n                TrapId::kTrapNullDereference);\n    }\n\n    LoadOp::Kind load_kind = implicit_null_check\n                                 ? LoadOp::Kind::TrapOnNull().Immutable()\n                                 : LoadOp::Kind::TaggedBase().Immutable();\n\n    return __ Load(array, load_kind, RepresentationFor(wasm::kWasmI32, true),\n                   WasmArray::kLengthOffset);\n  }\n\n  V<WasmArray> REDUCE(WasmAllocateArray)(V<Map> rtt, V<Word32> length,\n                                         const wasm::ArrayType* array_type) {\n    __ TrapIfNot(\n        __ Uint32LessThanOrEqual(\n            length, __ Word32Constant(WasmArray::MaxLength(array_type))),\n        TrapId::kTrapArrayTooLarge);\n    wasm::ValueType element_type = array_type->element_type();\n\n    // RoundUp(length * value_size, kObjectAlignment) =\n    //   RoundDown(length * value_size + kObjectAlignment - 1,\n    //             kObjectAlignment);\n    V<Word32> padded_length = __ Word32BitwiseAnd(\n        __ Word32Add(__ Word32Mul(length, __ Word32Constant(\n                                              element_type.value_kind_size())),\n                     __ Word32Constant(int32_t{kObjectAlignment - 1})),\n        __ Word32Constant(int32_t{-kObjectAlignment}));\n    Uninitialized<WasmArray> a = __ template Allocate<WasmArray>(\n        __ ChangeUint32ToUintPtr(__ Word32Add(\n            padded_length, __ Word32Constant(WasmArray::kHeaderSize))),\n        AllocationType::kYoung);\n\n    // TODO(14108): The map and empty fixed array initialization should be an\n    // immutable store.\n    __ InitializeField(a, AccessBuilder::ForMap(compiler::kNoWriteBarrier),\n                       rtt);\n    __ InitializeField(a, AccessBuilder::ForJSObjectPropertiesOrHash(),\n                       LOAD_ROOT(EmptyFixedArray));\n    __ InitializeField(a, AccessBuilder::ForWasmArrayLength(), length);\n\n    // Note: Only the array header initialization is finished here, the elements\n    // still need to be initialized by other code.\n    V<WasmArray> array = __ FinishInitialization(std::move(a));\n    return array;\n  }\n\n  V<WasmStruct> REDUCE(WasmAllocateStruct)(\n      V<Map> rtt, const wasm::StructType* struct_type) {\n    int size = WasmStruct::Size(struct_type);\n    Uninitialized<WasmStruct> s =\n        __ template Allocate<WasmStruct>(size, AllocationType::kYoung);\n    __ InitializeField(s, AccessBuilder::ForMap(compiler::kNoWriteBarrier),\n                       rtt);\n    __ InitializeField(s, AccessBuilder::ForJSObjectPropertiesOrHash(),\n                       LOAD_ROOT(EmptyFixedArray));\n    // Note: Struct initialization isn't finished here, the user defined fields\n    // still need to be initialized by other operations.\n    V<WasmStruct> struct_value = __ FinishInitialization(std::move(s));\n    return struct_value;\n  }\n\n  V<WasmFuncRef> REDUCE(WasmRefFunc)(V<WasmTrustedInstanceData> wasm_instance,\n                                     uint32_t function_index) {\n    V<FixedArray> func_refs = LOAD_IMMUTABLE_INSTANCE_FIELD(\n        wasm_instance, FuncRefs, MemoryRepresentation::TaggedPointer());\n    V<Object> maybe_func_ref =\n        __ LoadFixedArrayElement(func_refs, function_index);\n\n    Label<WasmFuncRef> done(&Asm());\n    IF (UNLIKELY(__ IsSmi(maybe_func_ref))) {\n      bool extract_shared_data =\n          !shared_ && module_->function_is_shared(function_index);\n\n      V<WasmFuncRef> from_builtin = __ template WasmCallBuiltinThroughJumptable<\n          BuiltinCallDescriptor::WasmRefFunc>(\n          {__ Word32Constant(function_index),\n           __ Word32Constant(extract_shared_data ? 1 : 0)});\n\n      GOTO(done, from_builtin);\n    } ELSE {\n      GOTO(done, V<WasmFuncRef>::Cast(maybe_func_ref));\n    }\n\n    BIND(done, result_value);\n    return result_value;\n  }\n\n  V<String> REDUCE(StringAsWtf16)(V<String> string) {\n    Label<String> done(&Asm());\n    V<Word32> instance_type = __ LoadInstanceTypeField(__ LoadMapField(string));\n    V<Word32> string_representation = __ Word32BitwiseAnd(\n        instance_type, __ Word32Constant(kStringRepresentationMask));\n    GOTO_IF(__ Word32Equal(string_representation, kSeqStringTag), done, string);\n\n    GOTO(done, __ template WasmCallBuiltinThroughJumptable<\n                   BuiltinCallDescriptor::WasmStringAsWtf16>({string}));\n    BIND(done, result);\n    return result;\n  }\n\n  OpIndex REDUCE(StringPrepareForGetCodeUnit)(V<Object> original_string) {\n    LoopLabel<Object /*string*/, Word32 /*instance type*/, Word32 /*offset*/>\n        dispatch(&Asm());\n    Label<Object /*string*/, Word32 /*instance type*/, Word32 /*offset*/>\n        direct_string(&Asm());\n\n    // These values will be used to replace the original node's projections.\n    // The first, \"string\", is either a SeqString or Tagged<Smi>(0) (in case of\n    // external string). Notably this makes it GC-safe: if that string moves,\n    // this pointer will be updated accordingly. The second, \"offset\", has full\n    // register width so that it can be used to store external pointers: for\n    // external strings, we add up the character backing store's base address\n    // and any slice offset. The third, \"character width\", is a shift width,\n    // i.e. it is 0 for one-byte strings, 1 for two-byte strings,\n    // kCharWidthBailoutSentinel for uncached external strings (for which\n    // \"string\"/\"offset\" are invalid and unusable).\n    Label<Object /*string*/, WordPtr /*offset*/, Word32 /*character width*/>\n        done(&Asm());\n\n    V<Word32> original_type =\n        __ LoadInstanceTypeField(__ LoadMapField(original_string));\n    GOTO(dispatch, original_string, original_type, __ Word32Constant(0));\n\n    BIND_LOOP(dispatch, string, instance_type, offset) {\n      Label<> thin_string(&Asm());\n      Label<> cons_string(&Asm());\n\n      static_assert(kIsIndirectStringTag == 1);\n      static constexpr int kIsDirectStringTag = 0;\n      GOTO_IF(__ Word32Equal(\n                  __ Word32BitwiseAnd(instance_type, kIsIndirectStringMask),\n                  kIsDirectStringTag),\n              direct_string, string, instance_type, offset);\n\n      // Handle indirect strings.\n      V<Word32> string_representation =\n          __ Word32BitwiseAnd(instance_type, kStringRepresentationMask);\n      GOTO_IF(__ Word32Equal(string_representation, kThinStringTag),\n              thin_string);\n      GOTO_IF(__ Word32Equal(string_representation, kConsStringTag),\n              cons_string);\n\n      // Sliced string.\n      V<Word32> new_offset = __ Word32Add(\n          offset, __ UntagSmi(__ template LoadField<Smi>(\n                      string, AccessBuilder::ForSlicedStringOffset())));\n      V<Object> parent = __ template LoadField<Object>(\n          string, AccessBuilder::ForSlicedStringParent());\n      V<Word32> parent_type = __ LoadInstanceTypeField(__ LoadMapField(parent));\n      GOTO(dispatch, parent, parent_type, new_offset);\n\n      // Thin string.\n      BIND(thin_string);\n      V<Object> actual = __ template LoadField<Object>(\n          string, AccessBuilder::ForThinStringActual());\n      V<Word32> actual_type = __ LoadInstanceTypeField(__ LoadMapField(actual));\n      // ThinStrings always reference (internalized) direct strings.\n      GOTO(direct_string, actual, actual_type, offset);\n\n      // Flat cons string. (Non-flat cons strings are ruled out by\n      // string.as_wtf16.)\n      BIND(cons_string);\n      V<Object> first = __ template LoadField<Object>(\n          string, AccessBuilder::ForConsStringFirst());\n      V<Word32> first_type = __ LoadInstanceTypeField(__ LoadMapField(first));\n      GOTO(dispatch, first, first_type, offset);\n    }\n    {\n      BIND(direct_string, string, instance_type, offset);\n\n      V<Word32> is_onebyte =\n          __ Word32BitwiseAnd(instance_type, kStringEncodingMask);\n      // Char width shift is 1 - (is_onebyte).\n      static_assert(kStringEncodingMask == 1 << 3);\n      V<Word32> charwidth_shift =\n          __ Word32Sub(1, __ Word32ShiftRightLogical(is_onebyte, 3));\n\n      Label<> external(&Asm());\n      V<Word32> string_representation =\n          __ Word32BitwiseAnd(instance_type, kStringRepresentationMask);\n      GOTO_IF(__ Word32Equal(string_representation, kExternalStringTag),\n              external);\n\n      // Sequential string.\n      DCHECK_EQ(AccessBuilder::ForSeqOneByteStringCharacter().header_size,\n                AccessBuilder::ForSeqTwoByteStringCharacter().header_size);\n      const int chars_start_offset =\n          AccessBuilder::ForSeqOneByteStringCharacter().header_size;\n      V<Word32> final_offset =\n          __ Word32Add(chars_start_offset - kHeapObjectTag,\n                       __ Word32ShiftLeft(offset, charwidth_shift));\n      GOTO(done, string, __ ChangeInt32ToIntPtr(final_offset), charwidth_shift);\n\n      // External string.\n      BIND(external);\n      GOTO_IF(__ Word32BitwiseAnd(instance_type, kUncachedExternalStringMask),\n              done, string, /*offset*/ 0, kCharWidthBailoutSentinel);\n      FieldAccess field_access = AccessBuilder::ForExternalStringResourceData();\n      V<WordPtr> resource = __ LoadExternalPointerFromObject(\n          string, field_access.offset, field_access.external_pointer_tag);\n      V<Word32> shifted_offset = __ Word32ShiftLeft(offset, charwidth_shift);\n      V<WordPtr> final_offset_external =\n          __ WordPtrAdd(resource, __ ChangeInt32ToIntPtr(shifted_offset));\n      GOTO(done, __ SmiConstant(Smi::FromInt(0)), final_offset_external,\n           charwidth_shift);\n    }\n    {\n      BIND(done, base, final_offset, charwidth_shift);\n      return __ Tuple({base, final_offset, charwidth_shift});\n    }\n  }\n\n private:\n  enum class GlobalMode { kLoad, kStore };\n\n  static constexpr MemoryRepresentation kMaybeSandboxedPointer =\n      V8_ENABLE_SANDBOX_BOOL ? MemoryRepresentation::SandboxedPointer()\n                             : MemoryRepresentation::UintPtr();\n\n  MemoryRepresentation RepresentationFor(wasm::ValueType type, bool is_signed) {\n    switch (type.kind()) {\n      case wasm::kI8:\n        return is_signed ? MemoryRepresentation::Int8()\n                         : MemoryRepresentation::Uint8();\n      case wasm::kI16:\n        return is_signed ? MemoryRepresentation::Int16()\n                         : MemoryRepresentation::Uint16();\n      case wasm::kI32:\n        return is_signed ? MemoryRepresentation::Int32()\n                         : MemoryRepresentation::Uint32();\n      case wasm::kI64:\n        return is_signed ? MemoryRepresentation::Int64()\n                         : MemoryRepresentation::Uint64();\n      case wasm::kF16:\n        return MemoryRepresentation::Float16();\n      case wasm::kF32:\n        return MemoryRepresentation::Float32();\n      case wasm::kF64:\n        return MemoryRepresentation::Float64();\n      case wasm::kS128:\n        return MemoryRepresentation::Simd128();\n      case wasm::kRef:\n      case wasm::kRefNull:\n        return MemoryRepresentation::AnyTagged();\n      case wasm::kVoid:\n      case wasm::kTop:\n      case wasm::kBottom:\n        UNREACHABLE();\n    }\n  }\n\n  V<Word32> ReduceWasmTypeCheckAbstract(V<Object> object,\n                                        WasmTypeCheckConfig config) {\n    const bool object_can_be_null = config.from.is_nullable();\n    const bool null_succeeds = config.to.is_nullable();\n    const bool object_can_be_i31 =\n        wasm::IsSubtypeOf(wasm::kWasmI31Ref.AsNonNull(), config.from,\n                          module_) ||\n        config.from.heap_representation() == wasm::HeapType::kExtern;\n\n    V<Word32> result;\n    Label<Word32> end_label(&Asm());\n\n    wasm::HeapType::Representation to_rep = config.to.heap_representation();\n    do {\n      // The none-types only perform a null check. They need no control flow.\n      if (to_rep == wasm::HeapType::kNone ||\n          to_rep == wasm::HeapType::kNoExtern ||\n          to_rep == wasm::HeapType::kNoFunc ||\n          to_rep == wasm::HeapType::kNoExn) {\n        result = __ IsNull(object, config.from);\n        break;\n      }\n      // Null checks performed by any other type check need control flow. We can\n      // skip the null check if null fails, because it's covered by the Smi\n      // check or instance type check we'll do later.\n      if (object_can_be_null && null_succeeds) {\n        const int kResult = 1;\n        GOTO_IF(UNLIKELY(__ IsNull(object, wasm::kWasmAnyRef)), end_label,\n                __ Word32Constant(kResult));\n      }\n      // i31 is special in that the Smi check is the last thing to do.\n      if (to_rep == wasm::HeapType::kI31) {\n        // If earlier optimization passes reached the limit of possible graph\n        // transformations, we could DCHECK(object_can_be_i31) here.\n        result = object_can_be_i31 ? __ IsSmi(object) : __ Word32Constant(0);\n        break;\n      }\n      if (to_rep == wasm::HeapType::kEq) {\n        if (object_can_be_i31) {\n          GOTO_IF(UNLIKELY(__ IsSmi(object)), end_label, __ Word32Constant(1));\n        }\n        result = IsDataRefMap(__ LoadMapField(object));\n        break;\n      }\n      // array, struct, string: i31 fails.\n      if (object_can_be_i31) {\n        GOTO_IF(UNLIKELY(__ IsSmi(object)), end_label, __ Word32Constant(0));\n      }\n      if (to_rep == wasm::HeapType::kArray) {\n        result = __ HasInstanceType(object, WASM_ARRAY_TYPE);\n        break;\n      }\n      if (to_rep == wasm::HeapType::kStruct) {\n        result = __ HasInstanceType(object, WASM_STRUCT_TYPE);\n        break;\n      }\n      if (to_rep == wasm::HeapType::kString ||\n          to_rep == wasm::HeapType::kExternString) {\n        V<Word32> instance_type =\n            __ LoadInstanceTypeField(__ LoadMapField(object));\n        result = __ Uint32LessThan(instance_type,\n                                   __ Word32Constant(FIRST_NONSTRING_TYPE));\n        break;\n      }\n      UNREACHABLE();\n    } while (false);\n\n    DCHECK(__ generating_unreachable_operations() || result.valid());\n    GOTO(end_label, result);\n    BIND(end_label, final_result);\n    return final_result;\n  }\n\n  V<Object> ReduceWasmTypeCastAbstract(V<Object> object,\n                                       WasmTypeCheckConfig config) {\n    const bool object_can_be_null = config.from.is_nullable();\n    const bool null_succeeds = config.to.is_nullable();\n    const bool object_can_be_i31 =\n        wasm::IsSubtypeOf(wasm::kWasmI31Ref.AsNonNull(), config.from,\n                          module_) ||\n        config.from.heap_representation() == wasm::HeapType::k"
}