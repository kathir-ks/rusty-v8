{
  "file_path": "/home/kathirks_gc/v8_go/archive/codebase/src/compiler/turboshaft/type-inference-analysis.h",
  "error": "JSON and XML parsing failed",
  "json_error_if_any": "Skipped JSON parsing for whole response due to presence of XML tags; XML is primary.",
  "xml_error": "not well-formed (invalid token): line 43, column 45",
  "raw_response": "```xml\n<file>\n    <metadata>\n        {\n            \"path\": \"/home/kathirks_gc/v8_go/archive/codebase/src/compiler/turboshaft/type-inference-analysis.h\",\n            \"file_name\": \"type-inference-analysis.h\",\n            \"language\": \"cpp\",\n            \"purpose\": \"Defines the TypeInferenceAnalysis class, which performs type inference on a Turboshaft graph using a fixpoint analysis.\"\n        }\n    </metadata>\n    <imports>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"purpose\": \"Includes standard library headers and V8 specific headers for type inference analysis.\"\n            }\n        </metadata>\n        <code><![CDATA[\n#include <limits>\n#include <optional>\n\n#include \"src/base/logging.h\"\n#include \"src/base/vector.h\"\n#include \"src/compiler/common-operator.h\"\n#include \"src/compiler/turboshaft/assembler.h\"\n#include \"src/compiler/turboshaft/operations.h\"\n#include \"src/compiler/turboshaft/representations.h\"\n#include \"src/compiler/turboshaft/sidetable.h\"\n#include \"src/compiler/turboshaft/snapshot-table.h\"\n#include \"src/compiler/turboshaft/typer.h\"\n#include \"src/compiler/turboshaft/types.h\"\n        ]]></code>\n    </imports>\n    <class>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"type\": \"class\",\n                \"name\": \"TypeInferenceAnalysis\",\n                \"about\": \"Infers types for all operations in a Turboshaft graph using a fixpoint analysis. Handles PhiOps with widening/narrowing mechanics.\",\n                \"attributes\": [\n                    {\n                        \"name\": \"graph_\",\n                        \"type\": \"const Graph&\",\n                        \"access\": \"private\",\n                        \"purpose\": \"The graph to analyze.\"\n                    },\n                    {\n                        \"name\": \"types_\",\n                        \"type\": \"GrowingOpIndexSidetable<Type>\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Stores the inferred types for each operation.\"\n                    },\n                    {\n                        \"name\": \"table_\",\n                        \"type\": \"SnapshotTable<Type>\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Table to manage snapshots of types during fixpoint iteration.\"\n                    },\n                    {\n                        \"name\": \"current_block_\",\n                        \"type\": \"const Block*\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Pointer to the currently processed block.\"\n                    },\n                    {\n                        \"name\": \"op_to_key_mapping_\",\n                        \"type\": \"GrowingOpIndexSidetable<std::optional<table_t::Key>>\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Maps operation indices to keys in the snapshot table.\"\n                    },\n                    {\n                        \"name\": \"block_to_snapshot_mapping_\",\n                        \"type\": \"GrowingBlockSidetable<std::optional<table_t::Snapshot>>\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Maps block indices to snapshots in the snapshot table.\"\n                    },\n                    {\n                        \"name\": \"predecessors_\",\n                        \"type\": \"ZoneVector<table_t::Snapshot>\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Vector to store snapshots of predecessors during merging.\"\n                    },\n                    {\n                        \"name\": \"graph_zone_\",\n                        \"type\": \"Zone*\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Memory zone for graph-related allocations.\"\n                    },\n                    {\n                        \"name\": \"block_refinements_\",\n                        \"type\": \"GrowingBlockSidetable<std::vector<std::pair<OpIndex, Type>>>*\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Stores block refinements only in debug builds.\"\n                    }\n                ],\n                \"dependencies\": [\n                    \"Graph\",\n                    \"Zone\",\n                    \"GrowingOpIndexSidetable\",\n                    \"Type\",\n                    \"SnapshotTable\",\n                    \"Block\",\n                    \"OpIndex\",\n                    \"Operation\",\n                    \"BlockIndex\",\n                    \"PhiOp\",\n                    \"GotoOp\",\n                    \"Typer\",\n                    \"BranchOp\",\n                    \"ConstantOp\",\n                    \"FloatBinopOp\",\n                    \"OverflowCheckedBinopOp\",\n                    \"ProjectionOp\",\n                    \"WordBinopOp\",\n                    \"TupleType\",\n                    \"Float64Type\",\n                    \"Float32Type\",\n                    \"WordOperationTyper\"\n                ]\n            }\n        </metadata>\n        <code><![CDATA[\nclass TypeInferenceAnalysis {\n public:\n  explicit TypeInferenceAnalysis(const Graph& graph, Zone* phase_zone)\n      : graph_(graph),\n        // TODO(nicohartmann@): Might put types back into phase_zone once we\n        // don't store them in the graph anymore.\n        types_(graph.op_id_count(), Type{}, graph.graph_zone(), &graph),\n        table_(phase_zone),\n        op_to_key_mapping_(phase_zone, &graph),\n        block_to_snapshot_mapping_(graph.block_count(), std::nullopt,\n                                   phase_zone),\n        predecessors_(phase_zone),\n        graph_zone_(graph.graph_zone()) {}\n\n  GrowingOpIndexSidetable<Type> Run(\n      GrowingBlockSidetable<std::vector<std::pair<OpIndex, Type>>>*\n          block_refinements = nullptr) {\n#ifdef DEBUG\n    block_refinements_ = block_refinements;\n#endif  // DEBUG\n    TURBOSHAFT_TRACE_TYPING(\"=== Running Type Inference Analysis ===\\n\");\n    for (uint32_t unprocessed_index = 0;\n         unprocessed_index < graph_.block_count();) {\n      BlockIndex block_index = static_cast<BlockIndex>(unprocessed_index);\n      ++unprocessed_index;\n      const Block& block = graph_.Get(block_index);\n\n#ifdef DEBUG\n      if (V8_UNLIKELY(v8_flags.turboshaft_trace_typing)) {\n        std::stringstream os;\n        os << block.kind() << \" \" << block.index().id();\n        TURBOSHAFT_TRACE_TYPING(\"=== %s ===\\n\", os.str().c_str());\n      }\n#endif  // DEBUG\n\n      ProcessBlock<false>(block, &unprocessed_index);\n    }\n    TURBOSHAFT_TRACE_TYPING(\"=== Completed Type Inference Analysis ===\\n\");\n\n    return std::move(types_);\n  }\n\n  template <bool revisit_loop_header>\n  void ProcessBlock(const Block& block, uint32_t* unprocessed_index) {\n    DCHECK_IMPLIES(revisit_loop_header, block.IsLoop());\n\n    // Seal the current block first.\n    if (table_.IsSealed()) {\n      DCHECK_NULL(current_block_);\n    } else {\n      // If we process a new block while the previous one is still unsealed, we\n      // finalize it.\n      DCHECK_NOT_NULL(current_block_);\n      DCHECK(current_block_->index().valid());\n      block_to_snapshot_mapping_[current_block_->index()] = table_.Seal();\n      current_block_ = nullptr;\n    }\n\n    // Collect the snapshots of all predecessors.\n    {\n      predecessors_.clear();\n      for (const Block* pred : block.PredecessorsIterable()) {\n        std::optional<table_t::Snapshot> pred_snapshot =\n            block_to_snapshot_mapping_[pred->index()];\n        if (pred_snapshot.has_value()) {\n          predecessors_.push_back(pred_snapshot.value());\n        } else {\n          // The only case where we might not have a snapshot for the\n          // predecessor is when we visit a loop header for the first time.\n          DCHECK(block.IsLoop() && pred == block.LastPredecessor() &&\n                 !revisit_loop_header);\n        }\n      }\n      std::reverse(predecessors_.begin(), predecessors_.end());\n    }\n\n    // Start a new snapshot for this block by merging information from\n    // predecessors.\n    {\n      auto MergeTypes = [&](table_t::Key,\n                            base::Vector<const Type> predecessors) -> Type {\n        DCHECK_GT(predecessors.size(), 0);\n        Type result_type = predecessors[0];\n        for (size_t i = 1; i < predecessors.size(); ++i) {\n          result_type =\n              Type::LeastUpperBound(result_type, predecessors[i], graph_zone_);\n        }\n        return result_type;\n      };\n\n      table_.StartNewSnapshot(base::VectorOf(predecessors_), MergeTypes);\n    }\n\n    // Check if the predecessor is a branch that allows us to refine a few\n    // types.\n    DCHECK_IMPLIES(revisit_loop_header, block.PredecessorCount() == 2);\n    if (block.PredecessorCount() == 1) {\n      Block* predecessor = block.LastPredecessor();\n      const Operation& terminator = predecessor->LastOperation(graph_);\n      if (const BranchOp* branch = terminator.TryCast<BranchOp>()) {\n        DCHECK(branch->if_true == &block || branch->if_false == &block);\n        RefineTypesAfterBranch(branch, &block, branch->if_true == &block);\n      }\n    }\n    current_block_ = &block;\n\n    bool loop_needs_revisit = false;\n    auto op_range = graph_.OperationIndices(block);\n    for (auto it = op_range.begin(); it != op_range.end(); ++it) {\n      OpIndex index = *it;\n      const Operation& op = graph_.Get(index);\n\n      switch (op.opcode) {\n        case Opcode::kBranch:\n        case Opcode::kDeoptimize:\n        case Opcode::kDeoptimizeIf:\n        case Opcode::kFrameState:\n        case Opcode::kReturn:\n        case Opcode::kStore:\n        case Opcode::kRetain:\n        case Opcode::kUnreachable:\n        case Opcode::kSwitch:\n        case Opcode::kTuple:\n        case Opcode::kStaticAssert:\n        case Opcode::kDebugBreak:\n        case Opcode::kDebugPrint:\n#if V8_ENABLE_WEBASSEMBLY\n        case Opcode::kGlobalSet:\n        case Opcode::kTrapIf:\n#endif\n        case Opcode::kCheckException:\n          // These operations do not produce any output that needs to be typed.\n          DCHECK_EQ(0, op.outputs_rep().size());\n          break;\n        case Opcode::kCheckTurboshaftTypeOf:\n          ProcessCheckTurboshaftTypeOf(index,\n                                       op.Cast<CheckTurboshaftTypeOfOp>());\n          break;\n        case Opcode::kComparison:\n          ProcessComparison(index, op.Cast<ComparisonOp>());\n          break;\n        case Opcode::kConstant:\n          ProcessConstant(index, op.Cast<ConstantOp>());\n          break;\n        case Opcode::kFloatBinop:\n          ProcessFloatBinop(index, op.Cast<FloatBinopOp>());\n          break;\n        case Opcode::kOverflowCheckedBinop:\n          ProcessOverflowCheckedBinop(index, op.Cast<OverflowCheckedBinopOp>());\n          break;\n        case Opcode::kProjection:\n          ProcessProjection(index, op.Cast<ProjectionOp>());\n          break;\n        case Opcode::kWordBinop:\n          ProcessWordBinop(V<Word>::Cast(index), op.Cast<WordBinopOp>());\n          break;\n        case Opcode::kWord32PairBinop:\n        case Opcode::kAtomicWord32Pair:\n        case Opcode::kPendingLoopPhi:\n          // Input graph must not contain these op codes.\n          UNREACHABLE();\n        case Opcode::kPhi:\n          if constexpr (revisit_loop_header) {\n            loop_needs_revisit =\n                ProcessLoopPhi(index, op.Cast<PhiOp>()) || loop_needs_revisit;\n          } else {\n            ProcessPhi(index, op.Cast<PhiOp>());\n          }\n          break;\n        case Opcode::kGoto: {\n          const GotoOp& gto = op.Cast<GotoOp>();\n          // Check if this is a backedge.\n          if (gto.destination->IsLoop()) {\n            if (gto.destination->index() < current_block_->index()) {\n              ProcessBlock<true>(*gto.destination, unprocessed_index);\n            } else if (gto.destination->index() == current_block_->index()) {\n              // This is a single block loop. We must only revisit the current\n              // header block if we actually need to, in order to prevent\n              // infinite recursion.\n              if (!revisit_loop_header || loop_needs_revisit) {\n                ProcessBlock<true>(*gto.destination, unprocessed_index);\n              }\n            }\n          }\n          break;\n        }\n\n        default:\n          // TODO(nicohartmann@): Support remaining operations. For now we\n          // compute fallback types.\n          if (op.outputs_rep().size() > 0) {\n            constexpr bool allow_narrowing = false;\n            constexpr bool is_fallback_for_unsupported_operation = true;\n            SetType(index,\n                    Typer::TypeForRepresentation(op.outputs_rep(), graph_zone_),\n                    allow_narrowing, is_fallback_for_unsupported_operation);\n          }\n          break;\n        case Opcode::kLoadRootRegister:\n          SetType(index,\n                  Typer::TypeForRepresentation(op.outputs_rep(), graph_zone_));\n          break;\n      }\n    }\n\n    if constexpr (revisit_loop_header) {\n      if (loop_needs_revisit) {\n        // This is a loop header and the loop body needs to be revisited. Reset\n        // {unprocessed_index} to the loop header's successor.\n        *unprocessed_index =\n            std::min(*unprocessed_index, block.index().id() + 1);\n      }\n    }\n  }\n\n  void ProcessCheckTurboshaftTypeOf(OpIndex index,\n                                    const CheckTurboshaftTypeOfOp& check) {\n    Type input_type = GetType(check.input());\n\n    if (input_type.IsSubtypeOf(check.type)) {\n      TURBOSHAFT_TRACE_TYPING_OK(\n          \"CTOF %3d:%-40s\\n  P: %3d:%-40s ~~> %s\\n\", index.id(),\n          graph_.Get(index).ToString().substr(0, 40).c_str(),\n          check.input().id(),\n          graph_.Get(check.input()).ToString().substr(0, 40).c_str(),\n          input_type.ToString().c_str());\n    } else if (check.successful) {\n      FATAL(\n          \"Checking type %s of operation %d:%s failed after it passed in a \"\n          \"previous phase\",\n          check.type.ToString().c_str(), check.input().id(),\n          graph_.Get(check.input()).ToString().c_str());\n    } else {\n      TURBOSHAFT_TRACE_TYPING_FAIL(\n          \"CTOF %3d:%-40s\\n  F: %3d:%-40s ~~> %s\\n\", index.id(),\n          graph_.Get(index).ToString().substr(0, 40).c_str(),\n          check.input().id(),\n          graph_.Get(check.input()).ToString().substr(0, 40).c_str(),\n          input_type.ToString().c_str());\n    }\n  }\n\n  void ProcessComparison(OpIndex index, const ComparisonOp& comparison) {\n    Type left_type = GetType(comparison.left());\n    Type right_type = GetType(comparison.right());\n\n    Type result_type = Typer::TypeComparison(\n        left_type, right_type, comparison.rep, comparison.kind, graph_zone_);\n    SetType(index, result_type);\n  }\n\n  void ProcessConstant(OpIndex index, const ConstantOp& constant) {\n    if (constant.kind == ConstantOp::Kind::kFloat64 &&\n        constant.float64().is_hole_nan()) {\n      // TODO(nicohartmann): figure out how to type Float64 NaN holes. Typing\n      // them simply as NaN is not always correct and can lead to replacing NaN\n      // holes with regular NaNs.\n      SetType(index, Type::Any());\n      return;\n    }\n    Type type = Typer::TypeConstant(constant.kind, constant.storage);\n    SetType(index, type);\n  }\n\n  void ProcessFloatBinop(OpIndex index, const FloatBinopOp& binop) {\n    Type left_type = GetType(binop.left());\n    Type right_type = GetType(binop.right());\n\n    Type result_type = Typer::TypeFloatBinop(left_type, right_type, binop.kind,\n                                             binop.rep, graph_zone_);\n    SetType(index, result_type);\n  }\n\n  bool ProcessLoopPhi(OpIndex index, const PhiOp& phi) {\n    Type old_type = GetTypeAtDefinition(index);\n    Type new_type = ComputeTypeForPhi(phi);\n\n    if (old_type.IsInvalid()) {\n      SetType(index, new_type);\n      return true;\n    }\n\n    // If the new type is smaller, we narrow it without revisiting the loop.\n    if (new_type.IsSubtypeOf(old_type)) {\n      TURBOSHAFT_TRACE_TYPING_OK(\n          \"LOOP %3d:%-40s (FIXPOINT)\\n  N:     %-40s ~~> %-40s\\n\", index.id(),\n          graph_.Get(index).ToString().substr(0, 40).c_str(),\n          old_type.ToString().c_str(), new_type.ToString().c_str());\n\n      constexpr bool allow_narrowing = true;\n      SetType(index, new_type, allow_narrowing);\n      return false;\n    }\n\n    // Otherwise, the new type is larger and we widen and revisit the loop.\n    TURBOSHAFT_TRACE_TYPING_OK(\n        \"LOOP %3d:%-40s (REVISIT)\\n  W:     %-40s ~~> %-40s\\n\", index.id(),\n        graph_.Get(index).ToString().substr(0, 40).c_str(),\n        old_type.ToString().c_str(), new_type.ToString().c_str());\n\n    if (!old_type.IsNone()) {\n      new_type = Widen(old_type, new_type);\n    }\n    SetType(index, new_type);\n    return true;\n  }\n\n  void ProcessOverflowCheckedBinop(OpIndex index,\n                                   const OverflowCheckedBinopOp& binop) {\n    Type left_type = GetType(binop.left());\n    Type right_type = GetType(binop.right());\n\n    Type result_type = Typer::TypeOverflowCheckedBinop(\n        left_type, right_type, binop.kind, binop.rep, graph_zone_);\n    SetType(index, result_type);\n  }\n\n  void ProcessPhi(OpIndex index, const PhiOp& phi) {\n    Type result_type = ComputeTypeForPhi(phi);\n    SetType(index, result_type);\n  }\n\n  void ProcessProjection(OpIndex index, const ProjectionOp& projection) {\n    Type input_type = GetType(projection.input());\n\n    Type result_type;\n    if (input_type.IsNone()) {\n      result_type = Type::None();\n    } else if (input_type.IsTuple()) {\n      const TupleType& tuple = input_type.AsTuple();\n      DCHECK_LT(projection.index, tuple.size());\n      result_type = tuple.element(projection.index);\n      DCHECK(result_type.IsSubtypeOf(\n          Typer::TypeForRepresentation(projection.rep)));\n    } else {\n      result_type = Typer::TypeForRepresentation(projection.rep);\n    }\n\n    SetType(index, result_type);\n  }\n\n  void ProcessWordBinop(V<Word> index, const WordBinopOp& binop) {\n    Type left_type = GetType(binop.left());\n    Type right_type = GetType(binop.right());\n\n    Type result_type = Typer::TypeWordBinop(left_type, right_type, binop.kind,\n                                            binop.rep, graph_zone_);\n    SetType(index, result_type);\n  }\n\n  Type ComputeTypeForPhi(const PhiOp& phi) {\n    // Word64 values are truncated to word32 implicitly, we need to handle this\n    // here.\n    auto MaybeTruncate = [&](Type t) -> Type {\n      if (t.IsNone()) return t;\n      if (phi.rep == RegisterRepresentation::Word32()) {\n        return Typer::TruncateWord32Input(t, true, graph_zone_);\n      }\n      return t;\n    };\n\n    Type result_type =\n        MaybeTruncate(GetTypeOrDefault(phi.inputs()[0], Type::None()));\n    for (size_t i = 1; i < phi.inputs().size(); ++i) {\n      Type input_type =\n          MaybeTruncate(GetTypeOrDefault(phi.inputs()[i], Type::None()));\n      result_type = Type::LeastUpperBound(result_type, input_type, graph_zone_);\n    }\n    return result_type;\n  }\n\n  void RefineTypesAfterBranch(const BranchOp* branch, const Block* new_block,\n                              bool then_branch) {\n    TURBOSHAFT_TRACE_TYPING_OK(\"Br   %3d:%-40s\\n\", graph_.Index(*branch).id(),\n                               branch->ToString().substr(0, 40).c_str());\n\n    Typer::BranchRefinements refinements(\n        [this](OpIndex index) { return GetType(index); },\n        [&](OpIndex index, const Type& refined_type) {\n          RefineOperationType(new_block, index, refined_type,\n                              then_branch ? 'T' : 'F');\n        });\n\n    // Inspect branch condition.\n    const Operation& condition = graph_.Get(branch->condition());\n    refinements.RefineTypes(condition, then_branch, graph_zone_);\n  }\n\n  void RefineOperationType(const Block* new_block, OpIndex op, const Type& type,\n                           char case_for_tracing) {\n    DCHECK(op.valid());\n    DCHECK(!type.IsInvalid());\n\n    TURBOSHAFT_TRACE_TYPING_OK(\"  %c: %3d:%-40s ~~> %s\\n\", case_for_tracing,\n                               op.id(),\n                               graph_.Get(op).ToString().substr(0, 40).c_str(),\n                               type.ToString().c_str());\n\n    auto key_opt = op_to_key_mapping_[op];\n    DCHECK(key_opt.has_value());\n    table_.Set(*key_opt, type);\n\n#ifdef DEBUG\n    if (block_refinements_) {\n      (*block_refinements_)[new_block->index()].emplace_back(op, type);\n    }\n#endif\n\n    // TODO(nicohartmann@): One could push the refined type deeper into the\n    // operations.\n  }\n\n  void SetType(OpIndex index, Type result_type, bool allow_narrowing = false,\n               bool is_fallback_for_unsupported_operation = false) {\n    DCHECK(!result_type.IsInvalid());\n\n    if (auto key_opt = op_to_key_mapping_[index]) {\n      table_.Set(*key_opt, result_type);\n      types_[index] = result_type;\n    } else {\n      auto key = table_.NewKey(Type::None());\n      op_to_key_mapping_[index] = key;\n      table_.Set(key, result_type);\n      types_[index] = result_type;\n    }\n\n    if (!is_fallback_for_unsupported_operation) {\n      TURBOSHAFT_TRACE_TYPING_OK(\n          \"Type %3d:%-40s ==> %s\\n\", index.id(),\n          graph_.Get(index).ToString().substr(0, 40).c_str(),\n          result_type.ToString().c_str());\n    } else {\n      // TODO(nicohartmann@): Remove the fallback case once all operations are\n      // supported.\n      TURBOSHAFT_TRACE_TYPING_FAIL(\n          \"TODO %3d:%-40s ==> %s\\n\", index.id(),\n          graph_.Get(index).ToString().substr(0, 40).c_str(),\n          result_type.ToString().c_str());\n    }\n  }\n\n  Type GetTypeOrInvalid(const OpIndex index) {\n    if (auto key = op_to_key_mapping_[index]) return table_.Get(*key);\n    return Type::Invalid();\n  }\n\n  Type GetTypeOrDefault(OpIndex index, const Type& default_type) {\n    Type t = GetTypeOrInvalid(index);\n    if (t.IsInvalid()) return default_type;\n    return t;\n  }\n\n  Type GetType(OpIndex index) {\n    Type t = GetTypeOrInvalid(index);\n    if (t.IsInvalid()) {\n      // TODO(nicohartmann@): This is a fallback mechanism as long as not all\n      // operations are properly typed. Remove this once typing is complete.\n      const Operation& op = graph_.Get(index);\n      return Typer::TypeForRepresentation(op.outputs_rep(), graph_zone_);\n    }\n    return t;\n  }\n\n  Type GetTypeAtDefinition(OpIndex index) const { return types_[index]; }\n\n  Type Widen(const Type& old_type, const Type& new_type) {\n    if (new_type.IsAny()) return new_type;\n    // We might have to relax this eventually and widen different types.\n    DCHECK_EQ(old_type.kind(), new_type.kind());\n\n    switch (old_type.kind()) {\n      case Type::Kind::kWord32:\n        // TODO(nicohartmann@): Reevaluate whether exponential widening is\n        // better here.\n        //\n        // return WordOperationTyper<32>::WidenExponential(old_type.AsWord32(),\n        // new_type.AsWord32(), graph_zone_);\n        return WordOperationTyper<32>::WidenMaximal(\n            old_type.AsWord32(), new_type.AsWord32(), graph_zone_);\n      case Type::Kind::kWord64:\n        // TODO(nicohartmann@): Reevaluate whether exponential widening is\n        // better here.\n        //\n        // return WordOperationTyper<64>::WidenExponential(old_type.AsWord64(),\n        // new_type.AsWord64(), graph_zone_);\n        return WordOperationTyper<64>::WidenMaximal(\n            old_type.AsWord64(), new_type.AsWord64(), graph_zone_);\n      case Type::Kind::kFloat32:\n        // TODO(nicohartmann@): Implement proper widening.\n        return Float32Type::Any();\n      case Type::Kind::kFloat64:\n        // TODO(nicohartmann@): Implement proper widening.\n        return Float64Type::Any();\n      default:\n        // TODO(nicohartmann@): Handle remaining cases.\n        UNREACHABLE();\n    }\n  }\n\n private:\n  const Graph& graph_;\n  GrowingOpIndexSidetable<Type> types_;\n  using table_t = SnapshotTable<Type>;\n  table_t table_;\n  const Block* current_block_ = nullptr;\n  GrowingOpIndexSidetable<std::optional<table_t::Key>> op_to_key_mapping_;\n  GrowingBlockSidetable<std::optional<table_t::Snapshot>>\n      block_to_snapshot_mapping_;\n  // {predecessors_} is used during merging, but we use an instance variable for\n  // it, in order to save memory and not reallocate it for each merge.\n  ZoneVector<table_t::Snapshot> predecessors_;\n  Zone* graph_zone_;\n\n#ifdef DEBUG\n  // {block_refinements_} are only stored for tracing in Debug builds.\n  GrowingBlockSidetable<std::vector<std::pair<OpIndex, Type>>>*\n      block_refinements_ = nullptr;\n#endif\n};\n        ]]></code>\n    </class>\n</file>\n```"
}