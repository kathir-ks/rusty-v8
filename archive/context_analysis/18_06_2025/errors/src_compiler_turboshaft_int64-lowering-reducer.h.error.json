{
  "file_path": "/home/kathirks_gc/v8_go/archive/codebase/src/compiler/turboshaft/int64-lowering-reducer.h",
  "error": "Response not JSON and not XML-like after cleanup",
  "json_error_if_any": "Skipped JSON parsing for whole response due to presence of XML tags; XML is primary.",
  "raw_response": "```xml\n<file>\n    <metadata>\n    {\n        \"path\": \"/home/kathirks_gc/v8_go/archive/codebase/src/compiler/turboshaft/int64-lowering-reducer.h\",\n        \"file_name\": \"int64-lowering-reducer.h\",\n        \"language\": \"cpp\",\n        \"purpose\": \"Defines a reducer that lowers 64-bit integer operations to 32-bit operations on 32-bit platforms for WebAssembly.\"\n    }\n    </metadata>\n    <imports>\n        <metadata>\n        {\n            \"language\": \"cpp\",\n            \"purpose\": \"Includes necessary headers for codegen, turboshaft operations, phases, wasm compilation, and wasm engine.\"\n        }\n        </metadata>\n        <code><![CDATA[\n            #include \"src/codegen/machine-type.h\"\n            #include \"src/compiler/turboshaft/assembler.h\"\n            #include \"src/compiler/turboshaft/operations.h\"\n            #include \"src/compiler/turboshaft/phase.h\"\n            #include \"src/compiler/wasm-compiler.h\"\n            #include \"src/compiler/wasm-graph-assembler.h\"\n            #include \"src/wasm/wasm-engine.h\"\n        ]]></code>\n    </imports>\n    <class>\n        <metadata>\n        {\n            \"language\": \"cpp\",\n            \"type\": \"class\",\n            \"name\": \"Int64LoweringReducer\",\n            \"about\": \"A reducer that lowers 64-bit integer operations to 32-bit operations on 32-bit platforms for WebAssembly.\",\n            \"attributes\": [],\n            \"dependencies\": [\n                \"Next\",\n                \"wasm::CallOrigin\",\n                \"Signature\",\n                \"TSCallDescriptor\",\n                \"FrameStateData\",\n                \"FrameStateFunctionInfo\",\n                \"FrameStateInfo\",\n                \"ConstantOp\",\n                \"WordBinopOp\",\n                \"ShiftOp\",\n                \"ComparisonOp\",\n                \"ChangeOp\",\n                \"LoadOp\",\n                \"StoreOp\",\n                \"AtomicRMWOp\",\n                \"Simd128SplatOp\",\n                \"Simd128ExtractLaneOp\",\n                \"Simd128ReplaceLaneOp\"\n            ]\n        }\n        </metadata>\n        <code><![CDATA[\n            template <class Next>\n            class Int64LoweringReducer : public Next {\n            public:\n            TURBOSHAFT_REDUCER_BOILERPLATE(Int64Lowering)\n\n            // We could use V<Union<Word32, Word32Pair>> for Word32OrWord32Pair instead of\n            // OpIndex, but this would clash with the expected return types of\n            // ReduceWordBinop/ReduceShift/etc.\n            using Word32OrWord32Pair = OpIndex;\n\n            Int64LoweringReducer() {\n                wasm::CallOrigin origin = __ data() -> is_js_to_wasm()\n                                            ? wasm::kCalledFromJS\n                                            : wasm::kCalledFromWasm;\n                // To compute the machine signature, it doesn't matter whether types\n                // are canonicalized, just use whichever signature is present (functions\n                // will have one and wrappers the other).\n                if (__ data()->wasm_module_sig()) {\n                sig_ =\n                    CreateMachineSignature(zone_, __ data()->wasm_module_sig(), origin);\n                } else {\n                sig_ = CreateMachineSignature(zone_, __ data()->wasm_canonical_sig(),\n                                                origin);\n                }\n\n                InitializeIndexMaps();\n            }\n\n            Word32OrWord32Pair REDUCE(WordBinop)(Word32OrWord32Pair left,\n                                                Word32OrWord32Pair right,\n                                                WordBinopOp::Kind kind,\n                                                WordRepresentation rep) {\n                if (rep == WordRepresentation::Word64()) {\n                V<Word32Pair> left_pair = V<Word32Pair>::Cast(left);\n                V<Word32Pair> right_pair = V<Word32Pair>::Cast(right);\n                switch (kind) {\n                    case WordBinopOp::Kind::kAdd:\n                    return LowerPairBinOp(left_pair, right_pair,\n                                            Word32PairBinopOp::Kind::kAdd);\n                    case WordBinopOp::Kind::kSub:\n                    return LowerPairBinOp(left_pair, right_pair,\n                                            Word32PairBinopOp::Kind::kSub);\n                    case WordBinopOp::Kind::kMul:\n                    return LowerPairBinOp(left_pair, right_pair,\n                                            Word32PairBinopOp::Kind::kMul);\n                    case WordBinopOp::Kind::kBitwiseAnd:\n                    return LowerBitwiseAnd(left_pair, right_pair);\n                    case WordBinopOp::Kind::kBitwiseOr:\n                    return LowerBitwiseOr(left_pair, right_pair);\n                    case WordBinopOp::Kind::kBitwiseXor:\n                    return LowerBitwiseXor(left_pair, right_pair);\n                    default:\n                    FATAL(\"WordBinopOp kind %d not supported by int64 lowering\",\n                            static_cast<int>(kind));\n                }\n                }\n                return Next::ReduceWordBinop(left, right, kind, rep);\n            }\n\n            Word32OrWord32Pair REDUCE(Shift)(Word32OrWord32Pair left, V<Word32> right,\n                                            ShiftOp::Kind kind, WordRepresentation rep) {\n                if (rep == WordRepresentation::Word64()) {\n                V<Word32Pair> left_pair = V<Word32Pair>::Cast(left);\n                switch (kind) {\n                    case ShiftOp::Kind::kShiftLeft:\n                    return LowerPairShiftOp(left_pair, right,\n                                            Word32PairBinopOp::Kind::kShiftLeft);\n                    case ShiftOp::Kind::kShiftRightArithmetic:\n                    return LowerPairShiftOp(\n                        left_pair, right, Word32PairBinopOp::Kind::kShiftRightArithmetic);\n                    case ShiftOp::Kind::kShiftRightLogical:\n                    return LowerPairShiftOp(left_pair, right,\n                                            Word32PairBinopOp::Kind::kShiftRightLogical);\n                    case ShiftOp::Kind::kRotateRight:\n                    return LowerRotateRight(left_pair, right);\n                    default:\n                    FATAL(\"Shiftop kind %d not supported by int64 lowering\",\n                            static_cast<int>(kind));\n                }\n                }\n                return Next::ReduceShift(left, right, kind, rep);\n            }\n\n            V<Word32> REDUCE(Comparison)(V<Any> left, V<Any> right,\n                                        ComparisonOp::Kind kind,\n                                        RegisterRepresentation rep) {\n                if (rep != WordRepresentation::Word64()) {\n                return Next::ReduceComparison(left, right, kind, rep);\n                }\n\n                auto [left_low, left_high] = Unpack(V<Word32Pair>::Cast(left));\n                auto [right_low, right_high] = Unpack(V<Word32Pair>::Cast(right));\n                V<Word32> high_comparison;\n                V<Word32> low_comparison;\n                switch (kind) {\n                case ComparisonOp::Kind::kEqual:\n                    // TODO(wasm): Use explicit comparisons and && here?\n                    return __ Word32Equal(\n                        __ Word32BitwiseOr(__ Word32BitwiseXor(left_low, right_low),\n                                            __ Word32BitwiseXor(left_high, right_high)),\n                        0);\n                case ComparisonOp::Kind::kSignedLessThan:\n                    high_comparison = __ Int32LessThan(left_high, right_high);\n                    low_comparison = __ Uint32LessThan(left_low, right_low);\n                    break;\n                case ComparisonOp::Kind::kSignedLessThanOrEqual:\n                    high_comparison = __ Int32LessThan(left_high, right_high);\n                    low_comparison = __ Uint32LessThanOrEqual(left_low, right_low);\n                    break;\n                case ComparisonOp::Kind::kUnsignedLessThan:\n                    high_comparison = __ Uint32LessThan(left_high, right_high);\n                    low_comparison = __ Uint32LessThan(left_low, right_low);\n                    break;\n                case ComparisonOp::Kind::kUnsignedLessThanOrEqual:\n                    high_comparison = __ Uint32LessThan(left_high, right_high);\n                    low_comparison = __ Uint32LessThanOrEqual(left_low, right_low);\n                    break;\n                }\n\n                return __ Word32BitwiseOr(\n                    high_comparison,\n                    __ Word32BitwiseAnd(__ Word32Equal(left_high, right_high),\n                                        low_comparison));\n            }\n\n            V<Any> REDUCE(Call)(V<CallTarget> callee, OptionalV<FrameState> frame_state,\n                                base::Vector<const OpIndex> arguments,\n                                const TSCallDescriptor* descriptor, OpEffects effects) {\n                const bool is_tail_call = false;\n                return LowerCall(callee, frame_state, arguments, descriptor, effects,\n                                is_tail_call);\n            }\n\n            OpIndex REDUCE(TailCall)(OpIndex callee,\n                                    base::Vector<const OpIndex> arguments,\n                                    const TSCallDescriptor* descriptor) {\n                const bool is_tail_call = true;\n                OptionalV<FrameState> frame_state = OptionalV<FrameState>::Nullopt();\n                return LowerCall(callee, frame_state, arguments, descriptor,\n                                OpEffects().CanCallAnything(), is_tail_call);\n            }\n\n            OpIndex REDUCE(Constant)(ConstantOp::Kind kind, ConstantOp::Storage value) {\n                if (kind == ConstantOp::Kind::kWord64) {\n                uint32_t high = value.integral >> 32;\n                uint32_t low = value.integral & std::numeric_limits<uint32_t>::max();\n                return __ Tuple(__ Word32Constant(low), __ Word32Constant(high));\n                }\n                return Next::ReduceConstant(kind, value);\n            }\n\n            OpIndex REDUCE(Parameter)(int32_t parameter_index, RegisterRepresentation rep,\n                                    const char* debug_name = \"\") {\n                int32_t param_count = static_cast<int32_t>(sig_->parameter_count());\n                // Handle special indices (closure, context).\n                if (parameter_index < 0) {\n                return Next::ReduceParameter(parameter_index, rep, debug_name);\n                }\n                if (parameter_index > param_count) {\n                DCHECK_NE(rep, RegisterRepresentation::Word64());\n                int param_offset =\n                    std::count(sig_->parameters().begin(), sig_->parameters().end(),\n                               MachineRepresentation::kWord64);\n                return Next::ReduceParameter(parameter_index + param_offset, rep,\n                                            debug_name);\n                }\n                int32_t new_index = param_index_map_[parameter_index];\n                if (rep == RegisterRepresentation::Word64()) {\n                rep = RegisterRepresentation::Word32();\n                return __ Tuple(Next::ReduceParameter(new_index, rep),\n                                Next::ReduceParameter(new_index + 1, rep));\n                }\n                return Next::ReduceParameter(new_index, rep, debug_name);\n            }\n\n            V<None> REDUCE(Return)(V<Word32> pop_count,\n                                 base::Vector<const OpIndex> return_values,\n                                 bool spill_caller_frame_slots) {\n                if (!returns_i64_) {\n                return Next::ReduceReturn(pop_count, return_values,\n                                        spill_caller_frame_slots);\n                }\n                base::SmallVector<OpIndex, 8> lowered_values;\n                for (size_t i = 0; i < sig_->return_count(); ++i) {\n                if (sig_->GetReturn(i) == MachineRepresentation::kWord64) {\n                    auto [low, high] = Unpack(return_values[i]);\n                    lowered_values.push_back(low);\n                    lowered_values.push_back(high);\n                } else {\n                    lowered_values.push_back(return_values[i]);\n                }\n                }\n                return Next::ReduceReturn(pop_count, base::VectorOf(lowered_values),\n                                        spill_caller_frame_slots);\n            }\n\n            Word32OrWord32Pair REDUCE(WordUnary)(Word32OrWord32Pair input,\n                                                WordUnaryOp::Kind kind,\n                                                WordRepresentation rep) {\n                if (rep == RegisterRepresentation::Word64()) {\n                V<Word32Pair> input_pair = V<Word32Pair>::Cast(input);\n                switch (kind) {\n                    case WordUnaryOp::Kind::kCountLeadingZeros:\n                    return LowerClz(input_pair);\n                    case WordUnaryOp::Kind::kCountTrailingZeros:\n                    return LowerCtz(input_pair);\n                    case WordUnaryOp::Kind::kPopCount:\n                    return LowerPopCount(input_pair);\n                    case WordUnaryOp::Kind::kSignExtend8:\n                    return LowerSignExtend(\n                        __ Word32SignExtend8(Unpack(input_pair).first));\n                    case WordUnaryOp::Kind::kSignExtend16:\n                    return LowerSignExtend(\n                        __ Word32SignExtend16(Unpack(input_pair).first));\n                    case WordUnaryOp::Kind::kReverseBytes: {\n                    auto [low, high] = Unpack(input_pair);\n                    V<Word32> reversed_low = __ Word32ReverseBytes(low);\n                    V<Word32> reversed_high = __ Word32ReverseBytes(high);\n                    return __ Tuple(reversed_high, reversed_low);\n                    }\n                    default:\n                    FATAL(\"WordUnaryOp kind %d not supported by int64 lowering\",\n                            static_cast<int>(kind));\n                }\n                }\n                return Next::ReduceWordUnary(input, kind, rep);\n            }\n\n            OpIndex REDUCE(Change)(OpIndex input, ChangeOp::Kind kind,\n                                 ChangeOp::Assumption assumption,\n                                 RegisterRepresentation from,\n                                 RegisterRepresentation to) {\n                auto word32 = RegisterRepresentation::Word32();\n                auto word64 = RegisterRepresentation::Word64();\n                auto float64 = RegisterRepresentation::Float64();\n                using Kind = ChangeOp::Kind;\n                if (from != word64 && to != word64) {\n                return Next::ReduceChange(input, kind, assumption, from, to);\n                }\n\n                if (from == word32 && to == word64) {\n                if (kind == Kind::kZeroExtend) {\n                    return __ Tuple(V<Word32>::Cast(input), __ Word32Constant(0));\n                }\n                if (kind == Kind::kSignExtend) {\n                    return LowerSignExtend(input);\n                }\n                }\n                if (from == float64 && to == word64) {\n                if (kind == Kind::kBitcast) {\n                    return __ Tuple(__ Float64ExtractLowWord32(input),\n                                    __ Float64ExtractHighWord32(input));\n                }\n                }\n                if (from == word64 && to == float64) {\n                if (kind == Kind::kBitcast) {\n                    auto input_w32p = V<Word32Pair>::Cast(input);\n                    return __ BitcastWord32PairToFloat64(\n                        __ template Projection<1>(input_w32p),\n                        __ template Projection<0>(input_w32p));\n                }\n                }\n                if (from == word64 && to == word32 && kind == Kind::kTruncate) {\n                auto input_w32p = V<Word32Pair>::Cast(input);\n                return __ template Projection<0>(input_w32p);\n                }\n                std::stringstream str;\n                str << \"ChangeOp \" << kind << \" from \" << from << \" to \" << to\n                    << \"not supported by int64 lowering\";\n                FATAL(\"%s\", str.str().c_str());\n            }\n\n            std::pair<OptionalV<Word32>, int32_t> IncreaseOffset(OptionalV<Word32> index,\n                                                                int32_t offset,\n                                                                int32_t add_offset,\n                                                                bool tagged_base) {\n                // Note that the offset will just wrap around. Still, we need to always\n                // use an offset that is not std::numeric_limits<int32_t>::min() on tagged\n                // loads.\n                // TODO(dmercadier): Replace LoadOp::OffsetIsValid by taking care of this\n                // special case in the LoadStoreSimplificationReducer instead.\n                int32_t new_offset =\n                    static_cast<uint32_t>(offset) + static_cast<uint32_t>(add_offset);\n                OptionalV<Word32> new_index = index;\n                if (!LoadOp::OffsetIsValid(new_offset, tagged_base)) {\n                // We cannot encode the new offset so we use the old offset\n                // instead and use the Index to represent the extra offset.\n                new_offset = offset;\n                if (index.has_value()) {\n                    new_index = __ Word32Add(new_index.value(), add_offset);\n                } else {\n                    new_index = __ Word32Constant(sizeof(int32_t));\n                }\n                }\n                return {new_index, new_offset};\n            }\n\n            OpIndex REDUCE(Load)(OpIndex base, OptionalOpIndex index, LoadOp::Kind kind,\n                                MemoryRepresentation loaded_rep,\n                                RegisterRepresentation result_rep, int32_t offset,\n                                uint8_t element_scale) {\n                if (kind.is_atomic) {\n                if (loaded_rep == MemoryRepresentation::Int64() ||\n                    loaded_rep == MemoryRepresentation::Uint64()) {\n                    // TODO(jkummerow): Support non-zero scales in AtomicWord32PairOp, and\n                    // remove the corresponding bailout in MachineOptimizationReducer to\n                    // allow generating them.\n                    CHECK_EQ(element_scale, 0);\n                    return __ AtomicWord32PairLoad(base, index, offset);\n                }\n                if (result_rep == RegisterRepresentation::Word64()) {\n                    return __ Tuple(\n                        __ Load(base, index, kind, loaded_rep,\n                                RegisterRepresentation::Word32(), offset, element_scale),\n                        __ Word32Constant(0));\n                }\n                }\n                if (loaded_rep == MemoryRepresentation::Int64() ||\n                    loaded_rep == MemoryRepresentation::Uint64()) {\n                auto [high_index, high_offset] =\n                    IncreaseOffset(index, offset, sizeof(int32_t), kind.tagged_base);\n                return __ Tuple(\n                    Next::ReduceLoad(base, index, kind, MemoryRepresentation::Int32(),\n                                    RegisterRepresentation::Word32(), offset,\n                                    element_scale),\n                    Next::ReduceLoad(\n                        base, high_index, kind, MemoryRepresentation::Int32(),\n                        RegisterRepresentation::Word32(), high_offset, element_scale));\n                }\n                return Next::ReduceLoad(base, index, kind, loaded_rep, result_rep, offset,\n                                        element_scale);\n            }\n\n            V<None> REDUCE(Store)(OpIndex base, OptionalOpIndex index, OpIndex value,\n                                StoreOp::Kind kind, MemoryRepresentation stored_rep,\n                                WriteBarrierKind write_barrier, int32_t offset,\n                                uint8_t element_size_log2,\n                                bool maybe_initializing_or_transitioning,\n                                IndirectPointerTag maybe_indirect_pointer_tag) {\n                if (stored_rep == MemoryRepresentation::Int64() ||\n                    stored_rep == MemoryRepresentation::Uint64()) {\n                auto [low, high] = Unpack(value);\n                if (kind.is_atomic) {\n                    // TODO(jkummerow): Support non-zero scales in AtomicWord32PairOp, and\n                    // remove the corresponding bailout in MachineOptimizationReducer to\n                    // allow generating them.\n                    CHECK_EQ(element_size_log2, 0);\n                    return __ AtomicWord32PairStore(base, index, low, high, offset);\n                }\n                // low store\n                Next::ReduceStore(base, index, low, kind, MemoryRepresentation::Int32(),\n                                    write_barrier, offset, element_size_log2,\n                                    maybe_initializing_or_transitioning,\n                                    maybe_indirect_pointer_tag);\n                // high store\n                auto [high_index, high_offset] =\n                    IncreaseOffset(index, offset, sizeof(int32_t), kind.tagged_base);\n                Next::ReduceStore(\n                    base, high_index, high, kind, MemoryRepresentation::Int32(),\n                    write_barrier, high_offset, element_size_log2,\n                    maybe_initializing_or_transitioning, maybe_indirect_pointer_tag);\n                return V<None>::Invalid();\n                }\n                return Next::ReduceStore(base, index, value, kind, stored_rep,\n                                        write_barrier, offset, element_size_log2,\n                                        maybe_initializing_or_transitioning,\n                                        maybe_indirect_pointer_tag);\n            }\n\n            OpIndex REDUCE(AtomicRMW)(OpIndex base, OpIndex index, OpIndex value,\n                                    OptionalOpIndex expected, AtomicRMWOp::BinOp bin_op,\n                                    RegisterRepresentation in_out_rep,\n                                    MemoryRepresentation memory_rep,\n                                    MemoryAccessKind kind) {\n                if (in_out_rep != RegisterRepresentation::Word64()) {\n                return Next::ReduceAtomicRMW(base, index, value, expected, bin_op,\n                                            in_out_rep, memory_rep, kind);\n                }\n                auto [value_low, value_high] = Unpack(value);\n                if (memory_rep == MemoryRepresentation::Int64() ||\n                    memory_rep == MemoryRepresentation::Uint64()) {\n                if (bin_op == AtomicRMWOp::BinOp::kCompareExchange) {\n                    auto [expected_low, expected_high] = Unpack(expected.value());\n                    return __ AtomicWord32PairCompareExchange(\n                        base, index, value_low, value_high, expected_low, expected_high);\n                } else {\n                    return __ AtomicWord32PairBinop(base, index, value_low, value_high,\n                                                    bin_op);\n                }\n                }\n\n                OpIndex new_expected = OpIndex::Invalid();\n                if (bin_op == AtomicRMWOp::BinOp::kCompareExchange) {\n                auto [expected_low, expected_high] = Unpack(expected.value());\n                new_expected = expected_low;\n                }\n                return __ Tuple(Next::ReduceAtomicRMW(\n                                    base, index, value_low, new_expected, bin_op,\n                                    RegisterRepresentation::Word32(), memory_rep, kind),\n                                __ Word32Constant(0));\n            }\n\n            OpIndex REDUCE(Phi)(base::Vector<const OpIndex> inputs,\n                                RegisterRepresentation rep) {\n                if (rep == RegisterRepresentation::Word64()) {\n                base::SmallVector<OpIndex, 8> inputs_low;\n                base::SmallVector<OpIndex, 8> inputs_high;\n                auto word32 = RegisterRepresentation::Word32();\n                inputs_low.reserve(inputs.size());\n                inputs_high.reserve(inputs.size());\n                for (OpIndex input : inputs) {\n                    auto input_w32p = V<Word32Pair>::Cast(input);\n                    inputs_low.push_back(__ template Projection<0>(input_w32p));\n                    inputs_high.push_back(__ template Projection<1>(input_w32p));\n                }\n                return __ Tuple(Next::ReducePhi(base::VectorOf(inputs_low), word32),\n                                Next::ReducePhi(base::VectorOf(inputs_high), word32));\n                }\n                return Next::ReducePhi(inputs, rep);\n            }\n\n            OpIndex REDUCE(PendingLoopPhi)(OpIndex input, RegisterRepresentation rep) {\n                if (rep == RegisterRepresentation::Word64()) {\n                auto input_w32p = V<Word32Pair>::Cast(input);\n                V<Word32> low = __ PendingLoopPhi(__ template Projection<0>(input_w32p));\n                V<Word32> high = __ PendingLoopPhi(__ template Projection<1>(input_w32p));\n                return __ Tuple(low, high);\n                }\n                return Next::ReducePendingLoopPhi(input, rep);\n            }\n\n            void FixLoopPhi(const PhiOp& input_phi, OpIndex output_index,\n                            Block* output_graph_loop) {\n                if (input_phi.rep == RegisterRepresentation::Word64()) {\n                const TupleOp& tuple = __ Get(output_index).template Cast<TupleOp>();\n                DCHECK_EQ(tuple.input_count, 2);\n                OpIndex new_inputs[2] = {__ MapToNewGraph(input_phi.input(0)),\n                                        __ MapToNewGraph(input_phi.input(1))};\n                for (size_t i = 0; i < 2; ++i) {\n                    OpIndex phi_index = tuple.input(i);\n                    if (!output_graph_loop->Contains(phi_index)) {\n                    continue;\n                    }\n#ifdef DEBUG\n                    const PendingLoopPhiOp& pending_phi =\n                        __ Get(phi_index).template Cast<PendingLoopPhiOp>();\n                    DCHECK_EQ(pending_phi.rep, RegisterRepresentation::Word32());\n                    DCHECK_EQ(\n                        pending_phi.first(),\n                        __ Projection(new_inputs[0], i, RegisterRepresentation::Word32()));\n#endif\n                    __ output_graph().template Replace<PhiOp>(\n                        phi_index,\n                        base::VectorOf({__ Projection(new_inputs[0], i,\n                                                    RegisterRepresentation::Word32()),\n                                        __ Projection(new_inputs[1], i,\n                                                    RegisterRepresentation::Word32())}),\n                        RegisterRepresentation::Word32());\n                }\n                return;\n                }\n                return Next::FixLoopPhi(input_phi, output_index, output_graph_loop);\n            }\n\n            V<Simd128> REDUCE(Simd128Splat)(V<Any> input, Simd128SplatOp::Kind kind) {\n                // TODO(14108): Introduce I32-pair splat for better codegen.\n                if (kind != Simd128SplatOp::Kind::kI64x2) {\n                return Next::ReduceSimd128Splat(input, kind);\n                }\n                auto [low, high] = Unpack(V<Word32Pair>::Cast(input));\n                V<Simd128> base = __ Simd128Splat(low, Simd128SplatOp::Kind::kI32x4);\n                V<Simd128> first_replaced = __ Simd128ReplaceLane(\n                    base, high, Simd128ReplaceLaneOp::Kind::kI32x4, 1);\n                return __ Simd128ReplaceLane(first_replaced, high,\n                                            Simd128ReplaceLaneOp::Kind::kI32x4, 3);\n            }\n\n            V<Any> REDUCE(Simd128ExtractLane)(V<Simd128> input,\n                                            Simd128ExtractLaneOp::Kind kind,\n                                            uint8_t lane) {\n                if (kind != Simd128ExtractLaneOp::Kind::kI64x2) {\n                return Next::ReduceSimd128ExtractLane(input, kind, lane);\n                }\n                V<Word32> low = V<Word32>::Cast(__ Simd128ExtractLane(\n                    input, Simd128ExtractLaneOp::Kind::kI32x4, 2 * lane));\n                V<Word32> high = V<Word32>::Cast(__ Simd128ExtractLane(\n                    input, Simd128ExtractLaneOp::Kind::kI32x4, 2 * lane + 1));\n                return __ Tuple(low, high);\n            }\n\n            V<Simd128> REDUCE(Simd128ReplaceLane)(V<Simd128> into, V<Any> new_lane,\n                                                Simd128ReplaceLaneOp::Kind kind,\n                                                uint8_t lane) {\n                // TODO(14108): Introduce I32-pair lane replacement for better codegen.\n                if (kind != Simd128ReplaceLaneOp::Kind::kI64x2) {\n                return Next::ReduceSimd128ReplaceLane(into, new_lane, kind, lane);\n                }\n                auto [low, high] = Unpack(V<Word32Pair>::Cast(new_lane));\n                V<Simd128> low_replaced = __ Simd128ReplaceLane(\n                    into, low, Simd128ReplaceLaneOp::Kind::kI32x4, 2 * lane);\n                return __ Simd128ReplaceLane(\n                    low_replaced, high, Simd128ReplaceLaneOp::Kind::kI32x4, 2 * lane + 1);\n            }\n\n            V<turboshaft::FrameState> REDUCE(FrameState)(\n                base::Vector<const OpIndex> inputs, bool inlined,\n                const FrameStateData* data) {\n                bool has_int64_input = false;\n\n                for (MachineType type : data->machine_types) {\n                if (RegisterRepresentation::FromMachineType(type) ==\n                    RegisterRepresentation::Word64()) {\n                    has_int64_input = true;\n                    break;\n                }\n                }\n                if (!has_int64_input) {\n                return Next::ReduceFrameState(inputs, inlined, data);\n                }\n                FrameStateData::Builder builder;\n                if (inlined) {\n                builder.AddParentFrameState(V<turboshaft::FrameState>(inputs[0]));\n                }\n                const FrameStateFunctionInfo* function_info =\n                    data->frame_state_info.function_info();\n                uint16_t lowered_parameter_count = function_info->parameter_count();\n                int lowered_local_count = function_info->local_count();\n\n                for (size_t i = inlined; i < inputs.size(); ++i) {\n                // In case of inlining the parent FrameState is an additional input,\n                // however, it doesn't have an entry in the machine_types vector, so that\n                // index has to be adapted.\n                size_t machine_type_index = i - inlined;\n                if (RegisterRepresentation::FromMachineType(\n                        data->machine_types[machine_type_index]) ==\n                    RegisterRepresentation::Word64()) {\n                    auto [low, high] = Unpack(V<Word32Pair>::Cast(inputs[i]));\n                    builder.AddInput(MachineType::Int32(), low);\n                    builder.AddInput(MachineType::Int32(), high);\n                    // Note that the first input (after the optional parent FrameState) is\n                    // the JSClosure, so the first parameter is at index 1 (+1 in case of\n                    // nested inlining).\n                    if (i <= inlined + function_info->parameter_count()) {\n                    ++lowered_parameter_count;\n                    } else {\n                    ++lowered_local_count;\n                    }\n                } else {\n                    // Just copy over the existing input.\n                    builder.AddInput(data->machine_types[machine_type_index], inputs[i]);\n                }\n                }\n                Zone* zone = Asm().data()->compilation_zone();\n                auto* function_info_lowered = zone->New<compiler::FrameStateFunctionInfo>(\n                    compiler::FrameStateType::kLiftoffFunction, lowered_parameter_count,\n                    function_info->max_arguments(), lowered_local_count,\n                    function_info->shared_info(), kNullMaybeHandle,\n                    function_info->wasm_liftoff_frame_size(),\n                    function_info->wasm_function_index());\n                const FrameStateInfo& frame_state_info = data->frame_state_info;\n                auto* frame_state_info_lowered = zone->New<compiler::FrameStateInfo>(\n                    frame_state_info.bailout_id(), frame_state_info.state_combine(),\n                    function_info_lowered);\n\n                return Next::ReduceFrameState(\n                    builder.Inputs(), builder.inlined(),\n                    builder.AllocateFrameStateData(*frame_state_info_lowered, zone));\n            }\n\n            private:\n            bool CheckPairOrPairOp(V<Word32Pair> input) {\n#ifdef DEBUG\n                if (const TupleOp* tuple = matcher_.TryCast<TupleOp>(input)) {\n                DCHECK_EQ(2, tuple->input_count);\n                RegisterRepresentation word32 = RegisterRepresentation::Word32();\n                ValidateOpInputRep(__ output_graph(), tuple->input(0), word32);\n                ValidateOpInputRep(__ output_graph(), tuple->input(1), word32);\n                } else if (const DidntThrowOp* didnt_throw =\n                                matcher_.TryCast<DidntThrowOp>(input)) {\n                // If it's a call, it must be a call that returns exactly one i64.\n                // (Note that the CallDescriptor has already been lowered to [i32, i32].)\n                const CallOp& call =\n                    __ Get(didnt_throw->throwing_operation()).template Cast<CallOp>();\n                DCHECK_EQ(call.descriptor->descriptor->ReturnCount(), 2);\n                DCHECK_EQ(call.descriptor->descriptor->GetReturnType(0),\n                            MachineType::Int32());\n                DCHECK_EQ"
}