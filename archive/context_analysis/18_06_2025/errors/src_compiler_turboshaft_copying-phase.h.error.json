{
  "file_path": "/home/kathirks_gc/v8_go/archive/codebase/src/compiler/turboshaft/copying-phase.h",
  "error": "Response not JSON and not XML-like after cleanup",
  "json_error_if_any": "Skipped JSON parsing for whole response due to presence of XML tags; XML is primary.",
  "raw_response": "```xml\n<file>\n    <metadata>\n    {\n        \"path\": \"/home/kathirks_gc/v8_go/archive/codebase/src/compiler/turboshaft/copying-phase.h\",\n        \"file_name\": \"copying-phase.h\",\n        \"language\": \"cpp\",\n        \"purpose\": \"Defines the CopyingPhase for the Turboshaft compiler, which performs graph transformations and optimizations by copying the graph while applying reductions.\"\n    }\n    </metadata>\n    <imports>\n        <metadata>\n        {\n            \"language\": \"cpp\",\n            \"purpose\": \"Includes standard library headers and V8 specific headers needed for data structures, algorithms, logging, code generation, and compiler phases.\"\n        }\n        </metadata>\n        <code><![CDATA[\n            #include <algorithm>\n            #include <cstddef>\n            #include <cstdint>\n            #include <optional>\n            #include <utility>\n\n            #include \"src/base/iterator.h\"\n            #include \"src/base/logging.h\"\n            #include \"src/base/small-vector.h\"\n            #include \"src/base/vector.h\"\n            #include \"src/codegen/optimized-compilation-info.h\"\n            #include \"src/codegen/source-position.h\"\n            #include \"src/compiler/node-origin-table.h\"\n            #include \"src/compiler/turboshaft/assembler.h\"\n            #include \"src/compiler/turboshaft/graph.h\"\n            #include \"src/compiler/turboshaft/index.h\"\n            #include \"src/compiler/turboshaft/operations.h\"\n            #include \"src/compiler/turboshaft/phase.h\"\n            #include \"src/compiler/turboshaft/reducer-traits.h\"\n            #include \"src/compiler/turboshaft/representations.h\"\n            #include \"src/compiler/turboshaft/snapshot-table.h\"\n            #include \"src/compiler/turboshaft/variable-reducer.h\"\n            #include \"src/zone/zone-containers.h\"\n        ]]></code>\n    </imports>\n\n    <func>\n        <metadata>\n        {\n            \"language\": \"cpp\",\n            \"type\": \"function\",\n            \"name\": \"CountDecimalDigits\",\n            \"about\": \"Counts the number of decimal digits in a 32-bit unsigned integer.\",\n            \"logic\": \"Likely implemented using bitwise operations or a loop.\",\n            \"parameters\": [\n                {\n                    \"name\": \"value\",\n                    \"type\": \"uint32_t\",\n                    \"purpose\": \"The integer to count digits in\"\n                }\n            ],\n            \"return\": {\n                \"type\": \"int\",\n                \"description\": \"The number of decimal digits in 'value'.\"\n            },\n            \"dependencies\": []\n        }\n        </metadata>\n        <code><![CDATA[\n            V8_EXPORT_PRIVATE int CountDecimalDigits(uint32_t value);\n        ]]></code>\n    </func>\n\n    <struct>\n        <metadata>\n        {\n            \"language\": \"cpp\",\n            \"type\": \"struct\",\n            \"name\": \"PaddingSpace\",\n            \"about\": \"Represents padding space for formatting output.\",\n            \"attributes\": [\n                {\n                    \"name\": \"spaces\",\n                    \"type\": \"int\",\n                    \"access\": \"public\",\n                    \"purpose\": \"Number of spaces to pad.\"\n                }\n            ],\n            \"dependencies\": []\n        }\n        </metadata>\n        <code><![CDATA[\n            struct PaddingSpace {\n              int spaces;\n            };\n        ]]></code>\n    </struct>\n\n    <func>\n        <metadata>\n        {\n            \"language\": \"cpp\",\n            \"type\": \"function\",\n            \"name\": \"operator<<\",\n            \"about\": \"Overloads the output stream operator to handle PaddingSpace objects.\",\n            \"logic\": \"Outputs a specified number of spaces to the output stream.\",\n            \"parameters\": [\n                {\n                    \"name\": \"os\",\n                    \"type\": \"std::ostream&\",\n                    \"purpose\": \"The output stream to write to\"\n                },\n                {\n                    \"name\": \"padding\",\n                    \"type\": \"PaddingSpace\",\n                    \"purpose\": \"The PaddingSpace object containing the number of spaces.\"\n                }\n            ],\n            \"return\": {\n                \"type\": \"std::ostream&\",\n                \"description\": \"The modified output stream.\"\n            },\n            \"dependencies\": []\n        }\n        </metadata>\n        <code><![CDATA[\n            V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,\n                                                       PaddingSpace padding);\n        ]]></code>\n    </func>\n\n    <class>\n        <metadata>\n        {\n            \"language\": \"cpp\",\n            \"type\": \"class\",\n            \"name\": \"ReducerBaseForwarder\",\n            \"about\": \"A template class used for forwarding reducer calls to the next reducer in a chain.\",\n            \"dependencies\": [\n                \"Next\"\n            ]\n        }\n        </metadata>\n        <code><![CDATA[\n            template <typename Next>\n            class ReducerBaseForwarder;\n        ]]></code>\n    </class>\n\n    <class>\n        <metadata>\n        {\n            \"language\": \"cpp\",\n            \"type\": \"class\",\n            \"name\": \"WasmRevecReducer\",\n            \"about\": \"A template class likely related to WebAssembly reverse vector reductions.\",\n            \"dependencies\": [\n                \"Next\"\n            ]\n        }\n        </metadata>\n        <code><![CDATA[\n            template <typename Next>\n            class WasmRevecReducer;\n        ]]></code>\n    </class>\n\n    <class>\n        <metadata>\n        {\n            \"language\": \"cpp\",\n            \"type\": \"class\",\n            \"name\": \"OutputGraphAssembler\",\n            \"about\": \"A template class that assembles the output graph during the copying phase. It inherits from a VariableReducer and provides methods to reduce operations from the input graph into the output graph.\",\n            \"attributes\": [],\n            \"dependencies\": [\n                \"Base\",\n                \"Assembler\",\n                \"ReducerList\",\n                \"OpIndex\",\n                \"OptionalOpIndex\"\n            ]\n        }\n        </metadata>\n        <code><![CDATA[\n            template <typename Derived, typename Base>\n            class OutputGraphAssembler : public Base {\n        #define FRIEND(op) friend struct op##Op;\n          TURBOSHAFT_OPERATION_LIST(FRIEND)\n        #undef FRIEND\n          template <size_t I, class D>\n          friend struct FixedArityOperationT;\n\n          OpIndex Map(OpIndex index) { return derived_this()->MapToNewGraph(index); }\n\n          OptionalOpIndex Map(OptionalOpIndex index) {\n            return derived_this()->MapToNewGraph(index);\n          }\n\n          template <size_t N>\n          base::SmallVector<OpIndex, N> Map(base::Vector<const OpIndex> indices) {\n            return derived_this()->template MapToNewGraph<N>(indices);\n          }\n\n         public:\n        #define ASSEMBLE(operation)                                         \\\n          OpIndex AssembleOutputGraph##operation(const operation##Op& op) { \\\n            return op.Explode(                                              \\\n                [a = assembler()](auto... args) {                           \\\n                  return a->Reduce##operation(args...);                     \\\n                },                                                          \\\n                *this);                                                     \\\n          }\n          TURBOSHAFT_OPERATION_LIST(ASSEMBLE)\n        #undef ASSEMBLE\n\n         private:\n          Derived* derived_this() { return static_cast<Derived*>(this); }\n          Assembler<typename Base::ReducerList>* assembler() {\n            return &derived_this()->Asm();\n          }\n        };\n        ]]></code>\n    </class>\n\n    <class>\n        <metadata>\n        {\n            \"language\": \"cpp\",\n            \"type\": \"class\",\n            \"name\": \"GraphVisitor\",\n            \"extends\": \"OutputGraphAssembler<GraphVisitor<AfterNext>, VariableReducer<AfterNext>>\",\n            \"about\": \"The main class responsible for visiting and copying the input graph.  It inherits from OutputGraphAssembler and VariableReducer. Manages the mapping between old and new operations and blocks, and handles inlining and cloning of blocks.\",\n            \"attributes\": [\n                {\n                    \"name\": \"input_graph_\",\n                    \"type\": \"Graph&\",\n                    \"access\": \"private\",\n                    \"purpose\": \"Reference to the input graph.\"\n                },\n                {\n                    \"name\": \"current_input_block_\",\n                    \"type\": \"const Block*\",\n                    \"access\": \"private\",\n                    \"purpose\": \"The currently visited block in the input graph.\"\n                },\n                {\n                    \"name\": \"op_mapping_\",\n                    \"type\": \"FixedOpIndexSidetable<OpIndex>\",\n                    \"access\": \"private\",\n                    \"purpose\": \"Mapping from operation indices in the input graph to operation indices in the output graph.\"\n                },\n                {\n                    \"name\": \"block_mapping_\",\n                    \"type\": \"FixedBlockSidetable<Block*>\",\n                    \"access\": \"private\",\n                    \"purpose\": \"Mapping from blocks in the input graph to blocks in the output graph.\"\n                },\n                {\n                    \"name\": \"blocks_needing_variables_\",\n                    \"type\": \"BitVector\",\n                    \"access\": \"private\",\n                    \"purpose\": \"A bit vector indicating which blocks need variables for mapping operations.\"\n                },\n                {\n                    \"name\": \"old_opindex_to_variables\",\n                    \"type\": \"FixedOpIndexSidetable<MaybeVariable>\",\n                    \"access\": \"private\",\n                    \"purpose\": \"Mapping from old OpIndex to Variables.\"\n                },\n                {\n                    \"name\": \"blocks_to_clone_\",\n                    \"type\": \"ZoneVector<BlockToClone>\",\n                    \"access\": \"private\",\n                    \"purpose\": \"A vector of blocks that need to be cloned.\"\n                },\n                {\n                    \"name\": \"turn_loop_without_backedge_into_merge_\",\n                    \"type\": \"bool\",\n                    \"access\": \"private\",\n                    \"purpose\": \"Flag indicating to convert loops without back edges into merges.\"\n                },\n                {\n                    \"name\": \"block_to_inline_now_\",\n                    \"type\": \"Block*\",\n                    \"access\": \"private\",\n                    \"purpose\": \"The block to inline in the current block.\"\n                }\n            ],\n            \"dependencies\": [\n                \"AfterNext\",\n                \"VariableReducer\",\n                \"OutputGraphAssembler\",\n                \"OpIndex\",\n                \"OptionalOpIndex\",\n                \"Block\",\n                \"Graph\",\n                \"Operation\",\n                \"PhiOp\",\n                \"GotoOp\",\n                \"BranchOp\",\n                \"SwitchOp\",\n                \"FrameStateOp\",\n                \"CallOp\",\n                \"DidntThrowOp\",\n                \"CheckExceptionOp\",\n                \"ParameterOp\",\n                \"V\",\n                \"CallTarget\",\n                \"FrameState\",\n                \"Opcode\",\n                \"base::SmallVector\",\n                \"base::Vector\",\n                \"base::Reversed\"\n            ]\n        }\n        </metadata>\n        <code><![CDATA[\n            template <class AfterNext>\n            class GraphVisitor : public OutputGraphAssembler<GraphVisitor<AfterNext>,\n                                                             VariableReducer<AfterNext>> {\n              template <typename N>\n              friend class ReducerBaseForwarder;\n              template <typename N>\n              friend class WasmRevecReducer;\n\n             public:\n              using Next = VariableReducer<AfterNext>;\n              TURBOSHAFT_REDUCER_BOILERPLATE(CopyingPhase)\n\n              GraphVisitor()\n                  : input_graph_(Asm().modifiable_input_graph()),\n                    current_input_block_(nullptr),\n                    op_mapping_(Asm().input_graph().op_id_count(), OpIndex::Invalid(),\n                                Asm().phase_zone(), &Asm().input_graph()),\n                    block_mapping_(Asm().input_graph().block_count(), nullptr,\n                                   Asm().phase_zone()),\n                    blocks_needing_variables_(Asm().input_graph().block_count(),\n                                              Asm().phase_zone()),\n                    old_opindex_to_variables(Asm().input_graph().op_id_count(),\n                                             Asm().phase_zone(), &Asm().input_graph()),\n                    blocks_to_clone_(Asm().phase_zone()) {\n                Asm().output_graph().Reset();\n              }\n\n              // `trace_reduction` is a template parameter to avoid paying for tracing at\n              // runtime.\n              template <bool trace_reduction>\n              void VisitGraph() {\n                Asm().Analyze();\n\n                // Creating initial old-to-new Block mapping.\n                for (Block& input_block : Asm().modifiable_input_graph().blocks()) {\n                  block_mapping_[input_block.index()] = Asm().output_graph().NewBlock(\n                      input_block.IsLoop() ? Block::Kind::kLoopHeader : Block::Kind::kMerge,\n                      &input_block);\n                }\n\n                // Visiting the graph.\n                VisitAllBlocks<trace_reduction>();\n\n                Finalize();\n              }\n\n              void Bind(Block* block) {\n                Next::Bind(block);\n                block->SetOrigin(current_input_block());\n              }\n\n              void Finalize() {\n                // Updating the source_positions.\n                if (!Asm().input_graph().source_positions().empty()) {\n                  for (OpIndex index : Asm().output_graph().AllOperationIndices()) {\n                    OpIndex origin = Asm().output_graph().operation_origins()[index];\n                    Asm().output_graph().source_positions()[index] =\n                        origin.valid() ? Asm().input_graph().source_positions()[origin]\n                                       : SourcePosition::Unknown();\n                  }\n                }\n                // Updating the operation origins.\n                NodeOriginTable* origins = Asm().data()->node_origins();\n                if (origins) {\n                  for (OpIndex index : Asm().output_graph().AllOperationIndices()) {\n                    OpIndex origin = Asm().output_graph().operation_origins()[index];\n                    if (origin.valid()) {\n                      origins->SetNodeOrigin(index.id(), origin.id());\n                    }\n                  }\n                }\n\n                input_graph_.SwapWithCompanion();\n              }\n\n              const Block* current_input_block() { return current_input_block_; }\n\n              bool* turn_loop_without_backedge_into_merge() {\n                return &turn_loop_without_backedge_into_merge_;\n              }\n\n              // Emits a Goto to a cloned version of {input_block}, assuming that the only\n              // predecessor of this cloned copy will be the current block. {input_block} is\n              // not cloned right away (because this would recursively call VisitBlockBody,\n              // which could cause stack overflows), and is instead added to the\n              // {blocks_to_clone_} stack, whose blocks will be cloned once the current\n              // block has been fully visited.\n              void CloneBlockAndGoto(const Block* input_block) {\n                Block* new_block =\n                    Asm().output_graph().NewBlock(input_block->kind(), input_block);\n\n                // Computing which input of Phi operations to use when visiting\n                // {input_block} (since {input_block} doesn't really have predecessors\n                // anymore).\n                int added_block_phi_input = input_block->GetPredecessorIndex(\n                    Asm().current_block()->OriginForBlockEnd());\n\n                // There is no guarantees that {input_block} will be entirely removed just\n                // because it's cloned/inlined, since it's possible that it has predecessors\n                // for which this optimization didn't apply. As a result, we add it to\n                // {blocks_needing_variables_}, so that if it's ever generated\n                // normally, Variables are used when emitting its content, so that\n                // they can later be merged when control flow merges with the current\n                // version of {input_block} that we just cloned.\n                blocks_needing_variables_.Add(input_block->index().id());\n\n                Asm().Goto(new_block);\n\n                blocks_to_clone_.push_back({input_block, added_block_phi_input, new_block});\n              }\n\n              // Visits and emits {input_block} right now (ie, in the current block). This\n              // should not be called recursively in order to avoid stack overflow (ie,\n              // processing {input_block} should never lead to calling CloneAndInlingBlock).\n              void CloneAndInlineBlock(const Block* input_block) {\n                if (Asm().generating_unreachable_operations()) return;\n\n        #ifdef DEBUG\n                // Making sure that we didn't call CloneAndInlineBlock recursively.\n                DCHECK(!is_in_recursive_inlining_);\n                ScopedModification<bool> recursive_guard(&is_in_recursive_inlining_, true);\n        #endif\n\n                // Computing which input of Phi operations to use when visiting\n                // {input_block} (since {input_block} doesn't really have predecessors\n                // anymore).\n                int added_block_phi_input = input_block->GetPredecessorIndex(\n                    Asm().current_block()->OriginForBlockEnd());\n\n                // There is no guarantees that {input_block} will be entirely removed just\n                // because it's cloned/inlined, since it's possible that it has predecessors\n                // for which this optimization didn't apply. As a result, we add it to\n                // {blocks_needing_variables_}, so that if it's ever generated\n                // normally, Variables are used when emitting its content, so that\n                // they can later be merged when control flow merges with the current\n                // version of {input_block} that we just cloned.\n                blocks_needing_variables_.Add(input_block->index().id());\n\n                ScopedModification<bool> set_true(&current_block_needs_variables_, true);\n                VisitBlockBody<CanHavePhis::kYes, ForCloning::kYes, false>(\n                    input_block, added_block_phi_input);\n              }\n\n              // {InlineOp} introduces two limitations unlike {CloneAndInlineBlock}:\n              // 1. The input operation must not be emitted anymore as part of its\n              // regular input block;\n              // 2. {InlineOp} must not be used multiple times for the same input op.\n              bool InlineOp(OpIndex index, const Block* input_block) {\n                return VisitOpAndUpdateMapping<false>(index, input_block);\n              }\n\n              template <bool can_be_invalid = false>\n              OpIndex MapToNewGraph(OpIndex old_index, int predecessor_index = -1) {\n                DCHECK(old_index.valid());\n                OpIndex result = op_mapping_[old_index];\n\n                if (!result.valid()) {\n                  // {op_mapping} doesn't have a mapping for {old_index}. The\n                  // VariableReducer should provide the mapping.\n                  MaybeVariable var = GetVariableFor(old_index);\n                  if constexpr (can_be_invalid) {\n                    if (!var.has_value()) {\n                      return OpIndex::Invalid();\n                    }\n                  }\n                  DCHECK(var.has_value());\n                  if (predecessor_index == -1) {\n                    result = Asm().GetVariable(var.value());\n                  } else {\n                    result = Asm().GetPredecessorValue(var.value(), predecessor_index);\n                  }\n                }\n\n                DCHECK_IMPLIES(!can_be_invalid, result.valid());\n                return result;\n              }\n\n              template <bool can_be_invalid = false, typename T>\n              V<T> MapToNewGraph(V<T> old_index, int predecessor_index = -1) {\n                return V<T>::Cast(MapToNewGraph<can_be_invalid>(\n                    static_cast<OpIndex>(old_index), predecessor_index));\n              }\n\n              Block* MapToNewGraph(const Block* block) const {\n                Block* new_block = block_mapping_[block->index()];\n                DCHECK_NOT_NULL(new_block);\n                return new_block;\n              }\n\n              template <typename FunctionType>\n              OpIndex ResolvePhi(const PhiOp& op, FunctionType&& map,\n                                 RegisterRepresentation rep) {\n                if (op.input_count == 1) {\n                  // If, in the previous CopyingPhase, a loop header was turned into a\n                  // regular blocks, its PendingLoopPhis became Phis with a single input. We\n                  // can now just get rid of these Phis.\n                  return map(op.input(0), -1);\n                }\n\n                OpIndex ig_index = Asm().input_graph().Index(op);\n                if (Asm().current_block()->IsLoop()) {\n                  DCHECK_EQ(op.input_count, 2);\n                  OpIndex og_index = map(op.input(0), -1);\n                  if (ig_index == op.input(PhiOp::kLoopPhiBackEdgeIndex)) {\n                    // Avoid emitting a Loop Phi which points to itself, instead\n                    // emit it's 0'th input.\n                    return og_index;\n                  }\n                  return Asm().PendingLoopPhi(og_index, rep);\n                }\n\n                base::Vector<const OpIndex> old_inputs = op.inputs();\n                base::SmallVector<OpIndex, 64> new_inputs;\n                int predecessor_count = Asm().current_block()->PredecessorCount();\n                Block* old_pred = current_input_block_->LastPredecessor();\n                Block* new_pred = Asm().current_block()->LastPredecessor();\n                // Control predecessors might be missing after the optimization phase. So we\n                // need to skip phi inputs that belong to control predecessors that have no\n                // equivalent in the new graph.\n\n                // We first assume that the order if the predecessors of the current block\n                // did not change. If it did, {new_pred} won't be nullptr at the end of this\n                // loop, and we'll instead fall back to the slower code below to compute the\n                // inputs of the Phi.\n                int predecessor_index = predecessor_count - 1;\n                int old_index = static_cast<int>(old_inputs.size()) - 1;\n                for (OpIndex input : base::Reversed(old_inputs)) {\n                  if (new_pred && new_pred->OriginForBlockEnd() == old_pred) {\n                    // Phis inputs have to come from predecessors. We thus have to\n                    // MapToNewGraph with {predecessor_index} so that we get an OpIndex that\n                    // is from a predecessor rather than one that comes from a Variable\n                    // merged in the current block.\n                    new_inputs.push_back(map(input, predecessor_index, old_index));\n                    new_pred = new_pred->NeighboringPredecessor();\n                    predecessor_index--;\n                  }\n                  old_pred = old_pred->NeighboringPredecessor();\n                  old_index--;\n                }\n                DCHECK_IMPLIES(new_pred == nullptr, old_pred == nullptr);\n\n                if (new_pred != nullptr) {\n                  // If {new_pred} is not nullptr, then the order of the predecessors\n                  // changed. This should only happen with blocks that were introduced in\n                  // the previous graph. For instance, consider this (partial) dominator\n                  // tree:\n                  //\n                  //     \u2560 7\n                  //     \u2551 \u2560 8\n                  //     \u2551 \u255a 10\n                  //     \u2560 9\n                  //     \u255a 11\n                  //\n                  // Where the predecessors of block 11 are blocks 9 and 10 (in that order).\n                  // In dominator visit order, block 10 will be visited before block 9.\n                  // Since blocks are added to predecessors when the predecessors are\n                  // visited, it means that in the new graph, the predecessors of block 11\n                  // are [10, 9] rather than [9, 10].\n                  // To account for this, we reorder the inputs of the Phi, and get rid of\n                  // inputs from blocks that vanished.\n\n        #ifdef DEBUG\n                  // To check that indices are set properly, we zap them in debug builds.\n                  for (auto& block : Asm().modifiable_input_graph().blocks()) {\n                    block.clear_custom_data();\n                  }\n        #endif\n                  uint32_t pos = current_input_block_->PredecessorCount() - 1;\n                  for (old_pred = current_input_block_->LastPredecessor();\n                       old_pred != nullptr; old_pred = old_pred->NeighboringPredecessor()) {\n                    // Store the current index of the {old_pred}.\n                    old_pred->set_custom_data(pos--, Block::CustomDataKind::kPhiInputIndex);\n                  }\n\n                  // Filling {new_inputs}: we iterate the new predecessors, and, for each\n                  // predecessor, we check the index of the input corresponding to the old\n                  // predecessor, and we put it next in {new_inputs}.\n                  new_inputs.clear();\n                  predecessor_index = predecessor_count - 1;\n                  for (new_pred = Asm().current_block()->LastPredecessor();\n                       new_pred != nullptr; new_pred = new_pred->NeighboringPredecessor()) {\n                    const Block* origin = new_pred->OriginForBlockEnd();\n                    DCHECK_NOT_NULL(origin);\n                    old_index =\n                        origin->get_custom_data(Block::CustomDataKind::kPhiInputIndex);\n                    OpIndex input = old_inputs[old_index];\n                    // Phis inputs have to come from predecessors. We thus have to\n                    // MapToNewGraph with {predecessor_index} so that we get an OpIndex that\n                    // is from a predecessor rather than one that comes from a Variable\n                    // merged in the current block.\n                    new_inputs.push_back(map(input, predecessor_index, old_index));\n                    predecessor_index--;\n                  }\n                }\n\n                DCHECK_EQ(new_inputs.size(), Asm().current_block()->PredecessorCount());\n\n                if (new_inputs.size() == 1) {\n                  // This Operation used to be a Phi in a Merge, but since one (or more) of\n                  // the inputs of the merge have been removed, there is no need for a Phi\n                  // anymore.\n                  return new_inputs[0];\n                }\n\n                std::reverse(new_inputs.begin(), new_inputs.end());\n                return Asm().ReducePhi(base::VectorOf(new_inputs), rep);\n              }\n\n              // The block from the input graph that corresponds to the current block as a\n              // branch destination. Such a block might not exist, and this function uses a\n              // trick to compute such a block in almost all cases, but might rarely fail\n              // and return `nullptr` instead.\n              const Block* OriginForBlockStart(Block* block) const {\n                // Check that `block->origin_` is still valid as a block start and was not\n                // changed to a semantically different block when inlining blocks.\n                const Block* origin = block->origin_;\n                if (origin && MapToNewGraph(origin) == block) return origin;\n                return nullptr;\n              }\n\n              // Clone all of the blocks in {sub_graph} (which should be Blocks of the input\n              // graph). If `keep_loop_kinds` is true, the loop headers are preserved, and\n              // otherwise they are marked as Merge. An initial GotoOp jumping to the 1st\n              // block of `sub_graph` is always emitted. The output Block corresponding to\n              // the 1st block of `sub_graph` is returned.\n              template <class Set>\n              Block* CloneSubGraph(Set sub_graph, bool keep_loop_kinds,\n                                   bool is_loop_after_peeling = false) {\n                // The BlockIndex of the blocks of `sub_graph` should be sorted so that\n                // visiting them in order is correct (all of the predecessors of a block\n                // should always be visited before the block itself).\n                DCHECK(std::is_sorted(sub_graph.begin(), sub_graph.end(),\n                                      [](const Block* a, const Block* b) {\n                                        return a->index().id() <= b->index().id();\n                                      }));\n\n                // 1. Create new blocks, and update old->new mapping. This is required to\n                // emit multiple times the blocks of {sub_graph}: if a block `B1` in\n                // {sub_graph} ends with a Branch/Goto to a block `B2` that is also in\n                // {sub_graph}, then this Branch/Goto should go to the version of `B2` that\n                // this CloneSubGraph will insert, rather than to a version inserted by a\n                // previous call to CloneSubGraph or the version that the regular\n                // VisitAllBlock function will emit.\n                ZoneVector<Block*> old_mappings(sub_graph.size(), Asm().phase_zone());\n                for (auto&& [input_block, old_mapping] :\n                     base::zip(sub_graph, old_mappings)) {\n                  old_mapping = block_mapping_[input_block->index()];\n                  Block::Kind kind = keep_loop_kinds && input_block->IsLoop()\n                                         ? Block::Kind::kLoopHeader\n                                         : Block::Kind::kMerge;\n                  block_mapping_[input_block->index()] =\n                      Asm().output_graph().NewBlock(kind, input_block);\n                }\n\n                // 2. Visit block in correct order (begin to end)\n\n                // Emit a goto to 1st block.\n                Block* start = block_mapping_[(*sub_graph.begin())->index()];\n        #ifdef DEBUG\n                if (is_loop_after_peeling) start->set_has_peeled_iteration();\n        #endif\n                Asm().Goto(start);\n                // Visiting `sub_graph`.\n                for (const Block* block : sub_graph) {\n                  blocks_needing_variables_.Add(block->index().id());\n                  VisitBlock<false>(block);\n                  ProcessWaitingCloningAndInlining<false>();\n                }\n\n                // 3. Restore initial old->new mapping\n                for (auto&& [input_block, old_mapping] :\n                     base::zip(sub_graph, old_mappings)) {\n                  block_mapping_[input_block->index()] = old_mapping;\n                }\n\n                return start;\n              }\n\n              template <bool can_be_invalid = false>\n              OptionalOpIndex MapToNewGraph(OptionalOpIndex old_index,\n                                            int predecessor_index = -1) {\n                if (!old_index.has_value()) return OptionalOpIndex::Nullopt();\n                return MapToNewGraph<can_be_invalid>(old_index.value(), predecessor_index);\n              }\n\n              template <size_t expected_size>\n              base::SmallVector<OpIndex, expected_size> MapToNewGraph(\n                  base::Vector<const OpIndex> inputs) {\n                base::SmallVector<OpIndex, expected_size> result;\n                for (OpIndex input : inputs) {\n                  result.push_back(MapToNewGraph(input));\n                }\n                return result;\n              }\n\n             private:\n              template <bool trace_reduction>\n              void VisitAllBlocks() {\n                base::SmallVector<const Block*, 128> visit_stack;\n\n                visit_stack.push_back(&Asm().input_graph().StartBlock());\n                while (!visit_stack.empty()) {\n                  const Block* block = visit_stack.back();\n                  visit_stack.pop_back();\n                  VisitBlock<trace_reduction>(block);\n                  ProcessWaitingCloningAndInlining<trace_reduction>();\n\n                  for (Block* child = block->LastChild(); child != nullptr;\n                       child = child->NeighboringChild()) {\n                    visit_stack.push_back(child);\n                  }\n                }\n              }\n\n              template <bool trace_reduction>\n              void VisitBlock(const Block* input_block) {\n                if (tick_counter_) tick_counter_->TickAndMaybeEnterSafepoint();\n                Asm().SetCurrentOrigin(OpIndex::Invalid());\n                current_block_needs_variables_ =\n                    blocks_needing_variables_.Contains(input_block->index().id());\n                if constexpr (trace_reduction) {\n                  std::cout << \"\\nold \" << PrintAsBlockHeader{*input_block} << \"\\n\";\n                  std::cout << \"new \"\n                            << PrintAsBlockHeader{*MapToNewGraph(input_block),\n                                                  Asm().output_graph().next_block_index()}\n                            << \"\\n\";\n                }\n                Block* new_block = MapToNewGraph(input_block);\n                if (Asm().Bind(new_block)) {\n                  VisitBlockBody<CanHavePhis::kYes, ForCloning::kNo, trace_reduction>(\n                      input_block);\n                  if constexpr (trace_reduction) TraceBlockFinished();\n                } else {\n                  if constexpr (trace_reduction) TraceBlockUnreachable();\n                }\n\n                // If we eliminate a loop backedge, we need to turn the loop into a\n                // single-predecessor merge block.\n                if (!turn_loop_without_backedge_into_merge_) return;\n                const Operation& last_op = input_block->LastOperation(Asm().input_graph());\n                if (auto* final_goto = last_op.TryCast<GotoOp>()) {\n                  if (final_goto->destination->IsLoop()) {\n                    if (input_block->index() >= final_goto->destination->index()) {\n                      Asm().FinalizeLoop(MapToNewGraph(final_goto->destination));\n                    } else {\n                      // We have a forward jump to a loop, rather than a backedge. We\n                      // don't need to do anything.\n                    }\n                  }\n                }\n              }\n\n              enum class CanHavePhis { kNo, kYes };\n              enum class ForCloning { kNo, kYes };\n\n              template <CanHavePhis can_have_phis, ForCloning for_cloning,\n                        bool trace_reduction>\n              void VisitBlockBody(const Block* input_block,\n                                  int added_block_phi_input = -1) {\n                DCHECK_NOT_NULL(Asm().current_block());\n                current_input_block_ = input_block;\n\n                // Phis could be mutually recursive, for instance (in a loop header):\n                //\n                //     p1"
}