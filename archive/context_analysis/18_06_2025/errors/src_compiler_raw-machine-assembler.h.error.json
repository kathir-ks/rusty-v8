{
  "file_path": "/home/kathirks_gc/v8_go/archive/codebase/src/compiler/raw-machine-assembler.h",
  "error": "Response not JSON and not XML-like after cleanup",
  "json_error_if_any": "Skipped JSON parsing for whole response due to presence of XML tags; XML is primary.",
  "raw_response": "```xml\n<file>\n    <metadata>\n        {\n            \"path\": \"/home/kathirks_gc/v8_go/archive/codebase/src/compiler/raw-machine-assembler.h\",\n            \"file_name\": \"raw-machine-assembler.h\",\n            \"language\": \"cpp\",\n            \"purpose\": \"Declares the RawMachineAssembler class, which produces a low-level IR graph for the Turbofan compiler.  It provides methods for creating nodes, managing basic blocks, and defining control flow.\"\n        }\n    </metadata>\n    <imports>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"purpose\": \"Includes standard library headers and V8-specific headers for compiler, execution, and object representation.\"\n            }\n        </metadata>\n        <code><![CDATA[\n            #include <initializer_list>\n            #include <optional>\n            #include <type_traits>\n\n            #include \"src/common/globals.h\"\n            #include \"src/compiler/access-builder.h\"\n            #include \"src/compiler/common-operator.h\"\n            #include \"src/compiler/linkage.h\"\n            #include \"src/compiler/machine-operator.h\"\n            #include \"src/compiler/node-matchers.h\"\n            #include \"src/compiler/node.h\"\n            #include \"src/compiler/operator.h\"\n            #include \"src/compiler/simplified-operator.h\"\n            #include \"src/compiler/turbofan-graph.h\"\n            #include \"src/compiler/write-barrier-kind.h\"\n            #include \"src/execution/isolate.h\"\n            #include \"src/heap/factory.h\"\n            #include \"src/objects/string.h\"\n        ]]></code>\n    </imports>\n    <class>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"type\": \"class\",\n                \"name\": \"RawMachineAssembler\",\n                \"about\": \"Produces a low-level IR graph.  Nodes are wired into a graph and also placed into a schedule immediately.  Keeps track of basic blocks and creates new basic blocks for labels.\",\n                \"attributes\": [],\n                \"dependencies\": [\n                    \"Isolate\",\n                    \"TFGraph\",\n                    \"CallDescriptor\",\n                    \"MachineRepresentation\",\n                    \"MachineOperatorBuilder\",\n                    \"BasicBlock\",\n                    \"RawMachineLabel\",\n                    \"Schedule\",\n                    \"SourcePositionTable\",\n                    \"Node\",\n                    \"Operator\",\n                    \"CommonOperatorBuilder\",\n                    \"SimplifiedOperatorBuilder\",\n                    \"ExternalReference\",\n                    \"Handle<HeapObject>\"\n                ]\n            }\n        </metadata>\n        <code><![CDATA[\n            class V8_EXPORT_PRIVATE RawMachineAssembler {\n            public:\n              RawMachineAssembler(\n                  Isolate* isolate, TFGraph* graph, CallDescriptor* call_descriptor,\n                  MachineRepresentation word = MachineType::PointerRepresentation(),\n                  MachineOperatorBuilder::Flags flags =\n                      MachineOperatorBuilder::Flag::kNoFlags,\n                  MachineOperatorBuilder::AlignmentRequirements alignment_requirements =\n                      MachineOperatorBuilder::AlignmentRequirements::\n                          FullUnalignedAccessSupport());\n              ~RawMachineAssembler() = default;\n\n              RawMachineAssembler(const RawMachineAssembler&) = delete;\n              RawMachineAssembler& operator=(const RawMachineAssembler&) = delete;\n\n              Isolate* isolate() const { return isolate_; }\n              TFGraph* graph() const { return graph_; }\n              Zone* zone() const { return graph()->zone(); }\n              MachineOperatorBuilder* machine() { return &machine_; }\n              CommonOperatorBuilder* common() { return &common_; }\n              SimplifiedOperatorBuilder* simplified() { return &simplified_; }\n              CallDescriptor* call_descriptor() const { return call_descriptor_; }\n\n              // Only used for tests: Finalizes the schedule and exports it to be used for\n              // code generation. Note that this RawMachineAssembler becomes invalid after\n              // export.\n              Schedule* ExportForTest();\n              // Finalizes the schedule and transforms it into a graph that's suitable for\n              // it to be used for Turbofan optimization and re-scheduling. Note that this\n              // RawMachineAssembler becomes invalid after export.\n              TFGraph* ExportForOptimization();\n\n              // ===========================================================================\n              // The following utility methods create new nodes with specific operators and\n              // place them into the current basic block. They don't perform control flow,\n              // hence will not switch the current basic block.\n\n              Node* NullConstant();\n              Node* UndefinedConstant();\n\n              // Constants.\n              Node* PointerConstant(void* value) {\n                return IntPtrConstant(reinterpret_cast<intptr_t>(value));\n              }\n              Node* IntPtrConstant(intptr_t value) {\n                // TODO(dcarney): mark generated code as unserializable if value != 0.\n                return kSystemPointerSize == 8 ? Int64Constant(value)\n                                               : Int32Constant(static_cast<int>(value));\n              }\n              Node* RelocatableIntPtrConstant(intptr_t value, RelocInfo::Mode rmode);\n              Node* Int32Constant(int32_t value) {\n                return AddNode(common()->Int32Constant(value));\n              }\n              Node* StackSlot(MachineRepresentation rep, int alignment = 0) {\n                return AddNode(machine()->StackSlot(rep, alignment));\n              }\n              Node* StackSlot(int size, int alignment) {\n                return AddNode(machine()->StackSlot(size, alignment));\n              }\n              Node* Int64Constant(int64_t value) {\n                return AddNode(common()->Int64Constant(value));\n              }\n              Node* NumberConstant(double value) {\n                return AddNode(common()->NumberConstant(value));\n              }\n              Node* Float32Constant(float value) {\n                return AddNode(common()->Float32Constant(value));\n              }\n              Node* Float64Constant(double value) {\n                return AddNode(common()->Float64Constant(value));\n              }\n              Node* HeapConstant(Handle<HeapObject> object) {\n                return AddNode(common()->HeapConstant(object));\n              }\n              Node* ExternalConstant(ExternalReference address) {\n                return AddNode(common()->ExternalConstant(address));\n              }\n              Node* RelocatableInt32Constant(int32_t value, RelocInfo::Mode rmode) {\n                return AddNode(common()->RelocatableInt32Constant(value, rmode));\n              }\n              Node* RelocatableInt64Constant(int64_t value, RelocInfo::Mode rmode) {\n                return AddNode(common()->RelocatableInt64Constant(value, rmode));\n              }\n\n              Node* Projection(int index, Node* a) {\n                return AddNode(common()->Projection(index), a);\n              }\n\n              // Memory Operations.\n              Node* Load(MachineType type, Node* base) {\n                return Load(type, base, IntPtrConstant(0));\n              }\n              Node* Load(MachineType type, Node* base, Node* index) {\n                const Operator* op = machine()->Load(type);\n                Node* load = AddNode(op, base, index);\n                return load;\n              }\n              Node* LoadImmutable(MachineType type, Node* base) {\n                return LoadImmutable(type, base, IntPtrConstant(0));\n              }\n              Node* LoadImmutable(MachineType type, Node* base, Node* index) {\n                const Operator* op = machine()->LoadImmutable(type);\n                return AddNode(op, base, index);\n              }\n              bool IsMapOffsetConstant(Node* node) {\n                Int64Matcher m(node);\n                if (m.Is(HeapObject::kMapOffset)) return true;\n                // Test if `node` is a `Phi(Int64Constant(0))`\n                if (node->opcode() == IrOpcode::kPhi) {\n                  for (Node* input : node->inputs()) {\n                    if (!Int64Matcher(input).Is(HeapObject::kMapOffset)) return false;\n                  }\n                  return true;\n                }\n                return false;\n              }\n              bool IsMapOffsetConstantMinusTag(Node* node) {\n                Int64Matcher m(node);\n                return m.Is(HeapObject::kMapOffset - kHeapObjectTag);\n              }\n              bool IsMapOffsetConstantMinusTag(int offset) {\n                return offset == HeapObject::kMapOffset - kHeapObjectTag;\n              }\n              Node* LoadFromObject(MachineType type, Node* base, Node* offset) {\n                DCHECK_IMPLIES(V8_MAP_PACKING_BOOL && IsMapOffsetConstantMinusTag(offset),\n                               type == MachineType::MapInHeader());\n                ObjectAccess access = {type, WriteBarrierKind::kNoWriteBarrier};\n                Node* load = AddNode(simplified()->LoadFromObject(access), base, offset);\n                return load;\n              }\n\n              Node* LoadProtectedPointerFromObject(Node* base, Node* offset) {\n            #if V8_ENABLE_SANDBOX\n                static_assert(COMPRESS_POINTERS_BOOL);\n                Node* tagged = LoadFromObject(MachineType::Int32(), base, offset);\n                Node* trusted_cage_base =\n                    LoadImmutable(MachineType::Pointer(), LoadRootRegister(),\n                                  IntPtrConstant(IsolateData::trusted_cage_base_offset()));\n                return BitcastWordToTagged(\n                    WordOr(trusted_cage_base, ChangeUint32ToUint64(tagged)));\n            #else\n                return LoadFromObject(MachineType::AnyTagged(), base, offset);\n            #endif  // V8_ENABLE_SANDBOX\n              }\n\n              Node* Store(MachineRepresentation rep, Node* base, Node* value,\n                          WriteBarrierKind write_barrier) {\n                return Store(rep, base, IntPtrConstant(0), value, write_barrier);\n              }\n              Node* Store(MachineRepresentation rep, Node* base, Node* index, Node* value,\n                          WriteBarrierKind write_barrier) {\n                return AddNode(machine()->Store(StoreRepresentation(rep, write_barrier)),\n                               base, index, value);\n              }\n              void StoreToObject(MachineRepresentation rep, Node* object, Node* offset,\n                                 Node* value, WriteBarrierKind write_barrier) {\n                ObjectAccess access = {MachineType::TypeForRepresentation(rep),\n                                       write_barrier};\n                DCHECK(!IsMapOffsetConstantMinusTag(offset));\n                AddNode(simplified()->StoreToObject(access), object, offset, value);\n              }\n              void OptimizedStoreField(MachineRepresentation rep, Node* object, int offset,\n                                       Node* value, WriteBarrierKind write_barrier) {\n                DCHECK(!IsMapOffsetConstantMinusTag(offset));\n                DCHECK_NE(rep, MachineRepresentation::kIndirectPointer);\n                AddNode(simplified()->StoreField(\n                            FieldAccess(BaseTaggedness::kTaggedBase, offset,\n                                        MaybeHandle<Name>(), OptionalMapRef(), Type::Any(),\n                                        MachineType::TypeForRepresentation(rep),\n                                        write_barrier, \"OptimizedStoreField\")),\n                        object, value);\n              }\n              void OptimizedStoreIndirectPointerField(Node* object, int offset,\n                                                      IndirectPointerTag tag, Node* value,\n                                                      WriteBarrierKind write_barrier) {\n                DCHECK(!IsMapOffsetConstantMinusTag(offset));\n                DCHECK(write_barrier == WriteBarrierKind::kNoWriteBarrier ||\n                       write_barrier == WriteBarrierKind::kIndirectPointerWriteBarrier);\n                FieldAccess access(BaseTaggedness::kTaggedBase, offset, MaybeHandle<Name>(),\n                                   OptionalMapRef(), Type::Any(),\n                                   MachineType::IndirectPointer(), write_barrier,\n                                   \"OptimizedStoreIndirectPointerField\");\n                access.indirect_pointer_tag = tag;\n                AddNode(simplified()->StoreField(access), object, value);\n              }\n              void OptimizedStoreMap(Node* object, Node* value,\n                                     WriteBarrierKind write_barrier = kMapWriteBarrier) {\n                AddNode(simplified()->StoreField(AccessBuilder::ForMap(write_barrier)),\n                        object, value);\n              }\n              Node* Retain(Node* value) { return AddNode(common()->Retain(), value); }\n\n              Node* OptimizedAllocate(Node* size, AllocationType allocation);\n\n              // Unaligned memory operations\n              Node* UnalignedLoad(MachineType type, Node* base) {\n                return UnalignedLoad(type, base, IntPtrConstant(0));\n              }\n              Node* UnalignedLoad(MachineType type, Node* base, Node* index) {\n                MachineRepresentation rep = type.representation();\n                // Tagged or compressed should never be unaligned\n                DCHECK(!(IsAnyTagged(rep) || IsAnyCompressed(rep)));\n                if (machine()->UnalignedLoadSupported(rep)) {\n                  return AddNode(machine()->Load(type), base, index);\n                } else {\n                  return AddNode(machine()->UnalignedLoad(type), base, index);\n                }\n              }\n              Node* UnalignedStore(MachineRepresentation rep, Node* base, Node* value) {\n                return UnalignedStore(rep, base, IntPtrConstant(0), value);\n              }\n              Node* UnalignedStore(MachineRepresentation rep, Node* base, Node* index,\n                                   Node* value) {\n                // Tagged or compressed should never be unaligned\n                DCHECK(!(IsAnyTagged(rep) || IsAnyCompressed(rep)));\n                if (machine()->UnalignedStoreSupported(rep)) {\n                  return AddNode(machine()->Store(StoreRepresentation(\n                                     rep, WriteBarrierKind::kNoWriteBarrier)),\n                                 base, index, value);\n                } else {\n                  return AddNode(\n                      machine()->UnalignedStore(UnalignedStoreRepresentation(rep)), base,\n                      index, value);\n                }\n              }\n\n              // Atomic memory operations.\n              Node* AtomicLoad(AtomicLoadParameters rep, Node* base, Node* index) {\n                DCHECK_NE(rep.representation().representation(),\n                          MachineRepresentation::kWord64);\n                return AddNode(machine()->Word32AtomicLoad(rep), base, index);\n              }\n\n              Node* AtomicLoad64(AtomicLoadParameters rep, Node* base, Node* index) {\n                if (machine()->Is64()) {\n                  // This uses Uint64() intentionally: AtomicLoad is not implemented for\n                  // Int64(), which is fine because the machine instruction only cares\n                  // about words.\n                  return AddNode(machine()->Word64AtomicLoad(rep), base, index);\n                } else {\n                  return AddNode(machine()->Word32AtomicPairLoad(rep.order()), base, index);\n                }\n              }\n\n            #if defined(V8_TARGET_BIG_ENDIAN)\n            #define VALUE_HALVES value_high, value\n            #else\n            #define VALUE_HALVES value, value_high\n            #endif\n\n              Node* AtomicStore(AtomicStoreParameters params, Node* base, Node* index,\n                                Node* value) {\n                DCHECK(!IsMapOffsetConstantMinusTag(index));\n                DCHECK_NE(params.representation(), MachineRepresentation::kWord64);\n                return AddNode(machine()->Word32AtomicStore(params), base, index, value);\n              }\n\n              Node* AtomicStore64(AtomicStoreParameters params, Node* base, Node* index,\n                                  Node* value, Node* value_high) {\n                if (machine()->Is64()) {\n                  DCHECK_NULL(value_high);\n                  return AddNode(machine()->Word64AtomicStore(params), base, index, value);\n                } else {\n                  DCHECK(params.representation() != MachineRepresentation::kTaggedPointer &&\n                         params.representation() != MachineRepresentation::kTaggedSigned &&\n                         params.representation() != MachineRepresentation::kTagged);\n                  return AddNode(machine()->Word32AtomicPairStore(params.order()), base,\n                                 index, VALUE_HALVES);\n                }\n              }\n\n            #define ATOMIC_FUNCTION(name)                                                  \\\n              Node* Atomic##name(MachineType type, Node* base, Node* index, Node* value) { \\\n                DCHECK_NE(type.representation(), MachineRepresentation::kWord64);          \\\n                return AddNode(machine()->Word32Atomic##name(type), base, index, value);   \\\n              }                                                                            \\\n              Node* Atomic##name##64(Node * base, Node * index, Node * value,              \\\n                                     Node * value_high) {                                  \\\n                if (machine()->Is64()) {                                                   \\\n                  DCHECK_NULL(value_high);                                                 \\\n                  /* This uses Uint64() intentionally: Atomic operations are not  */       \\\n                  /* implemented for Int64(), which is fine because the machine   */       \\\n                  /* instruction only cares about words.                          */       \\\n                  return AddNode(machine()->Word64Atomic##name(MachineType::Uint64()),     \\\n                                 base, index, value);                                      \\\n                } else {                                                                   \\\n                  return AddNode(machine()->Word32AtomicPair##name(), base, index,         \\\n                                 VALUE_HALVES);                                            \\\n                }                                                                          \\\n              }\n              ATOMIC_FUNCTION(Exchange)\n              ATOMIC_FUNCTION(Add)\n              ATOMIC_FUNCTION(Sub)\n              ATOMIC_FUNCTION(And)\n              ATOMIC_FUNCTION(Or)\n              ATOMIC_FUNCTION(Xor)\n            #undef ATOMIC_FUNCTION\n            #undef VALUE_HALVES\n\n              Node* AtomicCompareExchange(MachineType type, Node* base, Node* index,\n                                          Node* old_value, Node* new_value) {\n                DCHECK_NE(type.representation(), MachineRepresentation::kWord64);\n                return AddNode(machine()->Word32AtomicCompareExchange(type), base, index,\n                               old_value, new_value);\n              }\n\n              Node* AtomicCompareExchange64(Node* base, Node* index, Node* old_value,\n                                            Node* old_value_high, Node* new_value,\n                                            Node* new_value_high) {\n                if (machine()->Is64()) {\n                  DCHECK_NULL(old_value_high);\n                  DCHECK_NULL(new_value_high);\n                  // This uses Uint64() intentionally: AtomicCompareExchange is not\n                  // implemented for Int64(), which is fine because the machine instruction\n                  // only cares about words.\n                  return AddNode(\n                      machine()->Word64AtomicCompareExchange(MachineType::Uint64()), base,\n                      index, old_value, new_value);\n                } else {\n                  return AddNode(machine()->Word32AtomicPairCompareExchange(), base, index,\n                                 old_value, old_value_high, new_value, new_value_high);\n                }\n              }\n\n              Node* MemoryBarrier(AtomicMemoryOrder order) {\n                return AddNode(machine()->MemoryBarrier(order));\n              }\n\n              // Arithmetic Operations.\n              Node* WordAnd(Node* a, Node* b) {\n                return AddNode(machine()->WordAnd(), a, b);\n              }\n              Node* WordOr(Node* a, Node* b) { return AddNode(machine()->WordOr(), a, b); }\n              Node* WordXor(Node* a, Node* b) {\n                return AddNode(machine()->WordXor(), a, b);\n              }\n              Node* WordShl(Node* a, Node* b) {\n                return AddNode(machine()->WordShl(), a, b);\n              }\n              Node* WordShr(Node* a, Node* b) {\n                return AddNode(machine()->WordShr(), a, b);\n              }\n              Node* WordSar(Node* a, Node* b) {\n                return AddNode(machine()->WordSar(), a, b);\n              }\n              Node* WordSarShiftOutZeros(Node* a, Node* b) {\n                return AddNode(machine()->WordSarShiftOutZeros(), a, b);\n              }\n              Node* WordRor(Node* a, Node* b) {\n                return AddNode(machine()->WordRor(), a, b);\n              }\n              Node* WordEqual(Node* a, Node* b) {\n                return AddNode(machine()->WordEqual(), a, b);\n              }\n              Node* WordNotEqual(Node* a, Node* b) {\n                return Word32BinaryNot(WordEqual(a, b));\n              }\n              Node* WordNot(Node* a) {\n                if (machine()->Is32()) {\n                  return Word32BitwiseNot(a);\n                } else {\n                  return Word64Not(a);\n                }\n              }\n\n              Node* Word32And(Node* a, Node* b) {\n                return AddNode(machine()->Word32And(), a, b);\n              }\n              Node* Word32Or(Node* a, Node* b) {\n                return AddNode(machine()->Word32Or(), a, b);\n              }\n              Node* Word32Xor(Node* a, Node* b) {\n                return AddNode(machine()->Word32Xor(), a, b);\n              }\n              Node* Word32Shl(Node* a, Node* b) {\n                return AddNode(machine()->Word32Shl(), a, b);\n              }\n              Node* Word32Shr(Node* a, Node* b) {\n                return AddNode(machine()->Word32Shr(), a, b);\n              }\n              Node* Word32Sar(Node* a, Node* b) {\n                return AddNode(machine()->Word32Sar(), a, b);\n              }\n              Node* Word32SarShiftOutZeros(Node* a, Node* b) {\n                return AddNode(machine()->Word32SarShiftOutZeros(), a, b);\n              }\n              Node* Word32Ror(Node* a, Node* b) {\n                return AddNode(machine()->Word32Ror(), a, b);\n              }\n              Node* Word32Clz(Node* a) { return AddNode(machine()->Word32Clz(), a); }\n              Node* Word32Equal(Node* a, Node* b) {\n                return AddNode(machine()->Word32Equal(), a, b);\n              }\n              Node* Word32NotEqual(Node* a, Node* b) {\n                return Word32BinaryNot(Word32Equal(a, b));\n              }\n              Node* Word32BitwiseNot(Node* a) { return Word32Xor(a, Int32Constant(-1)); }\n              Node* Word32BinaryNot(Node* a) { return Word32Equal(a, Int32Constant(0)); }\n\n              Node* Word64And(Node* a, Node* b) {\n                return AddNode(machine()->Word64And(), a, b);\n              }\n              Node* Word64Or(Node* a, Node* b) {\n                return AddNode(machine()->Word64Or(), a, b);\n              }\n              Node* Word64Xor(Node* a, Node* b) {\n                return AddNode(machine()->Word64Xor(), a, b);\n              }\n              Node* Word64Shl(Node* a, Node* b) {\n                return AddNode(machine()->Word64Shl(), a, b);\n              }\n              Node* Word64Shr(Node* a, Node* b) {\n                return AddNode(machine()->Word64Shr(), a, b);\n              }\n              Node* Word64Sar(Node* a, Node* b) {\n                return AddNode(machine()->Word64Sar(), a, b);\n              }\n              Node* Word64Ror(Node* a, Node* b) {\n                return AddNode(machine()->Word64Ror(), a, b);\n              }\n              Node* Word64Clz(Node* a) { return AddNode(machine()->Word64Clz(), a); }\n              Node* Word64Equal(Node* a, Node* b) {\n                return AddNode(machine()->Word64Equal(), a, b);\n              }\n              Node* Word64NotEqual(Node* a, Node* b) {\n                return Word32BinaryNot(Word64Equal(a, b));\n              }\n              Node* Word64Not(Node* a) { return Word64Xor(a, Int64Constant(-1)); }\n\n              Node* Int32Add(Node* a, Node* b) {\n                return AddNode(machine()->Int32Add(), a, b);\n              }\n              Node* Int32AddWithOverflow(Node* a, Node* b) {\n                return AddNode(machine()->Int32AddWithOverflow(), a, b);\n              }\n              Node* Int32Sub(Node* a, Node* b) {\n                return AddNode(machine()->Int32Sub(), a, b);\n              }\n              Node* Int32SubWithOverflow(Node* a, Node* b) {\n                return AddNode(machine()->Int32SubWithOverflow(), a, b);\n              }\n              Node* Int32Mul(Node* a, Node* b) {\n                return AddNode(machine()->Int32Mul(), a, b);\n              }\n              Node* Int32MulHigh(Node* a, Node* b) {\n                return AddNode(machine()->Int32MulHigh(), a, b);\n              }\n              Node* Int32MulWithOverflow(Node* a, Node* b) {\n                return AddNode(machine()->Int32MulWithOverflow(), a, b);\n              }\n              Node* Int32Div(Node* a, Node* b) {\n                return AddNode(machine()->Int32Div(), a, b);\n              }\n              Node* Int32Mod(Node* a, Node* b) {\n                return AddNode(machine()->Int32Mod(), a, b);\n              }\n              Node* Int32LessThan(Node* a, Node* b) {\n                return AddNode(machine()->Int32LessThan(), a, b);\n              }\n              Node* Int32LessThanOrEqual(Node* a, Node* b) {\n                return AddNode(machine()->Int32LessThanOrEqual(), a, b);\n              }\n              Node* Uint32Div(Node* a, Node* b) {\n                return AddNode(machine()->Uint32Div(), a, b);\n              }\n              Node* Uint32LessThan(Node* a, Node* b) {\n                return AddNode(machine()->Uint32LessThan(), a, b);\n              }\n              Node* Uint32LessThanOrEqual(Node* a, Node* b) {\n                return AddNode(machine()->Uint32LessThanOrEqual(), a, b);\n              }\n              Node* Uint32Mod(Node* a, Node* b) {\n                return AddNode(machine()->Uint32Mod(), a, b);\n              }\n              Node* Uint32MulHigh(Node* a, Node* b) {\n                return AddNode(machine()->Uint32MulHigh(), a, b);\n              }\n              Node* Int32GreaterThan(Node* a, Node* b) { return Int32LessThan(b, a); }\n              Node* Int32GreaterThanOrEqual(Node* a, Node* b) {\n                return Int32LessThanOrEqual(b, a);\n              }\n              Node* Uint32GreaterThan(Node* a, Node* b) { return Uint32LessThan(b, a); }\n              Node* Uint32GreaterThanOrEqual(Node* a, Node* b) {\n                return Uint32LessThanOrEqual(b, a);\n              }\n              Node* Int32Neg(Node* a) { return Int32Sub(Int32Constant(0), a); }\n\n              Node* Int64Add(Node* a, Node* b) {\n                return AddNode(machine()->Int64Add(), a, b);\n              }\n              Node* Int64AddWithOverflow(Node* a, Node* b) {\n                return AddNode(machine()->Int64AddWithOverflow(), a, b);\n              }\n              Node* Int64Sub(Node* a, Node* b) {\n                return AddNode(machine()->Int64Sub(), a, b);\n              }\n              Node* Int64SubWithOverflow(Node* a, Node* b) {\n                return AddNode(machine()->Int64SubWithOverflow(), a, b);\n              }\n              Node* Int64Mul(Node* a, Node* b) {\n                return AddNode(machine()->Int64Mul(), a, b);\n              }\n              Node* Int64MulHigh(Node* a, Node* b) {\n                return AddNode(machine()->Int64MulHigh(), a, b);\n              }\n              Node* Uint64MulHigh(Node* a, Node* b) {\n                return AddNode(machine()->Uint64MulHigh(), a, b);\n              }\n              Node* Int64MulWithOverflow(Node* a, Node* b) {\n                return AddNode(machine()->Int64MulWithOverflow(), a, b);\n              }\n              Node* Int64Div(Node* a, Node* b) {\n                return AddNode(machine()->Int64Div(), a, b);\n              }\n              Node* Int64Mod(Node* a, Node* b) {\n                return AddNode(machine()->Int64Mod(), a, b);\n              }\n              Node* Int64Neg(Node* a) { return Int64Sub(Int64Constant(0), a); }\n              Node* Int64LessThan(Node* a, Node* b) {\n                return AddNode(machine()->Int64LessThan(), a, b);\n              }\n              Node* Int64LessThanOrEqual(Node* a, Node* b) {\n                return AddNode(machine()->Int64LessThanOrEqual(), a, b);\n              }\n              Node* Uint64LessThan(Node* a, Node* b) {\n                return AddNode(machine()->Uint64LessThan(), a, b);\n              }\n              Node* Uint64LessThanOrEqual(Node* a, Node* b) {\n                return AddNode(machine()->Uint64LessThanOrEqual(), a, b);\n              }\n              Node* Int64GreaterThan(Node* a, Node* b) { return Int64LessThan(b, a); }\n              Node* Int64GreaterThanOrEqual(Node* a, Node* b) {\n                return Int64LessThanOrEqual(b, a);\n              }\n              Node* Uint64GreaterThan(Node* a, Node* b) { return Uint64LessThan(b, a); }\n              Node* Uint64GreaterThanOrEqual(Node* a, Node* b) {\n                return Uint64LessThanOrEqual(b, a);\n              }\n              Node* Uint64Div(Node* a, Node* b) {\n                return AddNode(machine()->Uint64Div(), a, b);\n              }\n              Node* Uint64Mod(Node* a, Node* b) {\n                return AddNode(machine()->Uint64Mod(), a, b);\n              }\n              Node* Int32PairAdd(Node* a_low, Node* a_high, Node* b_low, Node* b_high) {\n                return AddNode(machine()->Int32PairAdd(), a_low, a_high, b_low, b_high);\n              }\n              Node* Int32PairSub(Node* a_low, Node* a_high, Node* b_low, Node* b_high) {\n                return AddNode(machine()->Int32PairSub(), a_low, a_high, b_low, b_high);\n              }\n              Node* Int32PairMul(Node* a_low, Node* a_high, Node* b_low, Node* b_high) {\n                return AddNode(machine()->Int32PairMul(), a_low, a_high, b_low, b_high);\n              }\n              Node* Word32PairShl(Node* low_word, Node* high_word, Node* shift) {\n                return AddNode(machine()->Word32PairShl(), low_word, high_word, shift);\n              }\n              Node* Word32PairShr(Node* low_word, Node* high_word, Node* shift) {\n                return AddNode(machine()->Word32PairShr(), low_word, high_word, shift);\n              }\n              Node* Word32PairSar(Node* low_word, Node* high_word, Node* shift) {\n                return AddNode(machine()->Word32PairSar(), low_word, high_word, shift);\n              }\n              Node* Word32Popcnt(Node* a) {\n                return AddNode(machine()->Word32Popcnt().op(), a);\n              }\n              Node* Word64Popcnt(Node* a) {\n                return AddNode(machine()->Word64Popcnt().op(), a);\n              }\n              Node* Word32Ctz(Node* a) { return AddNode(machine()->Word32Ctz().op(), a); }\n              Node* Word64Ctz(Node* a) { return AddNode(machine()->Word64Ctz().op(), a); }\n\n              Node* Word32Select(Node* condition, Node* b, Node* c) {\n                return AddNode(machine()->Word32Select().op(), condition, b, c);\n              }\n\n              Node* Word64Select(Node* condition, Node* b, Node* c) {\n                return AddNode(machine()->Word64Select().op(), condition, b, c);\n              }\n\n              Node* StackPointerGreaterThan(Node* value) {\n                return AddNode(\n                    machine()->StackPointerGreaterThan(StackCheckKind::kCodeStubAssembler),\n                    value);\n              }\n\n            #define INTPTR_BINOP(prefix, name)                           \\\n              Node* IntPtr##name(Node* a, Node* b) {                     \\\n                return kSystemPointerSize == 8 ? prefix##64##name(a, b)  \\\n                                               : prefix##32##name(a, b); \\\n              }\n\n              INTPTR_BINOP(Int, Add)\n              INTPTR_BINOP(Int, AddWithOverflow)\n              INTPTR_BINOP(Int, Sub)\n              INTPTR_BINOP(Int, SubWithOverflow)\n              INTPTR_BINOP(Int, Mul)\n              INTPTR_BINOP(Int, MulHigh)\n              INTPTR_BINOP(Int, MulWith"
}