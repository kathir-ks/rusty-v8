{
  "file_path": "/home/kathirks_gc/v8_go/archive/codebase/src/compiler/turboshaft/memory-optimization-reducer.h",
  "error": "JSON and XML parsing failed",
  "json_error_if_any": "Skipped JSON parsing for whole response due to presence of XML tags; XML is primary.",
  "xml_error": "not well-formed (invalid token): line 83, column 49",
  "raw_response": "```xml\n<file>\n    <metadata>\n        {\n            \"path\": \"/home/kathirks_gc/v8_go/archive/codebase/src/compiler/turboshaft/memory-optimization-reducer.h\",\n            \"file_name\": \"memory-optimization-reducer.h\",\n            \"language\": \"cpp\",\n            \"purpose\": \"Defines the MemoryOptimizationReducer class for the Turboshaft compiler, which aims to optimize memory allocation by folding allocations and eliminating write barriers.\"\n        }\n    </metadata>\n    <imports>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"purpose\": \"Includes standard library and V8 headers for optional types, base utilities, built-in functions, code generation, compiler phases, operations, opmasks, utilities, write barrier kinds, and zone containers.\"\n            }\n        </metadata>\n        <code><![CDATA[\n#include <optional>\n\n#include \"src/base/template-utils.h\"\n#include \"src/builtins/builtins.h\"\n#include \"src/codegen/external-reference.h\"\n#include \"src/compiler/turboshaft/assembler.h\"\n#include \"src/compiler/turboshaft/copying-phase.h\"\n#include \"src/compiler/turboshaft/operations.h\"\n#include \"src/compiler/turboshaft/opmasks.h\"\n#include \"src/compiler/turboshaft/phase.h\"\n#include \"src/compiler/turboshaft/utils.h\"\n#include \"src/compiler/write-barrier-kind.h\"\n#include \"src/zone/zone-containers.h\"\n        ]]></code>\n    </imports>\n    <func>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"type\": \"function\",\n                \"name\": \"CreateAllocateBuiltinDescriptor\",\n                \"parent\": null,\n                \"about\": \"Creates a call descriptor for the allocate builtin function.\",\n                \"logic\": \"The function likely constructs a TSCallDescriptor object that describes the signature and calling convention of the allocation builtin function. This descriptor is used during code generation to ensure the call to the builtin is performed correctly.\",\n                \"parameters\": [\n                    {\n                        \"name\": \"zone\",\n                        \"type\": \"Zone*\",\n                        \"purpose\": \"The memory zone to allocate the descriptor in.\"\n                    },\n                    {\n                        \"name\": \"isolate\",\n                        \"type\": \"Isolate*\",\n                        \"purpose\": \"The isolate associated with the compilation.\"\n                    }\n                ],\n                \"return\": {\n                    \"type\": \"const TSCallDescriptor*\",\n                    \"description\": \"A pointer to the created TSCallDescriptor.\"\n                },\n                \"dependencies\": []\n            }\n        </metadata>\n        <code><![CDATA[\nconst TSCallDescriptor* CreateAllocateBuiltinDescriptor(Zone* zone,\n                                                        Isolate* isolate);\n        ]]></code>\n    </func>\n    <func>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"type\": \"function\",\n                \"name\": \"ValueNeedsWriteBarrier\",\n                \"parent\": null,\n                \"about\": \"Determines whether a given value needs a write barrier.\",\n                \"logic\": \"Checks if the value is a bitcast from WordPtr to Smi or a constant heap object that's an immortal immovable root.  If it's a Phi node, it recursively checks the inputs to the Phi.\",\n                \"parameters\": [\n                    {\n                        \"name\": \"graph\",\n                        \"type\": \"const Graph*\",\n                        \"purpose\": \"The Turboshaft graph.\"\n                    },\n                    {\n                        \"name\": \"value\",\n                        \"type\": \"const Operation&\",\n                        \"purpose\": \"The operation representing the value.\"\n                    },\n                    {\n                        \"name\": \"isolate\",\n                        \"type\": \"Isolate*\",\n                        \"purpose\": \"The isolate associated with the compilation.\"\n                    }\n                ],\n                \"return\": {\n                    \"type\": \"bool\",\n                    \"description\": \"True if the value needs a write barrier, false otherwise.\"\n                },\n                \"dependencies\": [\n                    \"Opmask::kBitcastWordPtrToSmi\",\n                    \"ConstantOp\",\n                    \"PhiOp\",\n                    \"RootsTable::IsImmortalImmovable\",\n                    \"base::any_of\"\n                ]\n            }\n        </metadata>\n        <code><![CDATA[\ninline bool ValueNeedsWriteBarrier(const Graph* graph, const Operation& value,\n                                   Isolate* isolate) {\n  if (value.Is<Opmask::kBitcastWordPtrToSmi>()) {\n    return false;\n  } else if (const ConstantOp* constant = value.TryCast<ConstantOp>()) {\n    if (constant->kind == ConstantOp::Kind::kHeapObject) {\n      RootIndex root_index;\n      if (isolate->roots_table().IsRootHandle(constant->handle(),\n                                              &root_index) &&\n          RootsTable::IsImmortalImmovable(root_index)) {\n        return false;\n      }\n    }\n  } else if (const PhiOp* phi = value.TryCast<PhiOp>()) {\n    if (phi->rep == RegisterRepresentation::Tagged()) {\n      return base::any_of(phi->inputs(), [graph, isolate](OpIndex input) {\n        const Operation& input_op = graph->Get(input);\n        // If we have a Phi as the Phi's input, we give up to avoid infinite\n        // recursion.\n        if (input_op.Is<PhiOp>()) return true;\n        return ValueNeedsWriteBarrier(graph, input_op, isolate);\n      });\n    }\n  }\n  return true;\n}\n        ]]></code>\n    </func>\n    <func>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"type\": \"function\",\n                \"name\": \"UnwrapAllocate\",\n                \"parent\": null,\n                \"about\": \"Unwraps an allocation operation by traversing through TaggedBitcastOp and WordBinopOp (Add/Sub) operations.\",\n                \"logic\": \"The function iteratively checks if the given operation is an AllocateOp, TaggedBitcastOp, or a WordBinopOp (Add/Sub). If it's a TaggedBitcastOp or WordBinopOp, it retrieves the input/left operand and continues the unwrapping process. It returns the underlying AllocateOp if found, otherwise returns nullptr.\",\n                \"parameters\": [\n                    {\n                        \"name\": \"graph\",\n                        \"type\": \"const Graph*\",\n                        \"purpose\": \"The Turboshaft graph.\"\n                    },\n                    {\n                        \"name\": \"op\",\n                        \"type\": \"const Operation*\",\n                        \"purpose\": \"The operation to unwrap.\"\n                    }\n                ],\n                \"return\": {\n                    \"type\": \"const AllocateOp*\",\n                    \"description\": \"The underlying AllocateOp if found, nullptr otherwise.\"\n                },\n                \"dependencies\": [\n                    \"AllocateOp\",\n                    \"TaggedBitcastOp\",\n                    \"WordBinopOp\"\n                ]\n            }\n        </metadata>\n        <code><![CDATA[\ninline const AllocateOp* UnwrapAllocate(const Graph* graph,\n                                        const Operation* op) {\n  while (true) {\n    if (const AllocateOp* allocate = op->TryCast<AllocateOp>()) {\n      return allocate;\n    } else if (const TaggedBitcastOp* bitcast =\n                   op->TryCast<TaggedBitcastOp>()) {\n      op = &graph->Get(bitcast->input());\n    } else if (const WordBinopOp* binop = op->TryCast<WordBinopOp>();\n               binop && binop->kind == any_of(WordBinopOp::Kind::kAdd,\n                                              WordBinopOp::Kind::kSub)) {\n      op = &graph->Get(binop->left());\n    } else {\n      return nullptr;\n    }\n  }\n}\n        ]]></code>\n    </func>\n    <class>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"type\": \"struct\",\n                \"name\": \"MemoryAnalyzer\",\n                \"extends\": null,\n                \"implements\": [],\n                \"about\": \"Analyzes memory allocation patterns to fold allocations and eliminate write barriers.\",\n                \"attributes\": [\n                    {\n                        \"name\": \"data\",\n                        \"type\": \"PipelineData*\",\n                        \"access\": \"public\",\n                        \"purpose\": \"Pipeline data for the Turboshaft compilation pipeline.\"\n                    },\n                    {\n                        \"name\": \"phase_zone\",\n                        \"type\": \"Zone*\",\n                        \"access\": \"public\",\n                        \"purpose\": \"Memory zone for allocating data during the analysis phase.\"\n                    },\n                    {\n                        \"name\": \"input_graph\",\n                        \"type\": \"const Graph&\",\n                        \"access\": \"public\",\n                        \"purpose\": \"The Turboshaft graph being analyzed.\"\n                    },\n                    {\n                        \"name\": \"isolate_\",\n                        \"type\": \"Isolate*\",\n                        \"access\": \"private\",\n                        \"purpose\": \"The isolate associated with the compilation.\"\n                    },\n                    {\n                        \"name\": \"allocation_folding\",\n                        \"type\": \"AllocationFolding\",\n                        \"access\": \"public\",\n                        \"purpose\": \"Enum that indicates whether to do allocation folding or not.\"\n                    },\n                    {\n                        \"name\": \"is_wasm\",\n                        \"type\": \"bool\",\n                        \"access\": \"public\",\n                        \"purpose\": \"Flag indicating if the code being compiled is WebAssembly.\"\n                    },\n                    {\n                        \"name\": \"block_states\",\n                        \"type\": \"FixedBlockSidetable<std::optional<BlockState>>\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Stores block states for each block in the graph.\"\n                    },\n                    {\n                        \"name\": \"folded_into\",\n                        \"type\": \"ZoneAbslFlatHashMap<const AllocateOp*, const AllocateOp*>\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Maps folded allocation operations to the allocation they are folded into.\"\n                    },\n                    {\n                        \"name\": \"skipped_write_barriers\",\n                        \"type\": \"ZoneAbslFlatHashSet<V<None>>\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Stores the indices of write barriers that were skipped.\"\n                    },\n                    {\n                        \"name\": \"reserved_size\",\n                        \"type\": \"ZoneAbslFlatHashMap<const AllocateOp*, uint32_t>\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Stores the reserved size for each allocation operation.\"\n                    },\n                    {\n                        \"name\": \"current_block\",\n                        \"type\": \"BlockIndex\",\n                        \"access\": \"private\",\n                        \"purpose\": \"The current block being processed.\"\n                    },\n                    {\n                        \"name\": \"state\",\n                        \"type\": \"BlockState\",\n                        \"access\": \"private\",\n                        \"purpose\": \"The current state of the memory analysis.\"\n                    },\n                    {\n                        \"name\": \"pipeline_kind\",\n                        \"type\": \"TurboshaftPipelineKind\",\n                        \"access\": \"private\",\n                        \"purpose\": \"The kind of Turboshaft pipeline.\"\n                    }\n                ],\n                \"dependencies\": [\n                    \"PipelineData\",\n                    \"Graph\",\n                    \"AllocateOp\",\n                    \"FixedBlockSidetable\",\n                    \"ZoneAbslFlatHashMap\",\n                    \"ZoneAbslFlatHashSet\",\n                    \"V\"\n                ]\n            }\n        </metadata>\n        <code><![CDATA[\nstruct MemoryAnalyzer {\n  enum class AllocationFolding { kDoAllocationFolding, kDontAllocationFolding };\n\n  PipelineData* data;\n  Zone* phase_zone;\n  const Graph& input_graph;\n  Isolate* isolate_ = data->isolate();\n  AllocationFolding allocation_folding;\n  bool is_wasm;\n  MemoryAnalyzer(PipelineData* data, Zone* phase_zone, const Graph& input_graph,\n                 AllocationFolding allocation_folding, bool is_wasm)\n      : data(data),\n        phase_zone(phase_zone),\n        input_graph(input_graph),\n        allocation_folding(allocation_folding),\n        is_wasm(is_wasm) {}\n\n  struct BlockState {\n    const AllocateOp* last_allocation = nullptr;\n    std::optional<uint32_t> reserved_size = std::nullopt;\n\n    bool operator!=(const BlockState& other) {\n      return last_allocation != other.last_allocation ||\n             reserved_size != other.reserved_size;\n    }\n  };\n  FixedBlockSidetable<std::optional<BlockState>> block_states{\n      input_graph.block_count(), phase_zone};\n  ZoneAbslFlatHashMap<const AllocateOp*, const AllocateOp*> folded_into{\n      phase_zone};\n  ZoneAbslFlatHashSet<V<None>> skipped_write_barriers{phase_zone};\n  ZoneAbslFlatHashMap<const AllocateOp*, uint32_t> reserved_size{phase_zone};\n  BlockIndex current_block = BlockIndex(0);\n  BlockState state;\n  TurboshaftPipelineKind pipeline_kind = data->pipeline_kind();\n\n  bool IsPartOfLastAllocation(const Operation* op) {\n    const AllocateOp* allocation = UnwrapAllocate(&input_graph, op);\n    if (allocation == nullptr) return false;\n    if (state.last_allocation == nullptr) return false;\n    if (state.last_allocation->type != AllocationType::kYoung) return false;\n    if (state.last_allocation == allocation) return true;\n    auto it = folded_into.find(allocation);\n    if (it == folded_into.end()) return false;\n    return it->second == state.last_allocation;\n  }\n\n  bool SkipWriteBarrier(const StoreOp& store) {\n    const Operation& object = input_graph.Get(store.base());\n    const Operation& value = input_graph.Get(store.value());\n\n    WriteBarrierKind write_barrier_kind = store.write_barrier;\n    if (write_barrier_kind != WriteBarrierKind::kAssertNoWriteBarrier) {\n      // If we have {kAssertNoWriteBarrier}, we cannot skip elimination\n      // checks.\n      if (ShouldSkipOptimizationStep()) return false;\n    }\n    if (IsPartOfLastAllocation(&object)) return true;\n    if (!ValueNeedsWriteBarrier(&input_graph, value, isolate_)) return true;\n    if (v8_flags.disable_write_barriers) return true;\n    if (write_barrier_kind == WriteBarrierKind::kAssertNoWriteBarrier) {\n      std::stringstream str;\n      str << \"MemoryOptimizationReducer could not remove write barrier for \"\n             \"operation\\n  #\"\n          << input_graph.Index(store) << \": \" << store.ToString() << \"\\n\";\n      FATAL(\"%s\", str.str().c_str());\n    }\n    return false;\n  }\n\n  bool IsFoldedAllocation(V<AnyOrNone> op) {\n    return folded_into.count(\n        input_graph.Get(op).template TryCast<AllocateOp>());\n  }\n\n  std::optional<uint32_t> ReservedSize(V<AnyOrNone> alloc) {\n    if (auto it = reserved_size.find(\n            input_graph.Get(alloc).template TryCast<AllocateOp>());\n        it != reserved_size.end()) {\n      return it->second;\n    }\n    return std::nullopt;\n  }\n\n  void Run();\n\n  void Process(const Operation& op);\n  void ProcessBlockTerminator(const Operation& op);\n  void ProcessAllocation(const AllocateOp& alloc);\n  void ProcessStore(const StoreOp& store);\n  void MergeCurrentStateIntoSuccessor(const Block* successor);\n};\n        ]]></code>\n    </class>\n    <class>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"type\": \"class\",\n                \"name\": \"MemoryOptimizationReducer\",\n                \"extends\": \"Next\",\n                \"implements\": [],\n                \"about\": \"Reduces the Turboshaft graph by optimizing memory allocations and write barriers.\",\n                \"attributes\": [\n                    {\n                        \"name\": \"analyzer_\",\n                        \"type\": \"std::optional<MemoryAnalyzer>\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Optional MemoryAnalyzer instance to perform memory analysis.\"\n                    },\n                    {\n                        \"name\": \"isolate_\",\n                        \"type\": \"Isolate*\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Isolate associated with the compilation.\"\n                    },\n                    {\n                        \"name\": \"allocate_builtin_descriptor_\",\n                        \"type\": \"const TSCallDescriptor*\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Descriptor for the allocation builtin function.\"\n                    },\n                    {\n                        \"name\": \"top_\",\n                        \"type\": \"std::optional<Variable>[2]\",\n                        \"access\": \"private\",\n                        \"purpose\": \"Variables holding the top of the allocation space for young and old generations.\"\n                    }\n                ],\n                \"dependencies\": [\n                    \"Next\",\n                    \"MemoryAnalyzer\",\n                    \"StoreOp\",\n                    \"AllocateOp\",\n                    \"WriteBarrierKind\",\n                    \"ExternalReference\",\n                    \"Builtin\",\n                    \"CallTarget\",\n                    \"Block\",\n                    \"HeapObject\",\n                    \"WasmTrustedInstanceData\",\n                    \"LoadOp\",\n                    \"IsolateData\",\n                    \"DecodeExternalPointer\",\n                    \"ExternalPointerTagRange\"\n                ]\n            }\n        </metadata>\n        <code><![CDATA[\ntemplate <class Next>\nclass MemoryOptimizationReducer : public Next {\n public:\n  TURBOSHAFT_REDUCER_BOILERPLATE(MemoryOptimization)\n  // TODO(dmercadier): Add static_assert that this is ran as part of a\n  // CopyingPhase.\n\n  void Analyze() {\n    auto* info = __ data() -> info();\n#if V8_ENABLE_WEBASSEMBLY\n    bool is_wasm = info->IsWasm() || info->IsWasmBuiltin();\n#else\n    bool is_wasm = false;\n#endif\n    analyzer_.emplace(\n        __ data(), __ phase_zone(), __ input_graph(),\n        info->allocation_folding()\n            ? MemoryAnalyzer::AllocationFolding::kDoAllocationFolding\n            : MemoryAnalyzer::AllocationFolding::kDontAllocationFolding,\n        is_wasm);\n    analyzer_->Run();\n    Next::Analyze();\n  }\n\n  V<None> REDUCE_INPUT_GRAPH(Store)(V<None> ig_index, const StoreOp& store) {\n    if (store.write_barrier != WriteBarrierKind::kAssertNoWriteBarrier) {\n      // We cannot skip this optimization if we have to eliminate a\n      // {kAssertNoWriteBarrier}.\n      if (ShouldSkipOptimizationStep()) {\n        return Next::ReduceInputGraphStore(ig_index, store);\n      }\n    }\n    if (analyzer_->skipped_write_barriers.count(ig_index)) {\n      __ Store(__ MapToNewGraph(store.base()), __ MapToNewGraph(store.index()),\n               __ MapToNewGraph(store.value()), store.kind, store.stored_rep,\n               WriteBarrierKind::kNoWriteBarrier, store.offset,\n               store.element_size_log2,\n               store.maybe_initializing_or_transitioning,\n               store.indirect_pointer_tag());\n      return V<None>::Invalid();\n    }\n    DCHECK_NE(store.write_barrier, WriteBarrierKind::kAssertNoWriteBarrier);\n    return Next::ReduceInputGraphStore(ig_index, store);\n  }\n\n  V<HeapObject> REDUCE(Allocate)(V<WordPtr> size, AllocationType type) {\n    DCHECK_EQ(type, any_of(AllocationType::kYoung, AllocationType::kOld));\n\n    if (v8_flags.single_generation && type == AllocationType::kYoung) {\n      type = AllocationType::kOld;\n    }\n\n    V<WordPtr> top_address;\n    if (isolate_ != nullptr) {\n      top_address = __ ExternalConstant(\n          type == AllocationType::kYoung\n              ? ExternalReference::new_space_allocation_top_address(isolate_)\n              : ExternalReference::old_space_allocation_top_address(isolate_));\n    } else {\n      // Wasm mode: producing isolate-independent code, loading the isolate\n      // address at runtime.\n#if V8_ENABLE_WEBASSEMBLY\n      V<WasmTrustedInstanceData> instance_data = __ WasmInstanceDataParameter();\n      int top_address_offset =\n          type == AllocationType::kYoung\n              ? WasmTrustedInstanceData::kNewAllocationTopAddressOffset\n              : WasmTrustedInstanceData::kOldAllocationTopAddressOffset;\n      top_address =\n          __ Load(instance_data, LoadOp::Kind::TaggedBase().Immutable(),\n                  MemoryRepresentation::UintPtr(), top_address_offset);\n#else\n      UNREACHABLE();\n#endif  // V8_ENABLE_WEBASSEMBLY\n    }\n\n    if (analyzer_->IsFoldedAllocation(__ current_operation_origin())) {\n      DCHECK_NE(__ GetVariable(top(type)), V<WordPtr>::Invalid());\n      V<WordPtr> obj_addr = __ GetVariable(top(type));\n      __ SetVariable(top(type), __ WordPtrAdd(__ GetVariable(top(type)), size));\n      __ StoreOffHeap(top_address, __ GetVariable(top(type)),\n                      MemoryRepresentation::UintPtr());\n      return __ BitcastWordPtrToHeapObject(\n          __ WordPtrAdd(obj_addr, __ IntPtrConstant(kHeapObjectTag)));\n    }\n\n    __ SetVariable(top(type), __ LoadOffHeap(top_address,\n                                             MemoryRepresentation::UintPtr()));\n\n    V<CallTarget> allocate_builtin;\n    if (!analyzer_->is_wasm) {\n      if (type == AllocationType::kYoung) {\n        allocate_builtin =\n            __ BuiltinCode(Builtin::kAllocateInYoungGeneration, isolate_);\n      } else {\n        allocate_builtin =\n            __ BuiltinCode(Builtin::kAllocateInOldGeneration, isolate_);\n      }\n    } else {\n#if V8_ENABLE_WEBASSEMBLY\n      // This lowering is used by Wasm, where we compile isolate-independent\n      // code. Builtin calls simply encode the target builtin ID, which will\n      // be patched to the builtin's address later.\n      if (isolate_ == nullptr) {\n        Builtin builtin;\n        if (type == AllocationType::kYoung) {\n          builtin = Builtin::kWasmAllocateInYoungGeneration;\n        } else {\n          builtin = Builtin::kWasmAllocateInOldGeneration;\n        }\n        static_assert(std::is_same<Smi, BuiltinPtr>(),\n                      \"BuiltinPtr must be Smi\");\n        allocate_builtin = __ NumberConstant(static_cast<int>(builtin));\n      } else {\n        if (type == AllocationType::kYoung) {\n          allocate_builtin =\n              __ BuiltinCode(Builtin::kWasmAllocateInYoungGeneration, isolate_);\n        } else {\n          allocate_builtin =\n              __ BuiltinCode(Builtin::kWasmAllocateInOldGeneration, isolate_);\n        }\n      }\n#else\n      UNREACHABLE();\n#endif\n    }\n\n    Block* call_runtime = __ NewBlock();\n    Block* done = __ NewBlock();\n\n    V<WordPtr> limit_address = GetLimitAddress(type);\n\n    // If the allocation size is not statically known or is known to be larger\n    // than kMaxRegularHeapObjectSize, do not update {top(type)} in case of a\n    // runtime call. This is needed because we cannot allocation-fold large and\n    // normal-sized objects.\n    uint64_t constant_size{};\n    if (!__ matcher().MatchIntegralWordConstant(\n            size, WordRepresentation::WordPtr(), &constant_size) ||\n        constant_size > kMaxRegularHeapObjectSize) {\n      Variable result =\n          __ NewLoopInvariantVariable(RegisterRepresentation::Tagged());\n      if (!constant_size) {\n        // Check if we can do bump pointer allocation here.\n        V<WordPtr> top_value = __ GetVariable(top(type));\n        __ SetVariable(result,\n                       __ BitcastWordPtrToHeapObject(__ WordPtrAdd(\n                           top_value, __ IntPtrConstant(kHeapObjectTag))));\n        V<WordPtr> new_top = __ WordPtrAdd(top_value, size);\n        V<WordPtr> limit =\n            __ LoadOffHeap(limit_address, MemoryRepresentation::UintPtr());\n        __ GotoIfNot(LIKELY(__ UintPtrLessThan(new_top, limit)), call_runtime);\n        __ GotoIfNot(LIKELY(__ UintPtrLessThan(\n                         size, __ IntPtrConstant(kMaxRegularHeapObjectSize))),\n                     call_runtime);\n        __ SetVariable(top(type), new_top);\n        __ StoreOffHeap(top_address, new_top, MemoryRepresentation::UintPtr());\n        __ Goto(done);\n      }\n      if (constant_size || __ Bind(call_runtime)) {\n        __ SetVariable(\n            result, __ template Call<HeapObject>(allocate_builtin, {size},\n                                                 AllocateBuiltinDescriptor()));\n        __ Goto(done);\n      }\n\n      __ BindReachable(done);\n      return __ GetVariable(result);\n    }\n\n    V<WordPtr> reservation_size;\n    if (auto c = analyzer_->ReservedSize(__ current_operation_origin())) {\n      reservation_size = __ UintPtrConstant(*c);\n    } else {\n      reservation_size = size;\n    }\n    // Check if we can do bump pointer allocation here.\n    bool reachable =\n        __ GotoIfNot(__ UintPtrLessThan(\n                         size, __ IntPtrConstant(kMaxRegularHeapObjectSize)),\n                     call_runtime, BranchHint::kTrue) !=\n        ConditionalGotoStatus::kGotoDestination;\n    if (reachable) {\n      V<WordPtr> limit =\n          __ LoadOffHeap(limit_address, MemoryRepresentation::UintPtr());\n      __ Branch(__ UintPtrLessThan(\n                    __ WordPtrAdd(__ GetVariable(top(type)), reservation_size),\n                    limit),\n                done, call_runtime, BranchHint::kTrue);\n    }\n\n    // Call the runtime if bump pointer area exhausted.\n    if (__ Bind(call_runtime)) {\n      V<HeapObject> allocated = __ template Call<HeapObject>(\n          allocate_builtin, {reservation_size}, AllocateBuiltinDescriptor());\n      __ SetVariable(top(type),\n                     __ WordPtrSub(__ BitcastHeapObjectToWordPtr(allocated),\n                                   __ IntPtrConstant(kHeapObjectTag)));\n      __ Goto(done);\n    }\n\n    __ BindReachable(done);\n    // Compute the new top and write it back.\n    V<WordPtr> obj_addr = __ GetVariable(top(type));\n    __ SetVariable(top(type), __ WordPtrAdd(__ GetVariable(top(type)), size));\n    __ StoreOffHeap(top_address, __ GetVariable(top(type)),\n                    MemoryRepresentation::UintPtr());\n    return __ BitcastWordPtrToHeapObject(\n        __ WordPtrAdd(obj_addr, __ IntPtrConstant(kHeapObjectTag)));\n  }\n\n  OpIndex REDUCE(DecodeExternalPointer)(OpIndex handle,\n                                        ExternalPointerTagRange tag_range) {\n#ifdef V8_ENABLE_SANDBOX\n    // Decode loaded external pointer.\n    V<WordPtr> table;\n    if (isolate_ != nullptr) {\n      // Here we access the external pointer table through an ExternalReference.\n      // Alternatively, we could also hardcode the address of the table since it\n      // is never reallocated. However, in that case we must be able to\n      // guarantee that the generated code is never executed under a different\n      // Isolate, as that would allow access to external objects from different\n      // Isolates. It also would break if the code is serialized/deserialized at\n      // some point.\n      V<WordPtr> table_address =\n          IsSharedExternalPointerType(tag_range)\n              ? __\n                LoadOffHeap(\n                    __ ExternalConstant(\n                        ExternalReference::\n                            shared_external_pointer_table_address_address(\n                                isolate_)),\n                    MemoryRepresentation::UintPtr())\n              : __ ExternalConstant(\n                    ExternalReference::external_pointer_table_address(\n                        isolate_));\n      table = __ LoadOffHeap(table_address,\n                             Internals::kExternalPointerTableBasePointerOffset,\n                             MemoryRepresentation::UintPtr());\n    } else {\n#if V8_ENABLE_WEBASSEMBLY\n      V<WordPtr> isolate_root = __ LoadRootRegister();\n      if (IsSharedExternalPointerType(tag_range)) {\n        V<WordPtr> table_address =\n            __ Load(isolate_root, LoadOp::Kind::RawAligned(),\n                    MemoryRepresentation::UintPtr(),\n                    IsolateData::shared_external_pointer_table_offset());\n        table = __ Load(table_address, LoadOp::Kind::RawAligned(),\n                        MemoryRepresentation::UintPtr(),\n                        Internals::kExternalPointerTableBasePointerOffset);\n      } else {\n        table = __ Load(isolate_root, LoadOp::Kind::RawAligned(),\n                        MemoryRepresentation::UintPtr(),\n                        IsolateData::external_pointer_table_offset() +\n                            Internals::kExternalPointerTableBasePointerOffset);\n      }\n#else\n      UNREACHABLE();\n#endif\n    }\n\n    V<Word32> index =\n        __ Word32ShiftRightLogical(handle, kExternalPointerIndexShift);\n    V<Word64> pointer = __ LoadOffHeap(table, __ ChangeUint32ToUint64(index), 0,\n                                       MemoryRepresentation::Uint64());\n\n    // We don't expect to see empty fields here. If this is ever needed,\n    // consider using an dedicated empty value entry for those tags instead\n    // (i.e. an entry with the right tag and nullptr payload).\n    DCHECK(!ExternalPointerCanBeEmpty(tag_range));\n\n    Block* done = __ NewBlock();\n    if (tag_range.Size() == 1) {\n      // The common and simple case: we expect a specific tag.\n      V<Word64> tag_bits = __ Word64BitwiseAnd(\n          pointer, __ Word64Constant(kExternalPointerTagMask));\n      tag_bits = __ Word64ShiftRightLogical(tag_bits, kExternalPointerTagShift);\n      V<Word32> tag = __ TruncateWord64ToWord32(tag_bits);\n      V<Word32> expected_tag = __ Word32Constant(tag_range.first);\n      __ GotoIf(__ Word32Equal(tag, expected_tag), done, BranchHint::kTrue);\n      // TODO(saelo): it would be nicer to abort here with\n      // AbortReason::kExternalPointerTagMismatch. That might require adding a\n      // builtin call here though, which is not currently available.\n      __ Unreachable();\n    } else {\n      // Not currently supported. Implement once needed.\n      DCHECK_NE(tag_range, kAnyExternalPointerTagRange);\n      UNREACHABLE();\n    }\n    __ BindReachable(done);\n    return __ Word64BitwiseAnd(pointer, kExternalPointerPayloadMask);\n#else   // V8_ENABLE_SANDBOX\n    UNREACHABLE();\n#endif  // V8_ENABLE_SANDBOX\n  }\n\n private:\n  std::optional<MemoryAnalyzer> analyzer_;\n  Isolate* isolate_ = __ data() -> isolate();\n  const TSCallDescriptor* allocate_builtin_descriptor_ = nullptr;\n  std::optional<Variable> top_[2];\n\n  static_assert(static_cast<int>(AllocationType::kYoung) == 0);\n  static_assert(static_cast<int>(AllocationType::kOld) == 1);\n  Variable top(AllocationType type) {\n    DCHECK(type == AllocationType::kYoung || type == AllocationType::kOld);\n    if (V8_UNLIKELY(!top_[static_cast<int>(type)].has_value())) {\n      top_[static_cast<int>(type)].emplace(\n          __ NewLoopInvariantVariable(RegisterRepresentation::WordPtr()));\n    }\n    return top_[static_cast<int>(type)].value();\n  }\n\n  const TSCallDescriptor* AllocateBuiltinDescriptor() {\n    if (allocate_builtin_descriptor_ == nullptr) {\n      allocate_builtin_descriptor_ =\n          CreateAllocateBuiltinDescriptor(__ graph_zone(), isolate_);\n    }\n    return allocate_builtin_descriptor_;\n  }\n\n  V<WordPtr> GetLimitAddress(AllocationType type) {\n    V<WordPtr> limit_address;\n    if (isolate_ != nullptr) {\n      limit_address = __ ExternalConstant(\n          type == AllocationType::kYoung\n              ? ExternalReference::new_space_allocation_limit_address(isolate_)\n              : ExternalReference::old_space_allocation_limit_address(\n                    isolate_));\n    } else {\n      // Wasm mode: producing isolate-independent code, loading the isolate\n      // address at runtime.\n#if V8_ENABLE_WEBASSEMBLY\n      V<WasmTrustedInstanceData> instance_node = __ WasmInstanceDataParameter();\n      int limit_address_offset =\n          type == AllocationType::kYoung\n              ? WasmTrustedInstanceData::kNewAllocationLimitAddressOffset\n              : WasmTrustedInstanceData::kOldAllocationLimitAddressOffset;\n      limit_address =\n          __ Load(instance_node, LoadOp::Kind::TaggedBase(),\n                  MemoryRepresentation::UintPtr(), limit_address_offset);\n#else\n      UNREACHABLE();\n#endif  // V8_ENABLE_WEBASSEMBLY\n    }\n    return limit_address;\n  }\n};\n        ]]></code>\n    </class>\n</file>\n```"
}