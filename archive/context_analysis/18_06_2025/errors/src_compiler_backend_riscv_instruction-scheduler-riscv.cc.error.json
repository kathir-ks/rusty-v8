{
  "file_path": "/home/kathirks_gc/v8_go/archive/codebase/src/compiler/backend/riscv/instruction-scheduler-riscv.cc",
  "error": "Response not JSON and not XML-like after cleanup",
  "json_error_if_any": "Skipped JSON parsing for whole response due to presence of XML tags; XML is primary.",
  "raw_response": "```xml\n<file>\n    <metadata>\n        {\n            \"path\": \"/home/kathirks_gc/v8_go/archive/codebase/src/compiler/backend/riscv/instruction-scheduler-riscv.cc\",\n            \"file_name\": \"instruction-scheduler-riscv.cc\",\n            \"language\": \"cpp\",\n            \"purpose\": \"Provides RISC-V specific instruction scheduling information for the V8 compiler backend.\"\n        }\n    </metadata>\n    <imports>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"purpose\": \"Includes necessary headers for code generation and instruction scheduling.\"\n            }\n        </metadata>\n        <code><![CDATA[\n#include \"src/codegen/macro-assembler.h\"\n#include \"src/compiler/backend/instruction-scheduler.h\"\n        ]]></code>\n    </imports>\n    <func>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"type\": \"method\",\n                \"name\": \"SchedulerSupported\",\n                \"parent\": \"InstructionScheduler\",\n                \"about\": \"Determines if instruction scheduling is supported for the RISC-V architecture.\",\n                \"logic\": \"Always returns true, indicating that scheduling is supported.\",\n                \"parameters\": [],\n                \"return\": {\n                    \"type\": \"bool\",\n                    \"description\": \"True, as scheduling is supported.\"\n                },\n                \"dependencies\": []\n            }\n        </metadata>\n        <code><![CDATA[\nbool InstructionScheduler::SchedulerSupported() { return true; }\n        ]]></code>\n    </func>\n    <func>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"type\": \"method\",\n                \"name\": \"GetTargetInstructionFlags\",\n                \"parent\": \"InstructionScheduler\",\n                \"about\": \"Retrieves instruction flags specific to the RISC-V architecture.\",\n                \"logic\": \"Uses a switch statement based on the instruction's `arch_opcode()` to determine flags. Many RISC-V opcodes return `kNoOpcodeFlags`. Some load instructions return `kIsLoadOperation` and some store operations return `kHasSideEffect`.\",\n                \"parameters\": [\n                    {\n                        \"name\": \"instr\",\n                        \"type\": \"const Instruction*\",\n                        \"purpose\": \"The instruction to get flags for.\"\n                    }\n                ],\n                \"return\": {\n                    \"type\": \"int\",\n                    \"description\": \"Instruction flags.\"\n                },\n                \"dependencies\": [\n                    \"Instruction\",\n                    \"kNoOpcodeFlags\",\n                    \"kIsLoadOperation\",\n                    \"kHasSideEffect\"\n                ]\n            }\n        </metadata>\n        <code><![CDATA[\nint InstructionScheduler::GetTargetInstructionFlags(\n    const Instruction* instr) const {\n  switch (instr->arch_opcode()) {\n    case kRiscvEnableDebugTrace:\n    case kRiscvDisableDebugTrace:\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvAdd32:\n    case kRiscvBitcastDL:\n    case kRiscvBitcastLD:\n    case kRiscvByteSwap64:\n    case kRiscvCvtDL:\n    case kRiscvCvtDUl:\n    case kRiscvCvtSL:\n    case kRiscvCvtSUl:\n    case kRiscvMulHigh64:\n    case kRiscvMulHighU64:\n    case kRiscvAdd64:\n    case kRiscvAddOvf64:\n    case kRiscvClz64:\n    case kRiscvCtz64:\n    case kRiscvDiv64:\n    case kRiscvDivU64:\n    case kRiscvZeroExtendWord:\n    case kRiscvSignExtendWord:\n    case kRiscvMod64:\n    case kRiscvModU64:\n    case kRiscvMul64:\n    case kRiscvMulOvf64:\n    case kRiscvPopcnt64:\n    case kRiscvRor64:\n    case kRiscvSar64:\n    case kRiscvShl64:\n    case kRiscvShr64:\n    case kRiscvSub64:\n    case kRiscvSubOvf64:\n    case kRiscvFloat64RoundDown:\n    case kRiscvFloat64RoundTiesEven:\n    case kRiscvFloat64RoundTruncate:\n    case kRiscvFloat64RoundUp:\n    case kRiscvSub32:\n    case kRiscvTruncLD:\n    case kRiscvTruncLS:\n    case kRiscvTruncUlD:\n    case kRiscvTruncUlS:\n    case kRiscvCmp32:\n    case kRiscvCmpZero32:\n#elif V8_TARGET_ARCH_RISCV32\n    case kRiscvAdd32:\n    case kRiscvAddPair:\n    case kRiscvSubPair:\n    case kRiscvMulPair:\n    case kRiscvAndPair:\n    case kRiscvOrPair:\n    case kRiscvXorPair:\n    case kRiscvShlPair:\n    case kRiscvShrPair:\n    case kRiscvSarPair:\n    case kRiscvAddOvf:\n    case kRiscvSubOvf:\n    case kRiscvSub32:\n#endif\n    case kRiscvSh1add:\n    case kRiscvSh2add:\n    case kRiscvSh3add:\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvAdduw:\n    case kRiscvSh1adduw:\n    case kRiscvSh2adduw:\n    case kRiscvSh3adduw:\n    case kRiscvSlliuw:\n#endif\n    case kRiscvAndn:\n    case kRiscvOrn:\n    case kRiscvXnor:\n    case kRiscvClz:\n    case kRiscvCtz:\n    case kRiscvCpop:\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvClzw:\n    case kRiscvCtzw:\n    case kRiscvCpopw:\n#endif\n    case kRiscvMax:\n    case kRiscvMaxu:\n    case kRiscvMin:\n    case kRiscvMinu:\n    case kRiscvSextb:\n    case kRiscvSexth:\n    case kRiscvZexth:\n    case kRiscvRev8:\n    case kRiscvBclr:\n    case kRiscvBclri:\n    case kRiscvBext:\n    case kRiscvBexti:\n    case kRiscvBinv:\n    case kRiscvBinvi:\n    case kRiscvBset:\n    case kRiscvBseti:\n    case kRiscvAbsD:\n    case kRiscvAbsS:\n    case kRiscvAddD:\n    case kRiscvAddS:\n    case kRiscvAnd:\n    case kRiscvAnd32:\n    case kRiscvAssertEqual:\n    case kRiscvBitcastInt32ToFloat32:\n    case kRiscvBitcastFloat32ToInt32:\n    case kRiscvByteSwap32:\n    case kRiscvCeilWD:\n    case kRiscvCeilWS:\n    case kRiscvClz32:\n    case kRiscvCmp:\n    case kRiscvCmpZero:\n    case kRiscvCmpD:\n    case kRiscvCmpS:\n    case kRiscvCtz32:\n    case kRiscvCvtDS:\n    case kRiscvCvtDUw:\n    case kRiscvCvtDW:\n    case kRiscvCvtSD:\n    case kRiscvCvtSUw:\n    case kRiscvCvtSW:\n    case kRiscvMulHighU32:\n    case kRiscvDiv32:\n    case kRiscvDivD:\n    case kRiscvDivS:\n    case kRiscvDivU32:\n    case kRiscvF64x2Abs:\n    case kRiscvF64x2Sqrt:\n    case kRiscvF64x2Pmin:\n    case kRiscvF64x2Pmax:\n    case kRiscvF64x2ConvertLowI32x4S:\n    case kRiscvF64x2ConvertLowI32x4U:\n    case kRiscvF64x2PromoteLowF32x4:\n    case kRiscvF64x2Ceil:\n    case kRiscvF64x2Floor:\n    case kRiscvF64x2Trunc:\n    case kRiscvF64x2NearestInt:\n    case kRiscvI64x2SplatI32Pair:\n    case kRiscvI64x2ExtractLane:\n    case kRiscvI64x2ReplaceLane:\n    case kRiscvI64x2ReplaceLaneI32Pair:\n    case kRiscvI64x2Shl:\n    case kRiscvI64x2ShrS:\n    case kRiscvI64x2ShrU:\n    case kRiscvF32x4Abs:\n    case kRiscvF32x4ExtractLane:\n    case kRiscvF32x4Sqrt:\n    case kRiscvF64x2Qfma:\n    case kRiscvF64x2Qfms:\n    case kRiscvF32x4Qfma:\n    case kRiscvF32x4Qfms:\n    case kRiscvF32x4ReplaceLane:\n    case kRiscvF32x4SConvertI32x4:\n    case kRiscvF32x4UConvertI32x4:\n    case kRiscvF32x4Pmin:\n    case kRiscvF32x4Pmax:\n    case kRiscvF32x4DemoteF64x2Zero:\n    case kRiscvF32x4Ceil:\n    case kRiscvF32x4Floor:\n    case kRiscvF32x4Trunc:\n    case kRiscvF32x4NearestInt:\n    case kRiscvF64x2ExtractLane:\n    case kRiscvF64x2ReplaceLane:\n    case kRiscvFloat32Max:\n    case kRiscvFloat32Min:\n    case kRiscvFloat32RoundDown:\n    case kRiscvFloat32RoundTiesEven:\n    case kRiscvFloat32RoundTruncate:\n    case kRiscvFloat32RoundUp:\n    case kRiscvFloat64ExtractLowWord32:\n    case kRiscvFloat64ExtractHighWord32:\n    case kRiscvFloat64InsertLowWord32:\n    case kRiscvFloat64InsertHighWord32:\n    case kRiscvFloat64Max:\n    case kRiscvFloat64Min:\n    case kRiscvFloat64SilenceNaN:\n    case kRiscvFloorWD:\n    case kRiscvFloorWS:\n    case kRiscvI64x2SConvertI32x4Low:\n    case kRiscvI64x2SConvertI32x4High:\n    case kRiscvI64x2UConvertI32x4Low:\n    case kRiscvI64x2UConvertI32x4High:\n    case kRiscvI16x8ExtractLaneU:\n    case kRiscvI16x8ExtractLaneS:\n    case kRiscvI16x8ReplaceLane:\n    case kRiscvI16x8Shl:\n    case kRiscvI16x8ShrS:\n    case kRiscvI16x8ShrU:\n    case kRiscvI32x4TruncSatF64x2SZero:\n    case kRiscvI32x4TruncSatF64x2UZero:\n    case kRiscvI32x4ExtractLane:\n    case kRiscvI32x4ReplaceLane:\n    case kRiscvI32x4SConvertF32x4:\n    case kRiscvI32x4Shl:\n    case kRiscvI32x4ShrS:\n    case kRiscvI32x4ShrU:\n    case kRiscvI32x4UConvertF32x4:\n    case kRiscvI8x16ExtractLaneU:\n    case kRiscvI8x16ExtractLaneS:\n    case kRiscvI8x16ReplaceLane:\n    case kRiscvI8x16Shl:\n    case kRiscvI8x16ShrS:\n    case kRiscvI8x16ShrU:\n    case kRiscvI8x16RoundingAverageU:\n    case kRiscvI8x16Popcnt:\n    case kRiscvMaxD:\n    case kRiscvMaxS:\n    case kRiscvMinD:\n    case kRiscvMinS:\n    case kRiscvMod32:\n    case kRiscvModU32:\n    case kRiscvMov:\n    case kRiscvMul32:\n    case kRiscvMulD:\n    case kRiscvMulHigh32:\n    case kRiscvMulOvf32:\n    case kRiscvMulS:\n    case kRiscvNegD:\n    case kRiscvNegS:\n    case kRiscvOr:\n    case kRiscvOr32:\n    case kRiscvPopcnt32:\n    case kRiscvRor32:\n    case kRiscvRoundWD:\n    case kRiscvRoundWS:\n    case kRiscvVnot:\n    case kRiscvS128Select:\n    case kRiscvS128Const:\n    case kRiscvS128Zero:\n    case kRiscvS128Load32Zero:\n    case kRiscvS128Load64Zero:\n    case kRiscvS128AllOnes:\n    case kRiscvV128AnyTrue:\n    case kRiscvI8x16Shuffle:\n    case kRiscvVwmul:\n    case kRiscvVwmulu:\n    case kRiscvVmv:\n    case kRiscvVandVv:\n    case kRiscvVorVv:\n    case kRiscvVnotVv:\n    case kRiscvVxorVv:\n    case kRiscvVmvSx:\n    case kRiscvVmvXs:\n    case kRiscvVfmvVf:\n    case kRiscvVcompress:\n    case kRiscvVaddVv:\n    case kRiscvVwaddVv:\n    case kRiscvVwadduVv:\n    case kRiscvVwadduWx:\n    case kRiscvVsubVv:\n    case kRiscvVnegVv:\n    case kRiscvVfnegVv:\n    case kRiscvVmaxuVv:\n    case kRiscvVmax:\n    case kRiscvVminsVv:\n    case kRiscvVminuVv:\n    case kRiscvVmulVv:\n    case kRiscvVdivu:\n    case kRiscvVsmulVv:\n    case kRiscvVmslt:\n    case kRiscvVgtsVv:\n    case kRiscvVgesVv:\n    case kRiscvVgeuVv:\n    case kRiscvVgtuVv:\n    case kRiscvVeqVv:\n    case kRiscvVneVv:\n    case kRiscvVAbs:\n    case kRiscvVaddSatUVv:\n    case kRiscvVaddSatSVv:\n    case kRiscvVsubSatUVv:\n    case kRiscvVsubSatSVv:\n    case kRiscvVrgather:\n    case kRiscvVslidedown:\n    case kRiscvVredminuVs:\n    case kRiscvVAllTrue:\n    case kRiscvVnclipu:\n    case kRiscvVnclip:\n    case kRiscvVsll:\n    case kRiscvVfaddVv:\n    case kRiscvVfsubVv:\n    case kRiscvVfmulVv:\n    case kRiscvVfdivVv:\n    case kRiscvVfminVv:\n    case kRiscvVfmaxVv:\n    case kRiscvVmfeqVv:\n    case kRiscvVmfneVv:\n    case kRiscvVmfltVv:\n    case kRiscvVmfleVv:\n    case kRiscvVmergeVx:\n    case kRiscvVzextVf2:\n    case kRiscvVsextVf2:\n    case kRiscvSar32:\n    case kRiscvSignExtendByte:\n    case kRiscvSignExtendShort:\n    case kRiscvShl32:\n    case kRiscvShr32:\n    case kRiscvSqrtD:\n    case kRiscvSqrtS:\n    case kRiscvSubD:\n    case kRiscvSubS:\n    case kRiscvTruncUwD:\n    case kRiscvTruncUwS:\n    case kRiscvTruncWD:\n    case kRiscvTruncWS:\n    case kRiscvTst32:\n    case kRiscvXor:\n    case kRiscvXor32:\n      return kNoOpcodeFlags;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvTst64:\n    case kRiscvLd:\n    case kRiscvLwu:\n    case kRiscvUlwu:\n    case kRiscvWord64AtomicLoadUint64:\n    case kRiscvLoadDecompressTaggedSigned:\n    case kRiscvLoadDecompressTagged:\n    case kRiscvLoadDecodeSandboxedPointer:\n    case kRiscvAtomicLoadDecompressTaggedSigned:\n    case kRiscvAtomicLoadDecompressTagged:\n    case kRiscvAtomicStoreCompressTagged:\n    case kRiscvLoadDecompressProtected:\n#elif V8_TARGET_ARCH_RISCV32\n    case kRiscvWord32AtomicPairLoad:\n#endif\n    case kRiscvLb:\n    case kRiscvLbu:\n    case kRiscvLoadDouble:\n    case kRiscvLh:\n    case kRiscvLhu:\n    case kRiscvLw:\n    case kRiscvLoadFloat:\n    case kRiscvRvvLd:\n    case kRiscvPeek:\n    case kRiscvUld:\n    case kRiscvULoadDouble:\n    case kRiscvUlh:\n    case kRiscvUlhu:\n    case kRiscvUlw:\n    case kRiscvULoadFloat:\n    case kRiscvS128LoadSplat:\n    case kRiscvS128Load64ExtendU:\n    case kRiscvS128Load64ExtendS:\n    case kRiscvS128LoadLane:\n      return kIsLoadOperation;\n\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvSd:\n    case kRiscvUsd:\n    case kRiscvWord64AtomicStoreWord64:\n    case kRiscvWord64AtomicAddUint64:\n    case kRiscvWord64AtomicSubUint64:\n    case kRiscvWord64AtomicAndUint64:\n    case kRiscvWord64AtomicOrUint64:\n    case kRiscvWord64AtomicXorUint64:\n    case kRiscvWord64AtomicExchangeUint64:\n    case kRiscvWord64AtomicCompareExchangeUint64:\n    case kRiscvStoreCompressTagged:\n    case kRiscvStoreEncodeSandboxedPointer:\n    case kRiscvStoreIndirectPointer:\n#elif V8_TARGET_ARCH_RISCV32\n    case kRiscvWord32AtomicPairStore:\n    case kRiscvWord32AtomicPairAdd:\n    case kRiscvWord32AtomicPairSub:\n    case kRiscvWord32AtomicPairAnd:\n    case kRiscvWord32AtomicPairOr:\n    case kRiscvWord32AtomicPairXor:\n    case kRiscvWord32AtomicPairExchange:\n    case kRiscvWord32AtomicPairCompareExchange:\n#endif\n    case kRiscvModD:\n    case kRiscvModS:\n    case kRiscvRvvSt:\n    case kRiscvPush:\n    case kRiscvSb:\n    case kRiscvStoreDouble:\n    case kRiscvSh:\n    case kRiscvStackClaim:\n    case kRiscvStoreToStackSlot:\n    case kRiscvSw:\n    case kRiscvStoreFloat:\n    case kRiscvUStoreDouble:\n    case kRiscvUsh:\n    case kRiscvUsw:\n    case kRiscvUStoreFloat:\n    case kRiscvSync:\n    case kRiscvS128StoreLane:\n      return kHasSideEffect;\n\n#define CASE(Name) case k##Name:\n      COMMON_ARCH_OPCODE_LIST(CASE)\n#undef CASE\n      // Already covered in architecture independent code.\n      UNREACHABLE();\n  }\n\n  UNREACHABLE();\n}\n        ]]></code>\n    </func>\n    <func>\n        <metadata>\n            {\n                \"language\": \"cpp\",\n                \"type\": \"method\",\n                \"name\": \"GetInstructionLatency\",\n                \"parent\": \"InstructionScheduler\",\n                \"about\": \"Retrieves the execution latency of a given instruction for RISC-V.\",\n                \"logic\": \"Uses a large switch statement to determine instruction latency based on instruction opcode. Many instruction latencies are estimations. Handles architecture-specific instructions using `#ifdef V8_TARGET_ARCH_RISCV64`.\",\n                \"parameters\": [\n                    {\n                        \"name\": \"instr\",\n                        \"type\": \"const Instruction*\",\n                        \"purpose\": \"The instruction to retrieve latency for.\"\n                    }\n                ],\n                \"return\": {\n                    \"type\": \"int\",\n                    \"description\": \"The execution latency of the instruction.\"\n                },\n                \"dependencies\": [\n                    \"Instruction\",\n                    \"CallLatency\",\n                    \"JumpLatency\",\n                    \"PrepareCallCFunctionLatency\",\n                    \"PushCallerSavedLatency\",\n                    \"PopCallerSavedLatency\",\n                    \"CallCFunctionLatency\",\n                    \"AssembleArchJumpLatency\",\n                    \"AssembleArchTableSwitchLatency\",\n                    \"AssemblerReturnLatency\",\n                    \"TruncateDoubleToIDelayedLatency\",\n                    \"CheckPageFlagLatency\",\n                    \"Add64Latency\",\n                    \"Mul32Latency\",\n                    \"Div32Latency\",\n                    \"AndLatency\",\n                    \"OrLatency\",\n                    \"XorLatency\",\n                    \"Clz64Latency\",\n                    \"Ctz32Latency\",\n                    \"Popcnt32Latency\",\n                    \"MoveLatency\",\n                    \"CompareF32Latency\",\n                    \"CompareF64Latency\",\n                    \"NegdLatency\",\n                    \"Float64RoundLatency\",\n                    \"Float32RoundLatency\",\n                    \"Float32MaxLatency\",\n                    \"Float64MaxLatency\",\n                    \"Float32MinLatency\",\n                    \"Float64MinLatency\",\n                    \"TruncLSLatency\",\n                    \"TruncLDLatency\",\n                    \"TruncUlSLatency\",\n                    \"TruncUlDLatency\",\n                    \"ByteSwapSignedLatency\",\n                    \"AlignedMemoryLatency\",\n                    \"ULoadFloatLatency\",\n                    \"LoadDoubleLatency\",\n                    \"StoreFloatLatency\",\n                    \"StoreDoubleLatency\",\n                    \"UlhuLatency\",\n                    \"UlwuLatency\",\n                    \"UldLatency\",\n                    \"UsdLatency\",\n                    \"UlwLatency\",\n                    \"UStoreFloatLatency\",\n                    \"UStoreDoubleLatency\",\n                    \"PushLatency\",\n                    \"CallCFunctionHelperLatency\"\n                ]\n            }\n        </metadata>\n        <code><![CDATA[\nint InstructionScheduler::GetInstructionLatency(const Instruction* instr) {\n  // TODO(RISCV): Verify these latencies for RISC-V (currently using MIPS\n  // numbers).\n  switch (instr->arch_opcode()) {\n    case kArchCallCodeObject:\n    case kArchCallWasmFunction:\n      return CallLatency();\n    case kArchTailCallCodeObject:\n    case kArchTailCallWasm:\n    case kArchTailCallAddress:\n      return JumpLatency();\n    case kArchCallJSFunction: {\n      int latency = 0;\n      if (v8_flags.debug_code) {\n        latency = 1 + AssertLatency();\n      }\n      return latency + 1 + Add64Latency(false) + CallLatency();\n    }\n    case kArchPrepareCallCFunction:\n      return PrepareCallCFunctionLatency();\n    case kArchSaveCallerRegisters: {\n      auto fp_mode =\n          static_cast<SaveFPRegsMode>(MiscField::decode(instr->opcode()));\n      return PushCallerSavedLatency(fp_mode);\n    }\n    case kArchRestoreCallerRegisters: {\n      auto fp_mode =\n          static_cast<SaveFPRegsMode>(MiscField::decode(instr->opcode()));\n      return PopCallerSavedLatency(fp_mode);\n    }\n    case kArchPrepareTailCall:\n      return 2;\n    case kArchCallCFunction:\n      return CallCFunctionLatency();\n    case kArchJmp:\n      return AssembleArchJumpLatency();\n    case kArchTableSwitch:\n      return AssembleArchTableSwitchLatency();\n    case kArchAbortCSADcheck:\n      return CallLatency() + 1;\n    case kArchDebugBreak:\n      return 1;\n    case kArchComment:\n    case kArchNop:\n    case kArchThrowTerminator:\n    case kArchDeoptimize:\n      return 0;\n    case kArchRet:\n      return AssemblerReturnLatency();\n    case kArchFramePointer:\n      return 1;\n    case kArchParentFramePointer:\n      // Estimated max.\n      return AlignedMemoryLatency();\n    case kArchTruncateDoubleToI:\n      return TruncateDoubleToIDelayedLatency();\n    case kArchStoreWithWriteBarrier:\n      return Add64Latency() + 1 + CheckPageFlagLatency();\n    case kArchStackSlot:\n      // Estimated max.\n      return Add64Latency(false) + AndLatency(false) + AssertLatency() +\n             Add64Latency(false) + AndLatency(false) + BranchShortLatency() +\n             1 + Sub64Latency() + Add64Latency();\n    case kIeee754Float64Acos:\n    case kIeee754Float64Acosh:\n    case kIeee754Float64Asin:\n    case kIeee754Float64Asinh:\n    case kIeee754Float64Atan:\n    case kIeee754Float64Atanh:\n    case kIeee754Float64Atan2:\n    case kIeee754Float64Cos:\n    case kIeee754Float64Cosh:\n    case kIeee754Float64Cbrt:\n    case kIeee754Float64Exp:\n    case kIeee754Float64Expm1:\n    case kIeee754Float64Log:\n    case kIeee754Float64Log1p:\n    case kIeee754Float64Log10:\n    case kIeee754Float64Log2:\n    case kIeee754Float64Pow:\n    case kIeee754Float64Sin:\n    case kIeee754Float64Sinh:\n    case kIeee754Float64Tan:\n    case kIeee754Float64Tanh:\n      return PrepareCallCFunctionLatency() + MovToFloatParametersLatency() +\n             CallCFunctionLatency() + MovFromFloatResultLatency();\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvAdd32:\n    case kRiscvAdd64:\n      return Add64Latency(instr->InputAt(1)->IsRegister());\n    case kRiscvAddOvf64:\n      return AddOverflow64Latency();\n    case kRiscvSub32:\n    case kRiscvSub64:\n      return Sub64Latency(instr->InputAt(1)->IsRegister());\n    case kRiscvSubOvf64:\n      return SubOverflow64Latency();\n    case kRiscvMulHigh64:\n      return Mulh64Latency();\n    case kRiscvMul64:\n      return Mul64Latency();\n    case kRiscvMulOvf64:\n      return MulOverflow64Latency();\n    case kRiscvDiv64: {\n      int latency = Div64Latency();\n      return latency + MovzLatency();\n    }\n    case kRiscvDivU64: {\n      int latency = Divu64Latency();\n      return latency + MovzLatency();\n    }\n    case kRiscvMod64:\n      return Mod64Latency();\n    case kRiscvModU64:\n      return Modu64Latency();\n#elif V8_TARGET_ARCH_RISCV32\n    case kRiscvAdd32:\n      return Add64Latency(instr->InputAt(1)->IsRegister());\n    case kRiscvAddOvf:\n      return AddOverflow64Latency();\n    case kRiscvSub32:\n      return Sub64Latency(instr->InputAt(1)->IsRegister());\n    case kRiscvSubOvf:\n      return SubOverflow64Latency();\n#endif\n    case kRiscvMul32:\n      return Mul32Latency();\n    case kRiscvMulOvf32:\n      return MulOverflow32Latency();\n    case kRiscvMulHigh32:\n      return Mulh32Latency();\n    case kRiscvMulHighU32:\n      return Mulhu32Latency();\n    case kRiscvDiv32: {\n      int latency = Div32Latency(instr->InputAt(1)->IsRegister());\n      return latency + MovzLatency();\n    }\n    case kRiscvDivU32: {\n      int latency = Divu32Latency(instr->InputAt(1)->IsRegister());\n      return latency + MovzLatency();\n    }\n    case kRiscvMod32:\n      return Mod32Latency();\n    case kRiscvModU32:\n      return Modu32Latency();\n    case kRiscvAnd:\n      return AndLatency(instr->InputAt(1)->IsRegister());\n    case kRiscvAnd32: {\n      bool is_operand_register = instr->InputAt(1)->IsRegister();\n      int latency = AndLatency(is_operand_register);\n      if (is_operand_register) {\n        return latency + 2;\n      } else {\n        return latency + 1;\n      }\n    }\n    case kRiscvOr:\n      return OrLatency(instr->InputAt(1)->IsRegister());\n    case kRiscvOr32: {\n      bool is_operand_register = instr->InputAt(1)->IsRegister();\n      int latency = OrLatency(is_operand_register);\n      if (is_operand_register) {\n        return latency + 2;\n      } else {\n        return latency + 1;\n      }\n    }\n    case kRiscvXor:\n      return XorLatency(instr->InputAt(1)->IsRegister());\n    case kRiscvXor32: {\n      bool is_operand_register = instr->InputAt(1)->IsRegister();\n      int latency = XorLatency(is_operand_register);\n      if (is_operand_register) {\n        return latency + 2;\n      } else {\n        return latency + 1;\n      }\n    }\n    case kRiscvClz32:\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvClz64:\n#endif\n      return Clz64Latency();\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvCtz64:\n      return Ctz64Latency();\n    case kRiscvPopcnt64:\n      return Popcnt64Latency();\n#endif\n    case kRiscvCtz32:\n      return Ctz32Latency();\n    case kRiscvPopcnt32:\n      return Popcnt32Latency();\n    case kRiscvShl32:\n      return 1;\n    case kRiscvShr32:\n    "
}