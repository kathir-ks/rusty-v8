{
  "metadata": {
    "path": "/home/kathirks_gc/v8_go/archive/codebase/src/compiler/backend/code-generator.h",
    "file_name": "code-generator.h",
    "language": "cpp",
    "purpose": "Declares the CodeGenerator class, which generates native code for a sequence of instructions in the V8 compiler backend."
  },
  "imports": {
    "metadata": {
      "language": "cpp",
      "purpose": "Includes the WebAssembly trap handler if WebAssembly is enabled."
    },
    "code": "#if V8_ENABLE_WEBASSEMBLY\n#include \"src/trap-handler/trap-handler.h\"\n#endif  // V8_ENABLE_WEBASSEMBLY"
  },
  "classes": [
    {
      "metadata": {
        "language": "cpp",
        "type": "class",
        "name": "CodeGenerator",
        "extends": "GapResolver::Assembler",
        "about": "Generates native code for a sequence of instructions.",
        "attributes": [],
        "dependencies": [
          "GapResolver",
          "Assembler",
          "Zone",
          "Frame",
          "Linkage",
          "InstructionSequence",
          "OptimizedCompilationInfo",
          "Isolate",
          "OsrHelper",
          "JumpOptimizationInfo",
          "AssemblerOptions",
          "Builtin",
          "Code",
          "SafepointTableBuilder",
          "Instruction",
          "FrameAccessState",
          "DeoptimizationExit",
          "base::OwnedVector",
          "SourcePosition",
          "ReferenceMap",
          "ZoneVector",
          "TurbolizerInstructionStartInfo",
          "TurbolizerCodeOffsetsInfo",
          "Handle",
          "HeapObject",
          "RootIndex",
          "InstructionBlock",
          "RpoNumber",
          "FlagsCondition",
          "DeoptimizationData",
          "IndirectHandle",
          "TrustedObject",
          "DeoptimizationLiteral",
          "DeoptimizationEntry",
          "FrameStateDescriptor",
          "InstructionOperandIterator",
          "OutputFrameStateCombine",
          "StateValueDescriptor",
          "StateValueList",
          "MachineType",
          "ZoneDeque",
          "MoveOperands",
          "MoveType",
          "OutOfLineCode",
          "JumpTable",
          "base::Vector",
          "SourcePositionTableBuilder",
          "trap_handler::ProtectedInstructionData",
          "MoveCycleState"
        ]
      },
      "code": "class V8_EXPORT_PRIVATE CodeGenerator final : public GapResolver::Assembler {\n public:\n  explicit CodeGenerator(Zone* codegen_zone, Frame* frame, Linkage* linkage,\n                         InstructionSequence* instructions,\n                         OptimizedCompilationInfo* info, Isolate* isolate,\n                         std::optional<OsrHelper> osr_helper,\n                         int start_source_position,\n                         JumpOptimizationInfo* jump_opt,\n                         const AssemblerOptions& options, Builtin builtin,\n                         size_t max_unoptimized_frame_height,\n                         size_t max_pushed_argument_count,\n                         const char* debug_name = nullptr);\n\n  // Generate native code. After calling AssembleCode, call FinalizeCode to\n  // produce the actual code object. If an error occurs during either phase,\n  // FinalizeCode returns an empty MaybeHandle.\n  void AssembleCode();  // Does not need to run on main thread.\n  MaybeHandle<Code> FinalizeCode();\n\n#if V8_ENABLE_WEBASSEMBLY\n  base::OwnedVector<uint8_t> GenerateWasmDeoptimizationData();\n#endif\n\n  base::OwnedVector<uint8_t> GetSourcePositionTable();\n  base::OwnedVector<uint8_t> GetProtectedInstructionsData();\n\n  InstructionSequence* instructions() const { return instructions_; }\n  FrameAccessState* frame_access_state() const { return frame_access_state_; }\n  const Frame* frame() const { return frame_access_state_->frame(); }\n  Isolate* isolate() const { return isolate_; }\n  Linkage* linkage() const { return linkage_; }\n\n  Label* GetLabel(RpoNumber rpo) { return &labels_[rpo.ToSize()]; }\n\n  void RecordProtectedInstruction(uint32_t instr_offset);\n\n  SourcePosition start_source_position() const {\n    return start_source_position_;\n  }\n\n  void AssembleSourcePosition(Instruction* instr);\n  void AssembleSourcePosition(SourcePosition source_position);\n\n  // Record a safepoint with the given pointer map. When pc_offset is 0, then\n  // the current pc is used to define the safepoint. Otherwise the provided\n  // pc_offset is used.\n  void RecordSafepoint(ReferenceMap* references, int pc_offset = 0);\n\n  Zone* zone() const { return zone_; }\n  MacroAssembler* masm() { return &masm_; }\n  SafepointTableBuilder* safepoint_table_builder() { return &safepoints_; }\n  size_t handler_table_offset() const { return handler_table_offset_; }\n\n  const ZoneVector<int>& block_starts() const { return block_starts_; }\n  const ZoneVector<TurbolizerInstructionStartInfo>& instr_starts() const {\n    return instr_starts_;\n  }\n\n  const TurbolizerCodeOffsetsInfo& offsets_info() const {\n    return offsets_info_;\n  }\n\n#if V8_ENABLE_WEBASSEMBLY\n  bool IsWasm() const { return info()->IsWasm(); }\n#endif\n\n  static constexpr int kBinarySearchSwitchMinimalCases = 4;\n\n  // Returns true if an offset should be applied to the given stack check. There\n  // are two reasons that this could happen:\n  // 1. The optimized frame is smaller than the corresponding deoptimized frames\n  //    and an offset must be applied in order to be able to deopt safely.\n  // 2. The current function pushes a large number of arguments to the stack.\n  //    These are not accounted for by the initial frame setup.\n  bool ShouldApplyOffsetToStackCheck(Instruction* instr, uint32_t* offset);\n  uint32_t GetStackCheckOffset();\n\n  CodeKind code_kind() const { return info_->code_kind(); }\n\n private:\n  GapResolver* resolver() { return &resolver_; }\n  SafepointTableBuilder* safepoints() { return &safepoints_; }\n  OptimizedCompilationInfo* info() const { return info_; }\n  OsrHelper* osr_helper() { return &(*osr_helper_); }\n\n  // Create the FrameAccessState object. The Frame is immutable from here on.\n  void CreateFrameAccessState(Frame* frame);\n\n  // Architecture - specific frame finalization.\n  void FinishFrame(Frame* frame);\n\n  // Checks if {block} will appear directly after {current_block_} when\n  // assembling code, in which case, a fall-through can be used.\n  bool IsNextInAssemblyOrder(RpoNumber block) const;\n\n  // Check if a heap object can be materialized by loading from a heap root,\n  // which is cheaper on some platforms than materializing the actual heap\n  // object constant.\n  bool IsMaterializableFromRoot(Handle<HeapObject> object,\n                                RootIndex* index_return);\n\n  enum CodeGenResult { kSuccess, kTooManyDeoptimizationBailouts };\n\n  // Assemble instructions for the specified block.\n  CodeGenResult AssembleBlock(const InstructionBlock* block);\n\n  // Assemble code for the specified instruction.\n  CodeGenResult AssembleInstruction(int instruction_index,\n                                    const InstructionBlock* block);\n  void AssembleGaps(Instruction* instr);\n\n  // Compute branch info from given instruction. Returns a valid rpo number\n  // if the branch is redundant, the returned rpo number point to the target\n  // basic block.\n  RpoNumber ComputeBranchInfo(BranchInfo* branch, FlagsCondition condition,\n                              Instruction* instr);\n\n  // Returns true if a instruction is a tail call that needs to adjust the stack\n  // pointer before execution. The stack slot index to the empty slot above the\n  // adjusted stack pointer is returned in |slot|.\n  bool GetSlotAboveSPBeforeTailCall(Instruction* instr, int* slot);\n\n  // Determines how to call helper stubs depending on the code kind.\n  StubCallMode DetermineStubCallMode() const;\n\n  CodeGenResult AssembleDeoptimizerCall(DeoptimizationExit* exit);\n\n  DeoptimizationExit* BuildTranslation(Instruction* instr, int pc_offset,\n                                       size_t frame_state_offset,\n                                       size_t immediate_args_count,\n                                       OutputFrameStateCombine state_combine);\n\n  // ===========================================================================\n  // ============= Architecture-specific code generation methods. ==============\n  // ===========================================================================\n\n  CodeGenResult AssembleArchInstruction(Instruction* instr);\n  void AssembleArchJump(RpoNumber target);\n  void AssembleArchJumpRegardlessOfAssemblyOrder(RpoNumber target);\n  void AssembleArchBranch(Instruction* instr, BranchInfo* branch);\n  void AssembleArchConditionalBranch(Instruction* instr, BranchInfo* branch);\n\n  // Generates special branch for deoptimization condition.\n  void AssembleArchDeoptBranch(Instruction* instr, BranchInfo* branch);\n\n  void AssembleArchBoolean(Instruction* instr, FlagsCondition condition);\n  void AssembleArchConditionalBoolean(Instruction* instr);\n  void AssembleArchSelect(Instruction* instr, FlagsCondition condition);\n#if V8_ENABLE_WEBASSEMBLY\n  void AssembleArchTrap(Instruction* instr, FlagsCondition condition);\n#endif  // V8_ENABLE_WEBASSEMBLY\n#if V8_TARGET_ARCH_X64\n  void AssembleArchBinarySearchSwitchRange(\n      Register input, RpoNumber def_block, std::pair<int32_t, Label*>* begin,\n      std::pair<int32_t, Label*>* end, std::optional<int32_t>& last_cmp_value);\n#else\n  void AssembleArchBinarySearchSwitchRange(Register input, RpoNumber def_block,\n                                           std::pair<int32_t, Label*>* begin,\n                                           std::pair<int32_t, Label*>* end);\n#endif  // V8_TARGET_ARCH_X64\n  void AssembleArchBinarySearchSwitch(Instruction* instr);\n  void AssembleArchTableSwitch(Instruction* instr);\n\n  // Generates code to check whether the {kJavaScriptCallCodeStartRegister}\n  // contains the expected pointer to the start of the instruction stream.\n  void AssembleCodeStartRegisterCheck();\n\n#ifdef V8_ENABLE_LEAPTIERING\n  // Generates code to check whether the {kJavaScriptCallDispatchHandleRegister}\n  // references a valid entry compatible with this code.\n  void AssembleDispatchHandleRegisterCheck();\n#endif  // V8_ENABLE_LEAPTIERING\n\n  // When entering a code that is marked for deoptimization, rather continuing\n  // with its execution, we jump to a lazy compiled code. We need to do this\n  // because this code has already been deoptimized and needs to be unlinked\n  // from the JS functions referring it.\n  // TODO(olivf, 42204201) Rename this to AssertNotDeoptimized once\n  // non-leaptiering is removed from the codebase.\n  void BailoutIfDeoptimized();\n\n  // Assemble NOP instruction for lazy deoptimization. This place will be\n  // patched later as a jump instruction to deoptimization trampoline.\n  void AssemblePlaceHolderForLazyDeopt(Instruction* instr);\n\n  // Generates an architecture-specific, descriptor-specific prologue\n  // to set up a stack frame.\n  void AssembleConstructFrame();\n\n  // Generates an architecture-specific, descriptor-specific return sequence\n  // to tear down a stack frame.\n  void AssembleReturn(InstructionOperand* pop);\n\n  void AssembleDeconstructFrame();\n\n  // Generates code to manipulate the stack in preparation for a tail call.\n  void AssemblePrepareTailCall();\n\n  enum PushTypeFlag {\n    kImmediatePush = 0x1,\n    kRegisterPush = 0x2,\n    kStackSlotPush = 0x4,\n    kScalarPush = kRegisterPush | kStackSlotPush\n  };\n\n  using PushTypeFlags = base::Flags<PushTypeFlag>;\n\n  static bool IsValidPush(InstructionOperand source, PushTypeFlags push_type);\n\n  // Generate a list of moves from an instruction that are candidates to be\n  // turned into push instructions on platforms that support them. In general,\n  // the list of push candidates are moves to a set of contiguous destination\n  // InstructionOperand locations on the stack that don't clobber values that\n  // are needed to resolve the gap or use values generated by the gap,\n  // i.e. moves that can be hoisted together before the actual gap and assembled\n  // together.\n  static void GetPushCompatibleMoves(Instruction* instr,\n                                     PushTypeFlags push_type,\n                                     ZoneVector<MoveOperands*>* pushes);\n\n  class MoveType {\n   public:\n    enum Type {\n      kRegisterToRegister,\n      kRegisterToStack,\n      kStackToRegister,\n      kStackToStack,\n      kConstantToRegister,\n      kConstantToStack\n    };\n\n    // Detect what type of move or swap needs to be performed. Note that these\n    // functions do not take into account the representation (Tagged, FP,\n    // ...etc).\n\n    static Type InferMove(InstructionOperand* source,\n                          InstructionOperand* destination);\n    static Type InferSwap(InstructionOperand* source,\n                          InstructionOperand* destination);\n  };\n  // Called before a tail call |instr|'s gap moves are assembled and allows\n  // gap-specific pre-processing, e.g. adjustment of the sp for tail calls that\n  // need it before gap moves or conversion of certain gap moves into pushes.\n  void AssembleTailCallBeforeGap(Instruction* instr,\n                                 int first_unused_stack_slot);\n  // Called after a tail call |instr|'s gap moves are assembled and allows\n  // gap-specific post-processing, e.g. adjustment of the sp for tail calls that\n  // need it after gap moves.\n  void AssembleTailCallAfterGap(Instruction* instr,\n                                int first_unused_stack_slot);\n\n  void FinishCode();\n  void MaybeEmitOutOfLineConstantPool();\n\n  void IncrementStackAccessCounter(InstructionOperand* source,\n                                   InstructionOperand* destination);\n\n  // ===========================================================================\n  // ============== Architecture-specific gap resolver methods. ================\n  // ===========================================================================\n\n  // Interface used by the gap resolver to emit moves and swaps.\n  void AssembleMove(InstructionOperand* source,\n                    InstructionOperand* destination) final;\n  void AssembleSwap(InstructionOperand* source,\n                    InstructionOperand* destination) final;\n  AllocatedOperand Push(InstructionOperand* src) final;\n  void Pop(InstructionOperand* src, MachineRepresentation rep) final;\n  void PopTempStackSlots() final;\n  void MoveToTempLocation(InstructionOperand* src,\n                          MachineRepresentation rep) final;\n  void MoveTempLocationTo(InstructionOperand* dst,\n                          MachineRepresentation rep) final;\n  void SetPendingMove(MoveOperands* move) final;\n\n  // ===========================================================================\n  // =================== Jump table construction methods. ======================\n  // ===========================================================================\n\n  class JumpTable;\n  // Adds a jump table that is emitted after the actual code.  Returns label\n  // pointing to the beginning of the table.  {targets} is assumed to be static\n  // or zone allocated.\n  Label* AddJumpTable(base::Vector<Label*> targets);\n  // Emits a jump table.\n  void AssembleJumpTable(base::Vector<Label*> targets);\n\n  // ===========================================================================\n  // ================== Deoptimization table construction. =====================\n  // ===========================================================================\n\n  void RecordCallPosition(Instruction* instr);\n  void RecordDeoptInfo(Instruction* instr, int pc_offset);\n  Handle<DeoptimizationData> GenerateDeoptimizationData();\n  int DefineProtectedDeoptimizationLiteral(\n      IndirectHandle<TrustedObject> object);\n  int DefineDeoptimizationLiteral(DeoptimizationLiteral literal);\n  bool HasProtectedDeoptimizationLiteral(\n      IndirectHandle<TrustedObject> object) const;\n  DeoptimizationEntry const& GetDeoptimizationEntry(Instruction* instr,\n                                                    size_t frame_state_offset);\n\n  void BuildTranslationForFrameStateDescriptor(\n      FrameStateDescriptor* descriptor, InstructionOperandIterator* iter,\n      OutputFrameStateCombine state_combine);\n  void TranslateStateValueDescriptor(StateValueDescriptor* desc,\n                                     StateValueList* nested,\n                                     InstructionOperandIterator* iter);\n  void TranslateFrameStateDescriptorOperands(FrameStateDescriptor* desc,\n                                             InstructionOperandIterator* iter);\n  void AddTranslationForOperand(Instruction* instr, InstructionOperand* op,\n                                MachineType type);\n\n  void PrepareForDeoptimizationExits(ZoneDeque<DeoptimizationExit*>* exits);\n  DeoptimizationExit* AddDeoptimizationExit(Instruction* instr,\n                                            size_t frame_state_offset,\n                                            size_t immediate_args_count);\n\n  // ===========================================================================\n\n  struct HandlerInfo {\n    // {handler} is nullptr if the Call should lazy deopt on exceptions.\n    Label* handler;\n    int pc_offset;\n  };\n\n  friend class OutOfLineCode;\n  friend class CodeGeneratorTester;\n\n  Zone* zone_;\n  Isolate* isolate_;\n  FrameAccessState* frame_access_state_;\n  Linkage* const linkage_;\n  InstructionSequence* const instructions_;\n  UnwindingInfoWriter unwinding_info_writer_;\n  OptimizedCompilationInfo* const info_;\n  Label* const labels_;\n  Label return_label_;\n  RpoNumber current_block_;\n  SourcePosition start_source_position_;\n  SourcePosition current_source_position_;\n  MacroAssembler masm_;\n  GapResolver resolver_;\n  SafepointTableBuilder safepoints_;\n  ZoneVector<HandlerInfo> handlers_;\n  int next_deoptimization_id_ = 0;\n  int deopt_exit_start_offset_ = 0;\n  int eager_deopt_count_ = 0;\n  int lazy_deopt_count_ = 0;\n  ZoneDeque<DeoptimizationExit*> deoptimization_exits_;\n  ZoneDeque<IndirectHandle<TrustedObject>> protected_deoptimization_literals_;\n  ZoneDeque<DeoptimizationLiteral> deoptimization_literals_;\n  size_t inlined_function_count_ = 0;\n  FrameTranslationBuilder translations_;\n  int handler_table_offset_ = 0;\n\n  // Deoptimization exits must be as small as possible, since their count grows\n  // with function size. {jump_deoptimization_entry_labels_} is an optimization\n  // to that effect, which extracts the (potentially large) instruction\n  // sequence for the final jump to the deoptimization entry into a single spot\n  // per InstructionStream object. All deopt exits can then near-call to this\n  // label. Note: not used on all architectures.\n  Label jump_deoptimization_entry_labels_[kDeoptimizeKindCount];\n\n  // The maximal combined height of all frames produced upon deoptimization, and\n  // the maximal number of pushed arguments for function calls. Applied as an\n  // offset to the first stack check of an optimized function.\n  const size_t max_unoptimized_frame_height_;\n  const size_t max_pushed_argument_count_;\n\n  // The number of incoming parameters for code using JS linkage (i.e.\n  // JavaScript functions). Only computed during AssembleCode.\n  uint16_t parameter_count_ = 0;\n\n  // kArchCallCFunction could be reached either:\n  //   kArchCallCFunction;\n  // or:\n  //   kArchSaveCallerRegisters;\n  //   kArchCallCFunction;\n  //   kArchRestoreCallerRegisters;\n  // The boolean is used to distinguish the two cases. In the latter case, we\n  // also need to decide if FP registers need to be saved, which is controlled\n  // by fp_mode_.\n  bool caller_registers_saved_;\n  SaveFPRegsMode fp_mode_;\n\n  JumpTable* jump_tables_;\n  OutOfLineCode* ools_;\n  std::optional<OsrHelper> osr_helper_;\n  int osr_pc_offset_;\n  SourcePositionTableBuilder source_position_table_builder_;\n#if V8_ENABLE_WEBASSEMBLY\n  ZoneVector<trap_handler::ProtectedInstructionData> protected_instructions_;\n#endif  // V8_ENABLE_WEBASSEMBLY\n  CodeGenResult result_;\n  ZoneVector<int> block_starts_;\n  TurbolizerCodeOffsetsInfo offsets_info_;\n  ZoneVector<TurbolizerInstructionStartInfo> instr_starts_;\n  MoveCycleState move_cycle_;\n\n  const char* debug_name_ = nullptr;\n};"
    },
    {
      "metadata": {
        "language": "cpp",
        "type": "class",
        "name": "InstructionOperandIterator",
        "about": "Iterates through the operands of an instruction.",
        "attributes": [],
        "dependencies": [
          "Instruction",
          "InstructionOperand"
        ]
      },
      "code": "class InstructionOperandIterator {\n public:\n  InstructionOperandIterator(Instruction* instr, size_t pos)\n      : instr_(instr), pos_(pos) {}\n\n  Instruction* instruction() const { return instr_; }\n  InstructionOperand* Advance() { return instr_->InputAt(pos_++); }\n\n private:\n  Instruction* instr_;\n  size_t pos_;\n};"
    }
  ],
  "file_path": "/home/kathirks_gc/v8_go/archive/codebase/src/compiler/backend/code-generator.h"
}