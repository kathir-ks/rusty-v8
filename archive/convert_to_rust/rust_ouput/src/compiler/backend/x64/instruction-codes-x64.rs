// Converted from V8 C++ source files:
// Header: instruction-codes-x64.h
// Implementation: N/A
// 
// This file combines both header and implementation into idiomatic Rust code.

pub mod instruction_codes_x64 {
    // Define a macro for opcodes with memory access mode
    macro_rules! target_arch_opcode_with_memory_access_mode_list {
        ($V:ident) => {
            $V!(X64F64x2PromoteLowF32x4);
            $V!(X64Movb);
            $V!(X64Movdqu);
            $V!(X64Movl);
            $V!(X64Movq);
            $V!(X64Movsd);
            $V!(X64Movss);
            $V!(X64Movsh);
            $V!(X64Movsxbl);
            $V!(X64Movsxbq);
            $V!(X64Movsxlq);
            $V!(X64Movsxwl);
            $V!(X64Movsxwq);
            $V!(X64Movw);
            $V!(X64Movzxbl);
            $V!(X64Movzxbq);
            $V!(X64Movzxwl);
            $V!(X64Movzxwq);
            $V!(X64Pextrb);
            $V!(X64Pextrw);
            $V!(X64Pinsrb);
            $V!(X64Pinsrd);
            $V!(X64Pinsrq);
            $V!(X64Pinsrw);
            $V!(X64S128Load16Splat);
            $V!(X64S128Load16x4S);
            $V!(X64S128Load16x4U);
            $V!(X64S128Load32Splat);
            $V!(X64S128Load32x2S);
            $V!(X64S128Load32x2U);
            $V!(X64S128Load64Splat);
            $V!(X64S128Load8Splat);
            $V!(X64S128Load8x8S);
            $V!(X64S128Load8x8U);
            $V!(X64S128Store32Lane);
            $V!(X64S128Store64Lane);
            $V!(X64Word64AtomicStoreWord64);
            $V!(X64Word64AtomicAddUint64);
            $V!(X64Word64AtomicSubUint64);
            $V!(X64Word64AtomicAndUint64);
            $V!(X64Word64AtomicOrUint64);
            $V!(X64Word64AtomicXorUint64);
            $V!(X64Word64AtomicExchangeUint64);
            $V!(X64Word64AtomicCompareExchangeUint64);
            $V!(X64Movdqu256);
            $V!(X64MovqDecompressTaggedSigned);
            $V!(X64MovqDecompressTagged);
            $V!(X64MovqCompressTagged);
            $V!(X64MovqDecompressProtected);
            $V!(X64S256Load8Splat);
            $V!(X64S256Load16Splat);
            $V!(X64S256Load32Splat);
            $V!(X64S256Load64Splat);
            $V!(X64S256Load8x16S);
            $V!(X64S256Load8x16U);
            $V!(X64S256Load8x8U);
            $V!(X64S256Load16x8S);
            $V!(X64S256Load16x8U);
            $V!(X64S256Load32x4S);
            $V!(X64S256Load32x4U);
            $V!(SSEFloat32Add);
            $V!(SSEFloat32Sub);
            $V!(SSEFloat32Mul);
            $V!(SSEFloat32Div);
            $V!(SSEFloat64Add);
            $V!(SSEFloat64Sub);
            $V!(SSEFloat64Mul);
            $V!(SSEFloat64Div);
            $V!(AVXFloat32Add);
            $V!(AVXFloat32Sub);
            $V!(AVXFloat32Mul);
            $V!(AVXFloat32Div);
            $V!(AVXFloat64Add);
            $V!(AVXFloat64Sub);
            $V!(AVXFloat64Mul);
            $V!(AVXFloat64Div);
        };
    }

    // Define a macro for the full opcode list
    macro_rules! target_arch_opcode_list {
        ($V:ident) => {
            target_arch_opcode_with_memory_access_mode_list!($V);
            $V!(X64Add);
            $V!(X64Add32);
            $V!(X64And);
            $V!(X64And32);
            $V!(X64Cmp);
            $V!(X64Cmp32);
            $V!(X64Cmp16);
            $V!(X64Cmp8);
            $V!(X64Test);
            $V!(X64Test32);
            $V!(X64Test16);
            $V!(X64Test8);
            $V!(X64Or);
            $V!(X64Or32);
            $V!(X64Xor);
            $V!(X64Xor32);
            $V!(X64Sub);
            $V!(X64Sub32);
            $V!(X64Imul);
            $V!(X64Imul32);
            $V!(X64ImulHigh32);
            $V!(X64ImulHigh64);
            $V!(X64UmulHigh32);
            $V!(X64UmulHigh64);
            $V!(X64Idiv);
            $V!(X64Idiv32);
            $V!(X64Udiv);
            $V!(X64Udiv32);
            $V!(X64Not);
            $V!(X64Not32);
            $V!(X64Neg);
            $V!(X64Neg32);
            $V!(X64Shl);
            $V!(X64Shl32);
            $V!(X64Shr);
            $V!(X64Shr32);
            $V!(X64Sar);
            $V!(X64Sar32);
            $V!(X64Rol);
            $V!(X64Rol32);
            $V!(X64Ror);
            $V!(X64Ror32);
            $V!(X64Lzcnt);
            $V!(X64Lzcnt32);
            $V!(X64Tzcnt);
            $V!(X64Tzcnt32);
            $V!(X64Popcnt);
            $V!(X64Popcnt32);
            $V!(X64Bswap);
            $V!(X64Bswap32);
            $V!(X64MFence);
            $V!(X64LFence);
            $V!(SSEFloat32Cmp);
            $V!(SSEFloat32Sqrt);
            $V!(SSEFloat32ToFloat64);
            $V!(SSEFloat32ToInt32);
            $V!(SSEFloat32ToUint32);
            $V!(SSEFloat32Round);
            $V!(SSEFloat64Cmp);
            $V!(SSEFloat64Mod);
            $V!(SSEFloat64Sqrt);
            $V!(SSEFloat64Round);
            $V!(SSEFloat32Max);
            $V!(SSEFloat64Max);
            $V!(SSEFloat32Min);
            $V!(SSEFloat64Min);
            $V!(SSEFloat64ToFloat32);
            $V!(SSEFloat64ToFloat16RawBits);
            $V!(SSEFloat16RawBitsToFloat64);
            $V!(SSEFloat64ToInt32);
            $V!(SSEFloat64ToUint32);
            $V!(SSEFloat32ToInt64);
            $V!(SSEFloat64ToInt64);
            $V!(SSEFloat32ToUint64);
            $V!(SSEFloat64ToUint64);
            $V!(SSEInt32ToFloat64);
            $V!(SSEInt32ToFloat32);
            $V!(SSEInt64ToFloat32);
            $V!(SSEInt64ToFloat64);
            $V!(SSEUint64ToFloat32);
            $V!(SSEUint64ToFloat64);
            $V!(SSEUint32ToFloat64);
            $V!(SSEUint32ToFloat32);
            $V!(SSEFloat64ExtractLowWord32);
            $V!(SSEFloat64ExtractHighWord32);
            $V!(SSEFloat64InsertLowWord32);
            $V!(SSEFloat64InsertHighWord32);
            $V!(SSEFloat64LoadLowWord32);
            $V!(SSEFloat64SilenceNaN);
            $V!(AVXFloat32Cmp);
            $V!(AVXFloat64Cmp);
            $V!(X64Float64Abs);
            $V!(X64Float64Neg);
            $V!(X64Float32Abs);
            $V!(X64Float32Neg);
            $V!(X64MovqStoreIndirectPointer);
            $V!(X64MovqEncodeSandboxedPointer);
            $V!(X64MovqDecodeSandboxedPointer);
            $V!(X64BitcastFI);
            $V!(X64BitcastDL);
            $V!(X64BitcastIF);
            $V!(X64BitcastLD);
            $V!(X64Lea32);
            $V!(X64Lea);
            $V!(X64Dec32);
            $V!(X64Inc32);
            $V!(X64Push);
            $V!(X64Poke);
            $V!(X64Peek);
            $V!(X64Cvttps2dq);
            $V!(X64Cvttpd2dq);
            $V!(X64I32x4TruncF64x2UZero);
            $V!(X64I32x4TruncF32x4U);
            $V!(X64I32x8TruncF32x8U);
            $V!(X64FSplat);
            $V!(X64FExtractLane);
            $V!(X64FReplaceLane);
            $V!(X64FAbs);
            $V!(X64FNeg);
            $V!(X64FSqrt);
            $V!(X64FAdd);
            $V!(X64FSub);
            $V!(X64FMul);
            $V!(X64FDiv);
            $V!(X64FMin);
            $V!(X64FMax);
            $V!(X64FEq);
            $V!(X64FNe);
            $V!(X64FLt);
            $V!(X64FLe);
            $V!(X64F64x2Qfma);
            $V!(X64F64x2Qfms);
            $V!(X64Minpd);
            $V!(X64Maxpd);
            $V!(X64F64x2Round);
            $V!(X64F64x2ConvertLowI32x4S);
            $V!(X64F64x4ConvertI32x4S);
            $V!(X64F64x2ConvertLowI32x4U);
            $V!(X64F32x4SConvertI32x4);
            $V!(X64F32x8SConvertI32x8);
            $V!(X64F32x4UConvertI32x4);
            $V!(X64F32x8UConvertI32x8);
            $V!(X64F32x4Qfma);
            $V!(X64F32x4Qfms);
            $V!(X64Minps);
            $V!(X64Maxps);
            $V!(X64F32x4Round);
            $V!(X64F32x4DemoteF64x2Zero);
            $V!(X64F32x4DemoteF64x4);
            $V!(X64F16x8Round);
            $V!(X64I16x8SConvertF16x8);
            $V!(X64I16x8UConvertF16x8);
            $V!(X64F16x8SConvertI16x8);
            $V!(X64F16x8UConvertI16x8);
            $V!(X64F16x8DemoteF32x4Zero);
            $V!(X64F16x8DemoteF64x2Zero);
            $V!(X64F32x4PromoteLowF16x8);
            $V!(X64F16x8Qfma);
            $V!(X64F16x8Qfms);
            $V!(X64Minph);
            $V!(X64Maxph);
            $V!(X64ISplat);
            $V!(X64IExtractLane);
            $V!(X64IAbs);
            $V!(X64INeg);
            $V!(X64IBitMask);
            $V!(X64IShl);
            $V!(X64IShrS);
            $V!(X64IAdd);
            $V!(X64ISub);
            $V!(X64IMul);
            $V!(X64IEq);
            $V!(X64IGtS);
            $V!(X64IGeS);
            $V!(X64INe);
            $V!(X64IShrU);
            $V!(X64I64x2ExtMulLowI32x4S);
            $V!(X64I64x2ExtMulHighI32x4S);
            $V!(X64I64x2ExtMulLowI32x4U);
            $V!(X64I64x2ExtMulHighI32x4U);
            $V!(X64I64x2SConvertI32x4Low);
            $V!(X64I64x2SConvertI32x4High);
            $V!(X64I64x4SConvertI32x4);
            $V!(X64I64x2UConvertI32x4Low);
            $V!(X64I64x2UConvertI32x4High);
            $V!(X64I64x4UConvertI32x4);
            $V!(X64I32x4SConvertF32x4);
            $V!(X64I32x8SConvertF32x8);
            $V!(X64I32x4SConvertI16x8Low);
            $V!(X64I32x4SConvertI16x8High);
            $V!(X64I32x8SConvertI16x8);
            $V!(X64IMinS);
            $V!(X64IMaxS);
            $V!(X64I32x4UConvertF32x4);
            $V!(X64I32x8UConvertF32x8);
            $V!(X64I32x4UConvertI16x8Low);
            $V!(X64I32x4UConvertI16x8High);
            $V!(X64I32x8UConvertI16x8);
            $V!(X64IMinU);
            $V!(X64IMaxU);
            $V!(X64IGtU);
            $V!(X64IGeU);
            $V!(X64I32x4DotI16x8S);
            $V!(X64I32x8DotI16x16S);
            $V!(X64I32x4DotI8x16I7x16AddS);
            $V!(X64I32x4ExtMulLowI16x8S);
            $V!(X64I32x4ExtMulHighI16x8S);
            $V!(X64I32x4ExtMulLowI16x8U);
            $V!(X64I32x4ExtMulHighI16x8U);
            $V!(X64I32x4ExtAddPairwiseI16x8S);
            $V!(X64I32x8ExtAddPairwiseI16x16S);
            $V!(X64I32x4ExtAddPairwiseI16x8U);
            $V!(X64I32x8ExtAddPairwiseI16x16U);
            $V!(X64I32x4TruncSatF64x2SZero);
            $V!(X64I32x4TruncSatF64x2UZero);
            $V!(X64I32X4ShiftZeroExtendI8x16);
            $V!(X64IExtractLaneS);
            $V!(X64I16x8SConvertI8x16Low);
            $V!(X64I16x8SConvertI8x16High);
            $V!(X64I16x16SConvertI8x16);
            $V!(X64I16x8SConvertI32x4);
            $V!(X64I16x16SConvertI32x8);
            $V!(X64IAddSatS);
            $V!(X64ISubSatS);
            $V!(X64I16x8UConvertI8x16Low);
            $V!(X64I16x8UConvertI8x16High);
            $V!(X64I16x16UConvertI8x16);
            $V!(X64I16x8UConvertI32x4);
            $V!(X64I16x16UConvertI32x8);
            $V!(X64IAddSatU);
            $V!(X64ISubSatU);
            $V!(X64IRoundingAverageU);
            $V!(X64I16x8ExtMulLowI8x16S);
            $V!(X64I16x8ExtMulHighI8x16S);
            $V!(X64I16x8ExtMulLowI8x16U);
            $V!(X64I16x8ExtMulHighI8x16U);
            $V!(X64I16x8ExtAddPairwiseI8x16S);
            $V!(X64I16x16ExtAddPairwiseI8x32S);
            $V!(X64I16x8ExtAddPairwiseI8x16U);
            $V!(X64I16x16ExtAddPairwiseI8x32U);
            $V!(X64I16x8Q15MulRSatS);
            $V!(X64I16x8RelaxedQ15MulRS);
            $V!(X64I16x8DotI8x16I7x16S);
            $V!(X64I8x16SConvertI16x8);
            $V!(X64I8x32SConvertI16x16);
            $V!(X64I8x16UConvertI16x8);
            $V!(X64I8x32UConvertI16x16);
            $V!(X64S128Const);
            $V!(X64S256Const);
            $V!(X64SZero);
            $V!(X64SAllOnes);
            $V!(X64SNot);
            $V!(X64SAnd);
            $V!(X64SOr);
            $V!(X64SXor);
            $V!(X64SSelect);
            $V!(X64SAndNot);
            $V!(X64I8x16Swizzle);
            $V!(X64I8x16Shuffle);
            $V!(X64Vpshufd);
            $V!(X64I8x16Popcnt);
            $V!(X64Shufps);
            $V!(X64S32x4Rotate);
            $V!(X64S32x4Swizzle);
            $V!(X64S32x4Shuffle);
            $V!(X64S16x8Blend);
            $V!(X64S16x8HalfShuffle1);
            $V!(X64S16x8HalfShuffle2);
            $V!(X64S8x16Alignr);
            $V!(X64S16x8Dup);
            $V!(X64S8x16Dup);
            $V!(X64S16x8UnzipHigh);
            $V!(X64S16x8UnzipLow);
            $V!(X64S8x16UnzipHigh);
            $V!(X64S8x16UnzipLow);
            $V!(X64S64x2UnpackHigh);
            $V!(X64S32x4UnpackHigh);
            $V!(X64S16x8UnpackHigh);
            $V!(X64S8x16UnpackHigh);
            $V!(X64S32x8UnpackHigh);
            $V!(X64S64x2UnpackLow);
            $V!(X64S32x4UnpackLow);
            $V!(X64S16x8UnpackLow);
            $V!(X64S8x16UnpackLow);
            $V!(X64S32x8UnpackLow);
            $V!(X64S8x16TransposeLow);
            $V!(X64S8x16TransposeHigh);
            $V!(X64S8x8Reverse);
            $V!(X64S8x4Reverse);
            $V!(X64S8x2Reverse);
            $V!(X64V128AnyTrue);
            $V!(X64IAllTrue);
            $V!(X64Blendvpd);
            $V!(X64Blendvps);
            $V!(X64Pblendvb);
            $V!(X64I64x4ExtMulI32x4S);
            $V!(X64I64x4ExtMulI32x4U);
            $V!(X64I32x8ExtMulI16x8S);
            $V!(X64I32x8ExtMulI16x8U);
            $V!(X64I16x16ExtMulI8x16S);
            $V!(X64I16x16ExtMulI8x16U);
            $V!(X64TraceInstruction);
            $V!(X64F32x8Pmin);
            $V!(X64F32x8Pmax);
            $V!(X64F64x4Pmin);
            $V!(X64F64x4Pmax);
            $V!(X64ExtractF128);
            $V!(X64F32x8Qfma);
            $V!(X64F32x8Qfms);
            $V!(X64F64x4Qfma);
            $V!(X64F64x4Qfms);
            $V!(X64InsertI128);
            $V!(X64I32x8DotI8x32I7x32AddS);
            $V!(X64I16x16DotI8x32I7x32S);
        };
    }

    // Addressing modes
    macro_rules! target_addressing_mode_list {
        ($V:ident) => {
            $V!(MR);
            $V!(MRI);
            $V!(MR1);
            $V!(MR2);
            $V!(MR4);
            $V!(MR8);
            $V!(MR1I);
            $V!(MR2I);
            $V!(MR4I);
            $V!(MR8I);
            $V!(M1);
            $V!(M2);
            $V!(M4);
            $V!(M8);
            $V!(M1I);
            $V!(M2I);
            $V!(M4I);
            $V!(M8I);
            $V!(Root);
            $V!(MCR);
            $V!(MCRI);
        };
    }

    #[derive(Debug, Copy, Clone)]
    pub enum AddressingMode {
        MR,
        MRI,
        MR1,
        MR2,
        MR4,
        MR8,
        MR1I,
        MR2I,
        MR4I,
        MR8I,
        M1,
        M2,
        M4,
        M8,
        M1I,
        M2I,
        M4I,
        M8I,
        Root,
        MCR,
        MCRI,
    }

    impl AddressingMode {
        pub fn from_str(s: &str) -> Option<Self> {
            match s {
                "MR" => Some(AddressingMode::MR),
                "MRI" => Some(AddressingMode::MRI),
                "MR1" => Some(AddressingMode::MR1),
                "MR2" => Some(AddressingMode::MR2),
                "MR4" => Some(AddressingMode::MR4),
                "MR8" => Some(AddressingMode::MR8),
                "MR1I" => Some(AddressingMode::MR1I),
                "MR2I" => Some(AddressingMode::MR2I),
                "MR4I" => Some(AddressingMode::MR4I),
                "MR8I" => Some(AddressingMode::MR8I),
                "M1" => Some(AddressingMode::M1),
                "M2" => Some(AddressingMode::M2),
                "M4" => Some(AddressingMode::M4),
                "M8" => Some(AddressingMode::M8),
                "M1I" => Some(AddressingMode::M1I),
                "M2I" => Some(AddressingMode::M2I),
                "M4I" => Some(AddressingMode::M4I),
                "M8I" => Some(AddressingMode::M8I),
                "Root" => Some(AddressingMode::Root),
                "MCR" => Some(AddressingMode::MCR),
                "MCRI" => Some(AddressingMode::MCRI),
                _ => None,
            }
        }
    }

    #[derive(Debug, Copy, Clone)]
    pub enum Opcode {
        X64F64x2PromoteLowF32x4,
        X64Movb,
        X64Movdqu,
        X64Movl,
        X64Movq,
        X64Movsd,
        X64Movss,
        X64Movsh,
        X64Movsxbl,
        X64Movsxbq,
        X64Movsxlq,
        X64Movsxwl,
        X64Movsxwq,
        X64Movw,
        X64Movzxbl,
        X64Movzxbq,
        X64Movzxwl,
        X64Movzxwq,
        X64Pextrb,
        X64Pextrw,
        X64Pinsrb,
        X64Pinsrd,
        X64Pinsrq,
        X64Pinsrw,
        X64S128Load16Splat,
        X64S128Load16x4S,
        X64S128Load16x4U,
        X64S128Load32Splat,
        X64S128Load32x2S,
        X64S128Load32x2U,
        X64S128Load64Splat,
        X64S128Load8Splat,
        X64S128Load8x8S,
        X64S128Load8x8U,
        X64S128Store32Lane,
        X64S128Store64Lane,
        X64Word64AtomicStoreWord64,
        X64Word64AtomicAddUint64,
        X64Word64AtomicSubUint64,
        X64Word64AtomicAndUint64,
        X64Word64AtomicOrUint64,
        X64Word64AtomicXorUint64,
        X64Word64AtomicExchangeUint64,
        X64Word64AtomicCompareExchangeUint64,
        X64Movdqu256,
        X64MovqDecompressTaggedSigned,
        X64MovqDecompressTagged,
        X64MovqCompressTagged,
        X64MovqDecompressProtected,
        X64S256Load8Splat,
        X64S256Load16Splat,
        X64S256Load32Splat,
        X64S256Load64Splat,
        X64S256Load8x16S,
        X64S256Load8x16U,
        X64S256Load8x8U,
        X64S256Load16x8S,
        X64S256Load16x8U,
        X64S256Load32x4S,
        X64S256Load32x4U,
        SSEFloat32Add,
        SSEFloat32Sub,
        SSEFloat32Mul,
        SSEFloat32Div,
        SSEFloat64Add,
        SSEFloat64Sub,
        SSEFloat64Mul,
        SSEFloat64Div,
        AVXFloat32Add,
        AVXFloat32Sub,
        AVXFloat32Mul,
        AVXFloat32Div,
        AVXFloat64Add,
        AVXFloat64Sub,
        AVXFloat64Mul,
        AVXFloat64Div,
        X64Add,
        X64Add32,
        X64And,
        X64And32,
        X64Cmp,
        X64Cmp32,
        X64Cmp16,
        X64Cmp8,
        X64Test,
        X64Test32,
        X64Test16,
        X64Test8,
        X64Or,
        X64Or32,
        X64Xor,
        X64Xor32,
        X64Sub,
        X64Sub32,
        X64Imul,
        X64Imul32,
        X64ImulHigh32,
        X64ImulHigh64,
        X64UmulHigh32,
        X64UmulHigh64,
        X64Idiv,
        X64Idiv32,
        X64Udiv,
        X64Udiv32,
        X64Not,
        X64Not32,
        X64Neg,
        X64Neg32,
        X64Shl,
        X64Shl32,
        X64Shr,
        X64Shr32,
        X64Sar,
        X64Sar32,
        X64Rol,
        X64Rol32,
        X64Ror,
        X64Ror32,
        X64Lzcnt,
        X64Lzcnt32,
        X64Tzcnt,
        X64Tzcnt32,
        X64Popcnt,
        X64Popcnt32,
        X64Bswap,
        X64Bswap32,
        X64MFence,
        X64LFence,
        SSEFloat32Cmp,
        SSEFloat32Sqrt
