// Converted from V8 C++ source files:
// Header: instruction-codes-arm.h
// Implementation: N/A
// 
// This file combines both header and implementation into idiomatic Rust code.

pub mod instruction_codes_arm {
    // ARM-specific opcodes that specify which assembly sequence to emit.
    // Most opcodes specify a single instruction.
    #[derive(Debug, Copy, Clone, PartialEq, Eq)]
    pub enum ArchOpcode {
        ArmAdd,
        ArmAnd,
        ArmBic,
        ArmClz,
        ArmCmp,
        ArmCmn,
        ArmTst,
        ArmTeq,
        ArmOrr,
        ArmEor,
        ArmSub,
        ArmRsb,
        ArmMul,
        ArmMla,
        ArmMls,
        ArmSmull,
        ArmSmmul,
        ArmSmmla,
        ArmUmull,
        ArmSdiv,
        ArmUdiv,
        ArmMov,
        ArmMvn,
        ArmBfc,
        ArmUbfx,
        ArmSbfx,
        ArmSxtb,
        ArmSxth,
        ArmSxtab,
        ArmSxtah,
        ArmUxtb,
        ArmUxth,
        ArmUxtab,
        ArmRbit,
        ArmRev,
        ArmUxtah,
        ArmAddPair,
        ArmSubPair,
        ArmMulPair,
        ArmLslPair,
        ArmLsrPair,
        ArmAsrPair,
        ArmVcmpF32,
        ArmVaddF32,
        ArmVsubF32,
        ArmVmulF32,
        ArmVmlaF32,
        ArmVmlsF32,
        ArmVdivF32,
        ArmVabsF32,
        ArmVnegF32,
        ArmVsqrtF32,
        ArmVcmpF64,
        ArmVaddF64,
        ArmVsubF64,
        ArmVmulF64,
        ArmVmlaF64,
        ArmVmlsF64,
        ArmVdivF64,
        ArmVmodF64,
        ArmVabsF64,
        ArmVnegF64,
        ArmVsqrtF64,
        ArmVmullLow,
        ArmVmullHigh,
        ArmVrintmF32,
        ArmVrintmF64,
        ArmVrintpF32,
        ArmVrintpF64,
        ArmVrintzF32,
        ArmVrintzF64,
        ArmVrintaF64,
        ArmVrintnF32,
        ArmVrintnF64,
        ArmVcvtF32F64,
        ArmVcvtF64F32,
        ArmVcvtF32S32,
        ArmVcvtF32U32,
        ArmVcvtF64S32,
        ArmVcvtF64U32,
        ArmVcvtS32F32,
        ArmVcvtU32F32,
        ArmVcvtS32F64,
        ArmVcvtU32F64,
        ArmVmovU32F32,
        ArmVmovF32U32,
        ArmVmovLowU32F64,
        ArmVmovLowF64U32,
        ArmVmovHighU32F64,
        ArmVmovHighF64U32,
        ArmVmovF64U32U32,
        ArmVmovU32U32F64,
        ArmVldrF32,
        ArmVstrF32,
        ArmVldrF64,
        ArmVld1F64,
        ArmVstrF64,
        ArmVst1F64,
        ArmVld1S128,
        ArmVst1S128,
        ArmVcnt,
        ArmVpadal,
        ArmVpaddl,
        ArmFloat32Max,
        ArmFloat64Max,
        ArmFloat32Min,
        ArmFloat64Min,
        ArmFloat64SilenceNaN,
        ArmLdrb,
        ArmLdrsb,
        ArmStrb,
        ArmLdrh,
        ArmLdrsh,
        ArmStrh,
        ArmLdr,
        ArmStr,
        ArmPush,
        ArmPoke,
        ArmPeek,
        ArmDmbIsh,
        ArmDsbIsb,
        ArmF64x2Splat,
        ArmF64x2ExtractLane,
        ArmF64x2ReplaceLane,
        ArmF64x2Abs,
        ArmF64x2Neg,
        ArmF64x2Sqrt,
        ArmF64x2Add,
        ArmF64x2Sub,
        ArmF64x2Mul,
        ArmF64x2Div,
        ArmF64x2Min,
        ArmF64x2Max,
        ArmF64x2Eq,
        ArmF64x2Ne,
        ArmF64x2Lt,
        ArmF64x2Le,
        ArmF64x2Pmin,
        ArmF64x2Pmax,
        ArmF64x2Qfma,
        ArmF64x2Qfms,
        ArmF64x2Ceil,
        ArmF64x2Floor,
        ArmF64x2Trunc,
        ArmF64x2NearestInt,
        ArmF64x2ConvertLowI32x4S,
        ArmF64x2ConvertLowI32x4U,
        ArmF64x2PromoteLowF32x4,
        ArmF32x4Splat,
        ArmF32x4ExtractLane,
        ArmF32x4ReplaceLane,
        ArmF32x4SConvertI32x4,
        ArmF32x4UConvertI32x4,
        ArmF32x4Abs,
        ArmF32x4Neg,
        ArmF32x4Sqrt,
        ArmF32x4Add,
        ArmF32x4Sub,
        ArmF32x4Mul,
        ArmF32x4Div,
        ArmF32x4Min,
        ArmF32x4Max,
        ArmF32x4Eq,
        ArmF32x4Ne,
        ArmF32x4Lt,
        ArmF32x4Le,
        ArmF32x4Pmin,
        ArmF32x4Pmax,
        ArmF32x4Qfma,
        ArmF32x4Qfms,
        ArmF32x4DemoteF64x2Zero,
        ArmI64x2SplatI32Pair,
        ArmI64x2ReplaceLaneI32Pair,
        ArmI64x2Abs,
        ArmI64x2Neg,
        ArmI64x2Shl,
        ArmI64x2ShrS,
        ArmI64x2Add,
        ArmI64x2Sub,
        ArmI64x2Mul,
        ArmI64x2ShrU,
        ArmI64x2BitMask,
        ArmI64x2Eq,
        ArmI64x2Ne,
        ArmI64x2GtS,
        ArmI64x2GeS,
        ArmI64x2SConvertI32x4Low,
        ArmI64x2SConvertI32x4High,
        ArmI64x2UConvertI32x4Low,
        ArmI64x2UConvertI32x4High,
        ArmI32x4Splat,
        ArmI32x4ExtractLane,
        ArmI32x4ReplaceLane,
        ArmI32x4SConvertF32x4,
        ArmI32x4SConvertI16x8Low,
        ArmI32x4SConvertI16x8High,
        ArmI32x4Neg,
        ArmI32x4Shl,
        ArmI32x4ShrS,
        ArmI32x4Add,
        ArmI32x4Sub,
        ArmI32x4Mul,
        ArmI32x4MinS,
        ArmI32x4MaxS,
        ArmI32x4Eq,
        ArmI32x4Ne,
        ArmI32x4GtS,
        ArmI32x4GeS,
        ArmI32x4UConvertF32x4,
        ArmI32x4UConvertI16x8Low,
        ArmI32x4UConvertI16x8High,
        ArmI32x4ShrU,
        ArmI32x4MinU,
        ArmI32x4MaxU,
        ArmI32x4GtU,
        ArmI32x4GeU,
        ArmI32x4Abs,
        ArmI32x4BitMask,
        ArmI32x4DotI16x8S,
        ArmI16x8DotI8x16S,
        ArmI32x4DotI8x16AddS,
        ArmI32x4TruncSatF64x2SZero,
        ArmI32x4TruncSatF64x2UZero,
        ArmI16x8Splat,
        ArmI16x8ExtractLaneS,
        ArmI16x8ReplaceLane,
        ArmI16x8SConvertI8x16Low,
        ArmI16x8SConvertI8x16High,
        ArmI16x8Neg,
        ArmI16x8Shl,
        ArmI16x8ShrS,
        ArmI16x8SConvertI32x4,
        ArmI16x8Add,
        ArmI16x8AddSatS,
        ArmI16x8Sub,
        ArmI16x8SubSatS,
        ArmI16x8Mul,
        ArmI16x8MinS,
        ArmI16x8MaxS,
        ArmI16x8Eq,
        ArmI16x8Ne,
        ArmI16x8GtS,
        ArmI16x8GeS,
        ArmI16x8ExtractLaneU,
        ArmI16x8UConvertI8x16Low,
        ArmI16x8UConvertI8x16High,
        ArmI16x8ShrU,
        ArmI16x8UConvertI32x4,
        ArmI16x8AddSatU,
        ArmI16x8SubSatU,
        ArmI16x8MinU,
        ArmI16x8MaxU,
        ArmI16x8GtU,
        ArmI16x8GeU,
        ArmI16x8RoundingAverageU,
        ArmI16x8Abs,
        ArmI16x8BitMask,
        ArmI16x8Q15MulRSatS,
        ArmI8x16Splat,
        ArmI8x16ExtractLaneS,
        ArmI8x16ReplaceLane,
        ArmI8x16Neg,
        ArmI8x16Shl,
        ArmI8x16ShrS,
        ArmI8x16SConvertI16x8,
        ArmI8x16Add,
        ArmI8x16AddSatS,
        ArmI8x16Sub,
        ArmI8x16SubSatS,
        ArmI8x16MinS,
        ArmI8x16MaxS,
        ArmI8x16Eq,
        ArmI8x16Ne,
        ArmI8x16GtS,
        ArmI8x16GeS,
        ArmI8x16ExtractLaneU,
        ArmI8x16ShrU,
        ArmI8x16UConvertI16x8,
        ArmI8x16AddSatU,
        ArmI8x16SubSatU,
        ArmI8x16MinU,
        ArmI8x16MaxU,
        ArmI8x16GtU,
        ArmI8x16GeU,
        ArmI8x16RoundingAverageU,
        ArmI8x16Abs,
        ArmI8x16BitMask,
        ArmS128Const,
        ArmS128Zero,
        ArmS128AllOnes,
        ArmS128Dup,
        ArmS128And,
        ArmS128Or,
        ArmS128Xor,
        ArmS128Not,
        ArmS128Select,
        ArmS128AndNot,
        ArmS32x4ZipLeft,
        ArmS32x4ZipRight,
        ArmS32x4UnzipLeft,
        ArmS32x4UnzipRight,
        ArmS32x4TransposeLeft,
        ArmS32x4TransposeRight,
        ArmS32x4Shuffle,
        ArmS16x8ZipLeft,
        ArmS16x8ZipRight,
        ArmS16x8UnzipLeft,
        ArmS16x8UnzipRight,
        ArmS16x8TransposeLeft,
        ArmS16x8TransposeRight,
        ArmS8x16ZipLeft,
        ArmS8x16ZipRight,
        ArmS8x16UnzipLeft,
        ArmS8x16UnzipRight,
        ArmS8x16TransposeLeft,
        ArmS8x16TransposeRight,
        ArmS8x16Concat,
        ArmI8x16Swizzle,
        ArmI8x16Shuffle,
        ArmS32x2Reverse,
        ArmS16x4Reverse,
        ArmS16x2Reverse,
        ArmS8x8Reverse,
        ArmS8x4Reverse,
        ArmS8x2Reverse,
        ArmI64x2AllTrue,
        ArmI32x4AllTrue,
        ArmI16x8AllTrue,
        ArmV128AnyTrue,
        ArmI8x16AllTrue,
        ArmS128Load8Splat,
        ArmS128Load16Splat,
        ArmS128Load32Splat,
        ArmS128Load64Splat,
        ArmS128Load8x8S,
        ArmS128Load8x8U,
        ArmS128Load16x4S,
        ArmS128Load16x4U,
        ArmS128Load32x2S,
        ArmS128Load32x2U,
        ArmS128Load32Zero,
        ArmS128Load64Zero,
        ArmS128LoadLaneLow,
        ArmS128LoadLaneHigh,
        ArmS128StoreLaneLow,
        ArmS128StoreLaneHigh,
        ArmWord32AtomicPairLoad,
        ArmWord32AtomicPairStore,
        ArmWord32AtomicPairAdd,
        ArmWord32AtomicPairSub,
        ArmWord32AtomicPairAnd,
        ArmWord32AtomicPairOr,
        ArmWord32AtomicPairXor,
        ArmWord32AtomicPairExchange,
        ArmWord32AtomicPairCompareExchange,
    }

    // Addressing modes represent the "shape" of inputs to an instruction.
    // Many instructions support multiple addressing modes. Addressing modes
    // are encoded into the InstructionCode of the instruction and tell the
    // code generator after register allocation which assembler method to call.
    #[derive(Debug, Copy, Clone, PartialEq, Eq)]
    pub enum AddressingMode {
        Offset_RI,        /* [%r0 + K] */
        Offset_RR,        /* [%r0 + %r1] */
        Operand2_I,       /* K */
        Operand2_R,       /* %r0 */
        Operand2_R_ASR_I, /* %r0 ASR K */
        Operand2_R_LSL_I, /* %r0 LSL K */
        Operand2_R_LSR_I, /* %r0 LSR K */
        Operand2_R_ROR_I, /* %r0 ROR K */
        Operand2_R_ASR_R, /* %r0 ASR %r1 */
        Operand2_R_LSL_R, /* %r0 LSL %r1 */
        Operand2_R_LSR_R, /* %r0 LSR %r1 */
        Operand2_R_ROR_R, /* %r0 ROR %r1 */
        Root,             /* [%rr + K] */
    }
}
